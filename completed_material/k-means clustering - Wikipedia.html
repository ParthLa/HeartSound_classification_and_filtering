<!DOCTYPE html>
<!-- saved from url=(0048)https://en.wikipedia.org/wiki/K-means_clustering -->
<html class="client-js ve-not-available" lang="en" dir="ltr"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>k-means clustering - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRequestId":"XeZdvwpAICEAAHpF3DYAAACK","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"K-means_clustering","wgTitle":"K-means clustering","wgCurRevisionId":929064204,"wgRevisionId":929064204,"wgArticleId":1860407,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 French-language sources (fr)","CS1 errors: missing periodical","CS1 maint: uses editors parameter","Articles with short description","Cluster analysis algorithms"],"wgPageContentLanguage":"en",
"wgPageContentModel":"wikitext","wgRelevantPageName":"K-means_clustering","wgRelevantArticleId":1860407,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q310401","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.math.styles":"ready",
"ext.cite.styles":"ready","mediawiki.page.gallery.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"};RLPAGEMODULES=["ext.math.scripts","ext.cite.ux-enhancements","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming",
"ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.tokens@tffin",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="./k-means clustering - Wikipedia_files/load.php">
<script async="" src="./k-means clustering - Wikipedia_files/load(1).php"></script>
<style>
.mw-editfont-monospace{font-family:monospace,monospace}.mw-editfont-sans-serif{font-family:sans-serif}.mw-editfont-serif{font-family:serif} .mw-editfont-monospace,.mw-editfont-sans-serif,.mw-editfont-serif{font-size:13px; }.mw-editfont-monospace.oo-ui-textInputWidget,.mw-editfont-sans-serif.oo-ui-textInputWidget,.mw-editfont-serif.oo-ui-textInputWidget{font-size:inherit}.mw-editfont-monospace > .oo-ui-inputWidget-input,.mw-editfont-sans-serif > .oo-ui-inputWidget-input,.mw-editfont-serif > .oo-ui-inputWidget-input{font-size:13px}
.mw-ui-button{background-color:#f8f9fa;color:#222222;display:inline-block;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;min-width:4em;max-width:28.75em;margin:0;padding:0.57142857em 0.9375em;border:1px solid #a2a9b1;border-radius:2px;cursor:pointer;vertical-align:middle;font-family:inherit;font-size:1em;font-weight:bold;line-height:1;text-align:center;-webkit-appearance:none}.mw-ui-button:visited{color:#222222}.mw-ui-button:hover{background-color:#ffffff;color:#444444;border-color:#a2a9b1}.mw-ui-button:focus{background-color:#ffffff;color:#222222;border-color:#3366cc;box-shadow:inset 0 0 0 1px #3366cc,inset 0 0 0 2px #ffffff;outline-width:0}.mw-ui-button:focus::-moz-focus-inner{border-color:transparent;padding:0}.mw-ui-button:active,.mw-ui-button.is-on{background-color:#c8ccd1;color:#000000;border-color:#72777d;box-shadow:none}.mw-ui-button:disabled{background-color:#c8ccd1;color:#ffffff;border-color:#c8ccd1;cursor:default}.mw-ui-button:disabled:hover,.mw-ui-button:disabled:active{background-color:#c8ccd1;color:#ffffff;box-shadow:none;border-color:#c8ccd1}.mw-ui-button:not(:disabled){-webkit-transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms;-moz-transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms;transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms}.mw-ui-button.mw-ui-quiet,.mw-ui-button.mw-ui-quiet.mw-ui-progressive,.mw-ui-button.mw-ui-quiet.mw-ui-destructive{background-color:transparent;color:#222222;border-color:transparent}.mw-ui-button.mw-ui-quiet:hover,.mw-ui-button.mw-ui-quiet.mw-ui-progressive:hover,.mw-ui-button.mw-ui-quiet.mw-ui-destructive:hover{background-color:transparent;color:#444444;border-color:transparent;box-shadow:none}.mw-ui-button.mw-ui-quiet:active,.mw-ui-button.mw-ui-quiet.mw-ui-progressive:active,.mw-ui-button.mw-ui-quiet.mw-ui-destructive:active{background-color:transparent;color:#000000;border-color:transparent}.mw-ui-button.mw-ui-quiet:focus,.mw-ui-button.mw-ui-quiet.mw-ui-progressive:focus,.mw-ui-button.mw-ui-quiet.mw-ui-destructive:focus{background-color:transparent;color:#222222;border-color:transparent;box-shadow:none}.mw-ui-button.mw-ui-quiet:disabled,.mw-ui-button.mw-ui-quiet.mw-ui-progressive:disabled,.mw-ui-button.mw-ui-quiet.mw-ui-destructive:disabled,.mw-ui-button.mw-ui-quiet:disabled:hover,.mw-ui-button.mw-ui-quiet.mw-ui-progressive:disabled:hover,.mw-ui-button.mw-ui-quiet.mw-ui-destructive:disabled:hover,.mw-ui-button.mw-ui-quiet:disabled:active,.mw-ui-button.mw-ui-quiet.mw-ui-progressive:disabled:active,.mw-ui-button.mw-ui-quiet.mw-ui-destructive:disabled:active{background-color:transparent;color:#72777d;border-color:transparent}.mw-ui-button.mw-ui-progressive{background-color:#3366cc;color:#fff;border:1px solid #3366cc}.mw-ui-button.mw-ui-progressive:hover{background-color:#447ff5;border-color:#447ff5}.mw-ui-button.mw-ui-progressive:focus{box-shadow:inset 0 0 0 1px #3366cc,inset 0 0 0 2px #ffffff}.mw-ui-button.mw-ui-progressive:active,.mw-ui-button.mw-ui-progressive.is-on{background-color:#2a4b8d;border-color:#2a4b8d;box-shadow:none}.mw-ui-button.mw-ui-progressive:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button.mw-ui-progressive:disabled:hover,.mw-ui-button.mw-ui-progressive:disabled:active{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1;box-shadow:none}.mw-ui-button.mw-ui-progressive.mw-ui-quiet{color:#3366cc}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:hover{background-color:transparent;color:#447ff5}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:active{color:#2a4b8d}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:focus{background-color:transparent;color:#3366cc}.mw-ui-button.mw-ui-destructive{background-color:#dd3333;color:#fff;border:1px solid #dd3333}.mw-ui-button.mw-ui-destructive:hover{background-color:#ff4242;border-color:#ff4242}.mw-ui-button.mw-ui-destructive:focus{box-shadow:inset 0 0 0 1px #dd3333,inset 0 0 0 2px #ffffff}.mw-ui-button.mw-ui-destructive:active,.mw-ui-button.mw-ui-destructive.is-on{background-color:#b32424;border-color:#b32424;box-shadow:none}.mw-ui-button.mw-ui-destructive:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button.mw-ui-destructive:disabled:hover,.mw-ui-button.mw-ui-destructive:disabled:active{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1;box-shadow:none}.mw-ui-button.mw-ui-destructive.mw-ui-quiet{color:#dd3333}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:hover{background-color:transparent;color:#ff4242}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:active{color:#b32424}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:focus{background-color:transparent;color:#dd3333}.mw-ui-button.mw-ui-big{font-size:1.3em}.mw-ui-button.mw-ui-block{display:block;width:100%;margin-left:auto;margin-right:auto}input.mw-ui-button::-moz-focus-inner,button.mw-ui-button::-moz-focus-inner{margin-top:-1px;margin-bottom:-1px}a.mw-ui-button{text-decoration:none}a.mw-ui-button:hover,a.mw-ui-button:focus{text-decoration:none}.mw-ui-button-group > *{min-width:48px;border-radius:0;float:left}.mw-ui-button-group > *:first-child{border-top-left-radius:2px;border-bottom-left-radius:2px}.mw-ui-button-group > *:not(:first-child){border-left:0}.mw-ui-button-group > *:last-child{border-top-right-radius:2px;border-bottom-right-radius:2px}.mw-ui-button-group .is-on .button{cursor:default}
.mw-ui-icon{position:relative;line-height:1.5em;min-height:1.5em;min-width:1.5em}span.mw-ui-icon{display:inline-block}.mw-ui-icon.mw-ui-icon-element{text-indent:-999px;overflow:hidden;width:3.5em;min-width:3.5em;max-width:3.5em}.mw-ui-icon.mw-ui-icon-element:before{left:0;right:0;position:absolute;margin:0 1em}.mw-ui-icon.mw-ui-icon-element.mw-ui-icon-large{width:4.625em;min-width:4.625em;max-width:4.625em;line-height:4.625em;min-height:4.625em}.mw-ui-icon.mw-ui-icon-element.mw-ui-icon-large:before{min-height:4.625em}.mw-ui-icon.mw-ui-icon-before:before,.mw-ui-icon.mw-ui-icon-element:before{background-position:50% 50%;background-repeat:no-repeat;background-size:100% auto;float:left;display:block;min-height:1.5em;content:''}.mw-ui-icon.mw-ui-icon-before:before{position:relative;width:1.5em;margin-right:1em}.mw-ui-icon.mw-ui-icon-small:before{background-size:66.67% auto}
.ve-init-mw-progressBarWidget{height:1em;overflow:hidden;margin:0 25%}.ve-init-mw-progressBarWidget-bar{height:1em;width:0} .ve-init-mw-progressBarWidget{height:0.75em;border:1px solid #36c;background:#fff;border-radius:2px;box-shadow:0 0.1em 0 0 rgba(0,0,0,0.15)}.ve-init-mw-progressBarWidget-bar{height:0.75em;background:#36c}
.wp-teahouse-question-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}#wp-th-question-ask{float:right}.wp-teahouse-ask a.external{background-image:none !important}.wp-teahouse-respond-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}.wp-th-respond{float:right}.wp-teahouse-respond a.external{background-image:none !important}
.rt-tooltip{position:absolute;z-index:100;max-width:350px;background:#fff;color:#222;font-size:13px;line-height:1.5em;border:1px solid #c8ccd1;border-radius:3px;box-shadow:0 15px 45px -10px rgba(0,0,0,0.3);overflow-wrap:break-word}.rt-tooltip.rt-tooltip-insideWindow{z-index:110}.rt-tooltipContent{padding:8px 11px}.rt-tooltip-above .rt-tooltipContent{margin-bottom:-8px;padding-bottom:16px}.rt-tooltip-below .rt-tooltipContent{margin-top:-10px;padding-top:18px}.rt-tooltipTail,.rt-tooltipTail:after{position:absolute;width:12px;height:12px}.rt-tooltipTail{background:#c8ccd1;background:-webkit-linear-gradient(bottom left,#c8ccd1 50%,rgba(0,0,0,0) 50%);background:linear-gradient(to top right,#c8ccd1 50%,rgba(0,0,0,0) 50%)}.rt-tooltipTail:after{content:"";background:#fff;bottom:1px;left:1px}.rt-tooltip-above .rt-tooltipTail{-webkit-transform:rotate(-45deg);transform:rotate(-45deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%;bottom:0;left:15px}.rt-tooltip-below .rt-tooltipTail{-webkit-transform:rotate(135deg);transform:rotate(135deg);-webkit-transform-origin:0 0;transform-origin:0 0;top:0;left:27px}.rt-settingsLink{background-image:linear-gradient(transparent,transparent),url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22utf-8%22%3F%3E%0D%0A%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%2024%2024%22%3E%0D%0A%20%20%20%20%3Cpath%20fill%3D%22%23555%22%20d%3D%22M20%2014.5v-2.9l-1.8-.3c-.1-.4-.3-.8-.6-1.4l1.1-1.5-2.1-2.1-1.5%201.1c-.5-.3-1-.5-1.4-.6L13.5%205h-2.9l-.3%201.8c-.5.1-.9.3-1.4.6L7.4%206.3%205.3%208.4l1%201.5c-.3.5-.4.9-.6%201.4l-1.7.2v2.9l1.8.3c.1.5.3.9.6%201.4l-1%201.5%202.1%202.1%201.5-1c.4.2.9.4%201.4.6l.3%201.8h3l.3-1.8c.5-.1.9-.3%201.4-.6l1.5%201.1%202.1-2.1-1.1-1.5c.3-.5.5-1%20.6-1.4l1.5-.3zM12%2016c-1.7%200-3-1.3-3-3s1.3-3%203-3%203%201.3%203%203-1.3%203-3%203z%22%2F%3E%0D%0A%3C%2Fsvg%3E);float:right;cursor:pointer;margin:-4px -4px 0 8px;height:24px;width:24px;border-radius:2px;background-position:center center;background-repeat:no-repeat;background-size:24px 24px}.rt-settingsLink:hover{background-color:#eee}.rt-target{background-color:#def}.rt-enableSelect{font-weight:bold}.rt-settingsFormSeparator{margin:0.85714286em 0}.rt-numberInput.rt-numberInput{width:150px}.rt-tooltipsForCommentsField.rt-tooltipsForCommentsField.rt-tooltipsForCommentsField{margin-top:1.64285714em}.rt-disabledHelp{border-collapse:collapse}.rt-disabledHelp td{padding:0}.rt-disabledNote.rt-disabledNote{vertical-align:bottom;padding-left:0.36em;font-weight:bold}@-webkit-keyframes rt-fade-in-up{0%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);transform:translate(0,20px) }100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }}@-moz-keyframes rt-fade-in-up{0%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);transform:translate(0,20px) }100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }}@keyframes rt-fade-in-up{0%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);transform:translate(0,20px) }100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }}@-webkit-keyframes rt-fade-in-down{0%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);transform:translate(0,-20px) }100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }}@-moz-keyframes rt-fade-in-down{0%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);transform:translate(0,-20px) }100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }}@keyframes rt-fade-in-down{0%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);transform:translate(0,-20px) }100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }}@-webkit-keyframes rt-fade-out-down{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }100%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);transform:translate(0,20px) }}@-moz-keyframes rt-fade-out-down{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }100%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);transform:translate(0,20px) }}@keyframes rt-fade-out-down{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }100%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);transform:translate(0,20px) }}@-webkit-keyframes rt-fade-out-up{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }100%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);transform:translate(0,-20px) }}@-moz-keyframes rt-fade-out-up{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }100%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);transform:translate(0,-20px) }}@keyframes rt-fade-out-up{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);transform:translate(0,0) }100%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);transform:translate(0,-20px) }}.rt-fade-in-up{-webkit-animation:rt-fade-in-up 0.2s ease forwards;-moz-animation:rt-fade-in-up 0.2s ease forwards;animation:rt-fade-in-up 0.2s ease forwards }.rt-fade-in-down{-webkit-animation:rt-fade-in-down 0.2s ease forwards;-moz-animation:rt-fade-in-down 0.2s ease forwards;animation:rt-fade-in-down 0.2s ease forwards }.rt-fade-out-down{-webkit-animation:rt-fade-out-down 0.2s ease forwards;-moz-animation:rt-fade-out-down 0.2s ease forwards;animation:rt-fade-out-down 0.2s ease forwards }.rt-fade-out-up{-webkit-animation:rt-fade-out-up 0.2s ease forwards;-moz-animation:rt-fade-out-up 0.2s ease forwards;animation:rt-fade-out-up 0.2s ease forwards }
.suggestions{overflow:hidden;position:absolute;top:0;left:0;width:0;border:0;z-index:1099;padding:0;margin:-1px 0 0 0}.suggestions-special{position:relative;background-color:#fff;cursor:pointer;border:1px solid #a2a9b1;margin:0;margin-top:-2px;display:none;padding:0.25em 0.25em;line-height:1.25em}.suggestions-results{background-color:#fff;cursor:pointer;border:1px solid #a2a9b1;padding:0;margin:0}.suggestions-result{color:#000;margin:0;line-height:1.5em;padding:0.01em 0.25em;text-align:left; overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.suggestions-result-current{background-color:#2a4b8d;color:#fff}.suggestions-special .special-label{color:#72777d;text-align:left}.suggestions-special .special-query{color:#000;font-style:italic;text-align:left}.suggestions-special .special-hover{background-color:#c8ccd1}.suggestions-result-current .special-label,.suggestions-result-current .special-query{color:#fff}.highlight{font-weight:bold}
@media screen {
	.tochidden,.toctoggle{-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.toctoggle{font-size:94%}}
@media print {
	.toc.tochidden,.toctoggle{display:none}}
@-webkit-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-webkit-transform:translateY(-20px)}100%{opacity:1;-webkit-transform:translateY(0)}}@-moz-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-moz-transform:translateY(-20px)}100%{opacity:1;-moz-transform:translateY(0)}}@-o-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-o-transform:translateY(-20px)}100%{opacity:1;-o-transform:translateY(0)}}@keyframes centralAuthPPersonalAnimation{0%{opacity:0;transform:translateY(-20px)}100%{opacity:1;transform:translateY(0)}}.centralAuthPPersonalAnimation{-webkit-animation-duration:1s;-moz-animation-duration:1s;-o-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;-moz-animation-fill-mode:both;-o-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:centralAuthPPersonalAnimation;-moz-animation-name:centralAuthPPersonalAnimation;-o-animation-name:centralAuthPPersonalAnimation;animation-name:centralAuthPPersonalAnimation}
.uls-menu{border-radius:2px; font-size:medium}.uls-search,.uls-language-settings-close-block{border-top-right-radius:2px;border-top-left-radius:2px}.uls-language-list{border-bottom-right-radius:2px;border-bottom-left-radius:2px}.uls-menu.callout:before,.uls-menu.callout:after{border-top:10px solid transparent;border-bottom:10px solid transparent;display:inline-block; top:17px;position:absolute;content:''}.uls-menu.callout.selector-right:before{ border-left:10px solid #c8ccd1; right:-11px}.uls-menu.callout.selector-right:after{ border-left:10px solid #fff; right:-10px}.uls-menu.callout.selector-left:before{ border-right:10px solid #c8ccd1; left:-11px}.uls-menu.callout.selector-left:after{ border-right:10px solid #fff; left:-10px}.uls-ui-languages button{margin:5px 15px 5px 0;white-space:nowrap;overflow:hidden}.uls-search-wrapper-wrapper{position:relative;padding-left:40px;margin-top:5px;margin-bottom:5px}.uls-icon-back{background:transparent url(/w/extensions/UniversalLanguageSelector/resources/images/back-grey-ltr.png?90e9b) no-repeat scroll center center;background-image:linear-gradient(transparent,transparent),url(/w/extensions/UniversalLanguageSelector/resources/images/back-grey-ltr.svg?e226b);background-size:28px;background-position:center center;height:32px;width:40px;display:block;position:absolute;left:0;border-right:1px solid #c8ccd1;opacity:0.8}.uls-icon-back:hover{opacity:1;cursor:pointer}.uls-menu .uls-no-results-view .uls-no-found-more{background-color:#fff}.uls-menu .uls-no-results-view h3{padding:0 28px;margin:0;color:#54595d;font-size:1em;font-weight:normal}  .skin-vector .uls-menu{border-color:#c8ccd1;-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.25);box-shadow:0 2px 2px 0 rgba(0,0,0,0.25);font-size:0.875em}.skin-vector .uls-search{border-bottom-color:#c8ccd1}.skin-vector .uls-search-label{opacity:0.51;-webkit-transition:opacity 250ms;-moz-transition:opacity 250ms;transition:opacity 250ms}.skin-vector .uls-search-wrapper:hover .uls-search-label{opacity:0.87}.skin-vector .uls-filtersuggestion{color:#72777d}.skin-vector .uls-lcd-region-title{color:#54595d}
.ext-quick-survey-panel,.ext-qs-loader-bar{width:auto;background-color:#eaecf0} .ext-qs-loader-bar{display:block;height:100px;margin-left:1.4em;clear:right;float:right;background-color:#eaecf0}.ext-qs-loader-bar.mw-ajax-loader{top:0}@media all and (min-width:720px){.ext-qs-loader-bar,.ext-quick-survey-panel{margin-left:1.4em;width:300px;clear:right;float:right}}
.cite-accessibility-label{ top:-99999px;clip:rect(1px,1px,1px,1px); position:absolute !important;padding:0 !important;border:0 !important;height:1px !important;width:1px !important; overflow:hidden}:target .mw-cite-targeted-backlink{font-weight:bold}.mw-cite-up-arrow-backlink{display:none}:target .mw-cite-up-arrow-backlink{display:inline}:target .mw-cite-up-arrow{display:none}
@media print{#centralNotice{display:none}}.cn-closeButton{display:inline-block;zoom:1;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUBAMAAAB/pwA+AAAAElBMVEUAAAAQEBDPz88AAABAQEDv7+9oe1vvAAAABnRSTlMA3rLe3rJS22KzAAAARElEQVQI12PAAUIUQCSTK5BwFgIxFU1AhKECUFAYKAAioXwwBeZChMGCEGGQIFQYJohgIhQgtCEMQ7ECYTHCOciOxA4AADgJTXIb9s8AAAAASUVORK5CYII=) no-repeat;width:20px;height:20px;text-indent:20px;white-space:nowrap;overflow:hidden}</style><style>
.suggestions a.mw-searchSuggest-link,.suggestions a.mw-searchSuggest-link:hover,.suggestions a.mw-searchSuggest-link:active,.suggestions a.mw-searchSuggest-link:focus{color:#000;text-decoration:none}.suggestions-result-current a.mw-searchSuggest-link,.suggestions-result-current a.mw-searchSuggest-link:hover,.suggestions-result-current a.mw-searchSuggest-link:active,.suggestions-result-current a.mw-searchSuggest-link:focus{color:#fff}.suggestions a.mw-searchSuggest-link .special-query{ overflow:hidden;text-overflow:ellipsis;white-space:nowrap}
.mw-mmv-overlay{position:fixed;top:0;left:0;right:0;bottom:0;z-index:1000;background-color:#000}body.mw-mmv-lightbox-open{overflow-y:auto;  }body.mw-mmv-lightbox-open #mw-page-base,body.mw-mmv-lightbox-open #mw-head-base,body.mw-mmv-lightbox-open #mw-navigation,body.mw-mmv-lightbox-open #content,body.mw-mmv-lightbox-open #footer,body.mw-mmv-lightbox-open #globalWrapper{ display:none}body.mw-mmv-lightbox-open > *{ display:none}body.mw-mmv-lightbox-open > .mw-mmv-overlay,body.mw-mmv-lightbox-open > .mw-mmv-wrapper{display:block}.mw-mmv-filepage-buttons{margin-top:5px}.mw-mmv-filepage-buttons .mw-mmv-view-expanded,.mw-mmv-filepage-buttons .mw-mmv-view-config{display:block;line-height:inherit}.mw-mmv-filepage-buttons .mw-mmv-view-expanded.mw-ui-icon:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 1024 768%22%3E %3Cpath d=%22M851.2 71.6L690.7 232.1l-40.1-40.3-9.6 164.8 164.8-9.3-40.3-40.4L926 146.4l58.5 58.5L997.6 0 792.7 13.1%22/%3E %3Cpath d=%22M769.6 89.3H611.9l70.9 70.8 7.9 7.5m-47.1 234.6l-51.2 3 3-51.2 9.4-164.4 5.8-100.3H26.4V768h883.1V387l-100.9 5.8-165 9.4zM813.9 678H113.6l207.2-270.2 31.5-12.9L548 599.8l105.9-63.2 159.8 140.8.2.6zm95.6-291.9V228l-79.1 78.9 7.8 7.9%22/%3E %3C/svg%3E")}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 1024 768%22%3E %3Cpath d=%22M897 454.6V313.4L810.4 299c-6.4-23.3-16-45.7-27.3-65.8l50.5-71.4-99.4-100.2-71.4 50.5c-20.9-11.2-42.5-20.9-65.8-27.3L582.6-1H441.4L427 85.6c-23.3 6.4-45.7 16-65.8 27.3l-71.4-50.5-100.3 99.5 50.5 71.4c-11.2 20.9-20.9 42.5-27.3 66.6L127 313.4v141.2l85.8 14.4c6.4 23.3 16 45.7 27.3 66.6L189.6 607l99.5 99.5 71.4-50.5c20.9 11.2 42.5 20.9 66.6 27.3l14.4 85.8h141.2l14.4-86.6c23.3-6.4 45.7-16 65.8-27.3l71.4 50.5 99.5-99.5-50.5-71.4c11.2-20.9 20.9-42.5 27.3-66.6l86.4-13.6zm-385 77c-81.8 0-147.6-66.6-147.6-147.6 0-81.8 66.6-147.6 147.6-147.6S659.6 302.2 659.6 384 593.8 531.6 512 531.6z%22/%3E %3C/svg%3E");opacity:0.75}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before:hover{opacity:1}.mw-mmv-button{background-color:transparent;min-width:0;border:0;padding:0;overflow-x:hidden;text-indent:-9999em}
.ve-init-mw-tempWikitextEditorWidget{border:0;padding:0;color:inherit;line-height:1.5em;width:100%; }.ve-init-mw-tempWikitextEditorWidget:focus{outline:0;padding:0}.ve-init-mw-tempWikitextEditorWidget::selection{background:rgba(109,169,247,0.5); }
#p-lang .body ul .uls-trigger,#p-lang .pBody ul .uls-trigger{background-image:none;padding:0} .mw-interlanguage-selector,.mw-interlanguage-selector:active{cursor:pointer;padding:4px 6px 4px 25px;font-size:13px;font-weight:normal;background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/compact-links-trigger.png?b0c8e);background-image:linear-gradient(transparent,transparent),url(/w/extensions/UniversalLanguageSelector/resources/images/compact-links-trigger.svg?ebb8e);background-size:18px;background-repeat:no-repeat;background-position:left 4px center;margin:4px 0;text-align:left}.mw-interlanguage-selector:active,.mw-interlanguage-selector.selector-open{background-color:#c8ccd1;color:#54595d}.interlanguage-uls-menu:before,.interlanguage-uls-menu:after{border-top:10px solid transparent;border-bottom:10px solid transparent;display:inline-block; top:17px;position:absolute;content:''}.interlanguage-uls-menu.selector-right:before{ border-left:10px solid #c8ccd1; right:-11px}.interlanguage-uls-menu.selector-right:after{ border-left:10px solid #fff; right:-10px}.interlanguage-uls-menu.selector-left:before{ border-right:10px solid #c8ccd1; left:-11px}.interlanguage-uls-menu.selector-left:after{ border-right:10px solid #fff; left:-10px}
#uls-settings-block{background-color:#f8f9fa;border-top:1px solid #c8ccd1;padding-left:10px;line-height:1.2em;border-radius:0 0 2px 2px}#uls-settings-block > button{background:left top transparent no-repeat;background-size:20px auto;color:#54595d;display:inline-block;margin:8px 15px;border:0;padding:0 0 0 26px;font-size:medium;cursor:pointer}#uls-settings-block > button:hover{color:#222}#uls-settings-block > button.display-settings-block{background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/display.png?d25f1);background-image:linear-gradient(transparent,transparent),url(/w/extensions/UniversalLanguageSelector/resources/images/display.svg?a28d5)}#uls-settings-block > button.input-settings-block{background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/input.png?aea9e);background-image:linear-gradient(transparent,transparent),url(/w/extensions/UniversalLanguageSelector/resources/images/input.svg?2ec7d)}</style><style>
.ve-activated .ve-init-mw-desktopArticleTarget-editableContent #toc,.ve-activated #siteNotice,.ve-activated .mw-indicators,.ve-activated #t-print,.ve-activated #t-permalink,.ve-activated #p-coll-print_export,.ve-activated #t-cite,.ve-deactivating .ve-ui-surface,.ve-active .ve-init-mw-desktopArticleTarget-editableContent,.ve-active .ve-init-mw-tempWikitextEditorWidget{display:none} .ve-activating .ve-ui-surface{height:0;padding:0 !important; overflow:hidden} .ve-loading #content > :not(.ve-init-mw-desktopArticleTarget-loading-overlay), .ve-activated .ve-init-mw-desktopArticleTarget-uneditableContent{pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;opacity:0.5}.ve-activated #firstHeading{ -webkit-user-select:text;-moz-user-select:text;-ms-user-select:text;user-select:text;pointer-events:auto;cursor:text}.ve-activated #firstHeading a{ pointer-events:none}.ve-activated #catlinks{cursor:pointer}.ve-activated #catlinks a{opacity:1}.ve-activated #content{position:relative} .ve-init-mw-desktopArticleTarget-loading-overlay{position:absolute;top:1.25em;left:0;right:0;z-index:1;margin-top:-0.5em}.ve-init-mw-desktopArticleTarget-toolbarPlaceholder{transition:height 250ms ease;height:0; } .oo-ui-element-hidden{display:none !important; } .mw-editsection{white-space:nowrap; unicode-bidi:-moz-isolate;unicode-bidi:-webkit-isolate;unicode-bidi:isolate}.mw-editsection-divider{color:#54595d} .ve-init-mw-desktopArticleTarget-toolbarPlaceholder{border-bottom:1px solid #c8ccd1;box-shadow:0 1px 1px 0 rgba(0,0,0,0.1)}.ve-init-mw-desktopArticleTarget-toolbarPlaceholder-open{height:42px} .ve-init-mw-desktopArticleTarget-toolbar,.ve-init-mw-desktopArticleTarget-toolbarPlaceholder{font-size:0.875em; margin:-1.14em -1.14em 1.14em -1.14em; }@media screen and (min-width:982px){.ve-init-mw-desktopArticleTarget-toolbar,.ve-init-mw-desktopArticleTarget-toolbarPlaceholder{ margin:-1.43em -1.71em 1.43em -1.71em}}</style><style>
.mw-ui-icon-popups-settings:before{background-image:url(/w/load.php?modules=ext.popups.images&image=popups-settings&format=rasterized&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Cg fill=%22%2354595d%22%3E %3Cpath d=%22M10.112 4.554a5.334 5.334 0 1 0 0 10.668 5.334 5.334 0 0 0 0-10.668zm0 7.823a2.49 2.49 0 1 1 0-4.978 2.49 2.49 0 0 1 0 4.978z%22/%3E %3Cpath d=%22M11.4 5.303L11.05 3h-2.1L8.6 5.303a4.9 4.9 0 0 1 2.8 0zm-2.8 9.394L8.95 17h2.1l.35-2.303a4.9 4.9 0 0 1-2.8 0zm5.712-7.028l1.4-1.876L14.2 4.309l-1.876 1.4a4.9 4.9 0 0 1 1.981 1.981l.007-.021zm-8.624 4.662L4.309 14.2 5.8 15.691l1.876-1.4a4.9 4.9 0 0 1-1.981-1.981l-.007.021zm9.009-.931L17 11.05v-2.1l-2.303-.35a4.9 4.9 0 0 1 0 2.8zM5.303 8.6L3 8.95v2.1l2.303.35a4.9 4.9 0 0 1 0-2.8zm7.028 5.712l1.876 1.4 1.484-1.512-1.4-1.876a4.9 4.9 0 0 1-1.981 1.981l.021.007zM7.669 5.688L5.8 4.309 4.309 5.8l1.4 1.876a4.9 4.9 0 0 1 1.96-1.988z%22/%3E %3C/g%3E %3C/svg%3E")}.mw-ui-icon-popups-close:before{background-image:url(/w/load.php?modules=ext.popups.images&image=popups-close&format=rasterized&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Ctitle%3E close %3C/title%3E %3Cpath d=%22M4.34 2.93l12.73 12.73-1.41 1.41L2.93 4.35z%22/%3E %3Cpath d=%22M17.07 4.34L4.34 17.07l-1.41-1.41L15.66 2.93z%22/%3E %3C/svg%3E")}.mw-ui-icon-preview-generic:before{background-image:url(/w/load.php?modules=ext.popups.images&image=preview-generic&format=rasterized&lang=en&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Ctitle%3E sad face %3C/title%3E %3Cpath d=%22M2 0a2 2 0 0 0-2 2v18l4-4h14a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H2zm4 4c1.336 0 2.007 1.617 1.06 2.56-.943.947-2.56.276-2.56-1.06A1.5 1.5 0 0 1 6 4zm8 0c1.336 0 2.007 1.617 1.06 2.56-.943.947-2.56.276-2.56-1.06A1.5 1.5 0 0 1 14 4zm-4 5c2.61 0 4.83.67 5.65 3H4.35C5.17 9.67 7.39 9 10 9z%22/%3E %3C/svg%3E")}.mw-ui-icon-footer:before{background-image:url(/w/load.php?modules=ext.popups.images&image=footer&format=rasterized&lang=en&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 xmlns:xlink=%22http://www.w3.org/1999/xlink%22 width=%22230%22 height=%22179%22 viewBox=%220 0 230 179%22%3E %3Cdefs%3E %3Crect id=%22a%22 width=%22201%22 height=%2213%22 rx=%222%22/%3E %3Crect id=%22b%22 width=%22201%22 height=%22169%22 y=%2210%22 rx=%222%22/%3E %3Crect id=%22c%22 width=%2230%22 height=%222%22 x=%22135%22 y=%22158%22 rx=%221%22/%3E %3C/defs%3E %3Cg fill=%22none%22 fill-rule=%22evenodd%22%3E %3Cg transform=%22matrix%281 0 0 -1 0 13%29%22%3E %3Cuse fill=%22%23f8f9fa%22 xlink:href=%22%23a%22/%3E %3Crect width=%22199%22 height=%2211%22 x=%221%22 y=%221%22 stroke=%22%23a2a9b1%22 stroke-width=%222%22 rx=%222%22/%3E %3C/g%3E %3Cuse fill=%22%23fff%22 xlink:href=%22%23b%22/%3E %3Crect width=%22199%22 height=%22167%22 x=%221%22 y=%2211%22 stroke=%22%23a2a9b1%22 stroke-width=%222%22 rx=%222%22/%3E %3Cg opacity=%22.4%22 fill=%22%2372777d%22 transform=%22translate%2867 35%29%22%3E %3Crect width=%2273%22 height=%222%22 y=%227%22 fill=%22%23c8ccd1%22 rx=%221%22/%3E %3Crect width=%2281%22 height=%222%22 y=%2231%22 rx=%221%22/%3E %3Crect width=%2232%22 height=%222%22 y=%2285%22 rx=%221%22/%3E %3Crect width=%2273%22 height=%222%22 x=%2235%22 y=%2285%22 rx=%221%22/%3E %3Crect width=%2217%22 height=%222%22 y=%2245%22 rx=%221%22/%3E %3Crect width=%2217%22 height=%222%22 x=%2291%22 y=%2245%22 rx=%221%22/%3E %3Crect width=%2268%22 height=%222%22 x=%2220%22 y=%2245%22 rx=%221%22/%3E %3Crect width=%2217%22 height=%222%22 y=%2278%22 rx=%221%22/%3E %3Crect width=%2237%22 height=%222%22 x=%2272%22 y=%2278%22 rx=%221%22/%3E %3Crect width=%2249%22 height=%222%22 x=%2220%22 y=%2278%22 rx=%221%22/%3E %3Crect width=%2224%22 height=%222%22 x=%2284%22 y=%2231%22 rx=%221%22 transform=%22matrix%28-1 0 0 1 192 0%29%22/%3E %3Crect width=%2281%22 height=%222%22 y=%2266%22 rx=%221%22/%3E %3Crect width=%2214%22 height=%222%22 x=%2254%22 y=%2224%22 rx=%221%22/%3E %3Crect width=%2237%22 height=%222%22 x=%2271%22 y=%2224%22 rx=%221%22/%3E %3Crect width=%2251%22 height=%222%22 y=%2224%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2259%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2252%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2292%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2238%22 rx=%221%22/%3E %3Crect width=%2251%22 height=%222%22 rx=%221%22/%3E %3C/g%3E %3Crect width=%2230%22 height=%222%22 x=%2267%22 y=%22158%22 fill=%22%2372777d%22 opacity=%22.4%22 rx=%221%22/%3E %3Crect width=%2230%22 height=%222%22 x=%2299%22 y=%22158%22 fill=%22%2372777d%22 opacity=%22.4%22 rx=%221%22/%3E %3Cuse fill=%22%2336c%22 xlink:href=%22%23c%22/%3E %3Crect width=%2233%22 height=%225%22 x=%22133.5%22 y=%22156.5%22 stroke=%22%23ffc057%22 stroke-opacity=%22.447%22 stroke-width=%223%22 rx=%222.5%22/%3E %3Ccircle cx=%2234%22 cy=%2249%22 r=%2219%22 fill=%22%23eaecf0%22/%3E %3Cg fill=%22%23a2a9b1%22 transform=%22translate%285 5%29%22%3E %3Ccircle cx=%221.5%22 cy=%221.5%22 r=%221.5%22/%3E %3Ccircle cx=%226%22 cy=%221.5%22 r=%221.5%22/%3E %3Ccircle cx=%2210.5%22 cy=%221.5%22 r=%221.5%22/%3E %3C/g%3E %3Cpath stroke=%22%23ff00af%22 d=%22M174.5 159.5h54.01%22 stroke-linecap=%22square%22/%3E %3C/g%3E %3C/svg%3E")}.mw-ui-icon-preview-disambiguation:before{background-image:url(/w/load.php?modules=ext.popups.images&image=preview-disambiguation&format=rasterized&lang=en&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Ctitle%3E articles %3C/title%3E %3Cpath d=%22M5 0v2h11v14h2V2a2 2 0 0 0-2-2z%22/%3E %3Cpath d=%22M13 20a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2H4a2 2 0 0 0-2 2v13a2 2 0 0 0 2 2zM9 5h4v5H9zM4 5h4v1H4zm0 2h4v1H4zm0 2h4v1H4zm0 2h9v1H4zm0 2h9v1H4zm0 2h9v1H4z%22/%3E %3C/svg%3E")}.mw-ui-icon-reference-generic:before{background-image:url(/w/load.php?modules=ext.popups.images&image=reference-generic&format=rasterized&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Ctitle%3E reference %3C/title%3E %3Cpath d=%22M15 10l-2.78-2.78L9.44 10V1H5a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3a2 2 0 0 0-2-2z%22/%3E %3C/svg%3E")}.mw-ui-icon-reference-book:before{background-image:url(/w/load.php?modules=ext.popups.images&image=reference-book&format=rasterized&lang=en&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Ctitle%3E book %3C/title%3E %3Cpath d=%22M15 2a7.65 7.65 0 0 0-5 2 7.65 7.65 0 0 0-5-2H1v15h4a7.65 7.65 0 0 1 5 2 7.65 7.65 0 0 1 5-2h4V2zm2.5 13.5H14a4.38 4.38 0 0 0-3 1V5s1-1.5 4-1.5h2.5z%22/%3E %3Cpath d=%22M9 3.5h2v1H9z%22/%3E %3C/svg%3E")}.mw-ui-icon-reference-journal:before{background-image:url(/w/load.php?modules=ext.popups.images&image=reference-journal&format=rasterized&lang=en&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Ctitle%3E journal %3C/title%3E %3Cpath d=%22M2 18.5A1.5 1.5 0 0 0 3.5 20H5V0H3.5A1.5 1.5 0 0 0 2 1.5zM6 0v20h10a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2zm7 8H8V7h5zm3-2H8V5h8z%22/%3E %3C/svg%3E")}.mw-ui-icon-reference-news:before{background-image:url(/w/load.php?modules=ext.popups.images&image=reference-news&format=rasterized&lang=en&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Ctitle%3E newspaper %3C/title%3E %3Cpath d=%22M5 2a2 2 0 0 0-2 2v12a1 1 0 0 1-1-1V5h-.5A1.5 1.5 0 0 0 0 6.5v10A1.5 1.5 0 0 0 1.5 18H18a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2zm1 2h11v4H6zm0 6h6v1H6zm0 2h6v1H6zm0 2h6v1H6zm7-4h4v5h-4z%22/%3E %3C/svg%3E")}.mw-ui-icon-reference-web:before{background-image:url(/w/load.php?modules=ext.popups.images&image=reference-web&format=rasterized&lang=en&skin=vector&version=wti3j);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Ctitle%3E browser %3C/title%3E %3Cpath d=%22M2 2a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2zm2 1.5A1.5 1.5 0 1 1 2.5 5 1.5 1.5 0 0 1 4 3.5zM18 16H2V8h16z%22/%3E %3C/svg%3E")}</style><style>
@-webkit-keyframes mwe-popups-fade-in-up{0%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@-moz-keyframes mwe-popups-fade-in-up{0%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@keyframes mwe-popups-fade-in-up{0%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@-webkit-keyframes mwe-popups-fade-in-down{0%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@-moz-keyframes mwe-popups-fade-in-down{0%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@keyframes mwe-popups-fade-in-down{0%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@-webkit-keyframes mwe-popups-fade-out-down{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}}@-moz-keyframes mwe-popups-fade-out-down{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}}@keyframes mwe-popups-fade-out-down{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}}@-webkit-keyframes mwe-popups-fade-out-up{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}}@-moz-keyframes mwe-popups-fade-out-up{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}}@keyframes mwe-popups-fade-out-up{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}}.mwe-popups-fade-in-up{-webkit-animation:mwe-popups-fade-in-up 0.2s ease forwards;-moz-animation:mwe-popups-fade-in-up 0.2s ease forwards;animation:mwe-popups-fade-in-up 0.2s ease forwards}.mwe-popups-fade-in-down{-webkit-animation:mwe-popups-fade-in-down 0.2s ease forwards;-moz-animation:mwe-popups-fade-in-down 0.2s ease forwards;animation:mwe-popups-fade-in-down 0.2s ease forwards}.mwe-popups-fade-out-down{-webkit-animation:mwe-popups-fade-out-down 0.2s ease forwards;-moz-animation:mwe-popups-fade-out-down 0.2s ease forwards;animation:mwe-popups-fade-out-down 0.2s ease forwards}.mwe-popups-fade-out-up{-webkit-animation:mwe-popups-fade-out-up 0.2s ease forwards;-moz-animation:mwe-popups-fade-out-up 0.2s ease forwards;animation:mwe-popups-fade-out-up 0.2s ease forwards}   #mwe-popups-settings{z-index:1000;background:#fff;width:420px;border:1px solid #a2a9b1;box-shadow:0 2px 2px 0 rgba(0,0,0,0.25);border-radius:2px;font-size:14px}#mwe-popups-settings header{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;border-bottom:1px solid #c8ccd1;position:relative;display:table;width:100%;padding:5px 7px 5px 0}#mwe-popups-settings header > div{display:table-cell;width:3.5em;vertical-align:middle;cursor:pointer}#mwe-popups-settings header h1{margin-bottom:0.6em;padding-top:0.5em;border:0;width:100%;font-family:sans-serif;font-size:18px;font-weight:bold;text-align:center}#mwe-popups-settings .mwe-ui-icon-popups-close{opacity:0.87;-webkit-transition:opacity 100ms;-moz-transition:opacity 100ms;transition:opacity 100ms}#mwe-popups-settings .mwe-ui-icon-popups-close:hover{opacity:0.73}#mwe-popups-settings .mwe-ui-icon-popups-close:active{opacity:1}#mwe-popups-settings main{display:block;width:350px;padding:32px 0 24px;margin:0 auto}#mwe-popups-settings main p{color:#54595d;font-size:17px;margin:16px 0 0}#mwe-popups-settings main p:first-child{margin-top:0}#mwe-popups-settings main form img,#mwe-popups-settings main form input,#mwe-popups-settings main form label{vertical-align:top}#mwe-popups-settings main form img{margin-right:60px}#mwe-popups-settings main form input{display:inline-block;margin:0 10px 0 0;padding:0}#mwe-popups-settings main form label{font-size:13px;display:inline-block;line-height:16px;width:300px}#mwe-popups-settings main form label > span{color:#000;font-size:18px;font-weight:bold;display:block;margin-bottom:5px;line-height:18px}.mwe-popups-settings-help{font-size:13px;font-weight:800;margin:40px;position:relative}.mwe-popups-settings-help .mw-ui-icon:before,.mwe-popups-settings-help .mw-ui-icon{height:140px;width:180px;max-width:none;margin:0}.mwe-popups-settings-help p{left:180px;bottom:20px;position:absolute}.mwe-popups{background:#fff;position:absolute;z-index:110;-webkit-box-shadow:0 30px 90px -20px rgba(0,0,0,0.3),0 0 1px 1px rgba(0,0,0,0.05);box-shadow:0 30px 90px -20px rgba(0,0,0,0.3),0 0 1px 1px rgba(0,0,0,0.05);padding:0;display:none;font-size:14px;line-height:20px;min-width:300px;border-radius:2px; }.mwe-popups .mw-ui-icon{font-size:16px}.mwe-popups .mw-ui-icon:before{background-size:20px;min-height:20px}.mwe-popups .mw-ui-icon-preview-disambiguation,.mwe-popups .mw-ui-icon-preview-generic{margin:21px 0 8px 0;opacity:0.25}.mwe-popups .mwe-popups-container{color:#222222;margin-top:-9px;padding-top:9px;text-decoration:none}.mwe-popups .mwe-popups-container footer{padding:16px;margin:0;font-size:10px;position:absolute;bottom:0;left:0}.mwe-popups .mwe-popups-extract{margin:16px;display:block;color:#222222;text-decoration:none;position:relative;   }.mwe-popups .mwe-popups-extract:hover{text-decoration:none}.mwe-popups .mwe-popups-extract:after{content:' ';position:absolute;bottom:0;width:25%;height:20px;background-color:transparent;pointer-events:none}.mwe-popups .mwe-popups-extract[dir='ltr']:after{ right:0; background-image:-webkit-linear-gradient(to right,rgba(255,255,255,0),#ffffff 50%); background-image:-moz-linear-gradient(to right,rgba(255,255,255,0),#ffffff 50%); background-image:linear-gradient(to right,rgba(255,255,255,0),#ffffff 50%)}.mwe-popups .mwe-popups-extract[dir='rtl']:after{ left:0; background-image:-webkit-linear-gradient(to left,rgba(255,255,255,0),#ffffff 50%); background-image:-moz-linear-gradient(to left,rgba(255,255,255,0),#ffffff 50%); background-image:linear-gradient(to left,rgba(255,255,255,0),#ffffff 50%)}.mwe-popups .mwe-popups-extract p{margin:0}.mwe-popups .mwe-popups-extract ul,.mwe-popups .mwe-popups-extract ol,.mwe-popups .mwe-popups-extract li,.mwe-popups .mwe-popups-extract dl,.mwe-popups .mwe-popups-extract dd,.mwe-popups .mwe-popups-extract dt{margin-top:0;margin-bottom:0}.mwe-popups svg{overflow:hidden}.mwe-popups.mwe-popups-is-tall{width:450px}.mwe-popups.mwe-popups-is-tall > div > a > svg{vertical-align:middle}.mwe-popups.mwe-popups-is-tall .mwe-popups-extract{width:215px;height:180px;overflow:hidden;float:left}.mwe-popups.mwe-popups-is-tall footer{width:215px;left:0}.mwe-popups.mwe-popups-is-not-tall{width:320px}.mwe-popups.mwe-popups-is-not-tall .mwe-popups-extract{min-height:40px;max-height:140px;overflow:hidden;margin-bottom:47px;padding-bottom:0}.mwe-popups.mwe-popups-is-not-tall footer{width:290px}.mwe-popups.mwe-popups-type-generic .mwe-popups-extract,.mwe-popups.mwe-popups-type-disambiguation .mwe-popups-extract{min-height:auto;padding-top:4px;margin-bottom:60px;margin-top:0}.mwe-popups.mwe-popups-type-generic .mwe-popups-read-link,.mwe-popups.mwe-popups-type-disambiguation .mwe-popups-read-link{font-weight:bold;font-size:12px}.mwe-popups.mwe-popups-type-generic .mwe-popups-extract:hover + footer .mwe-popups-read-link,.mwe-popups.mwe-popups-type-disambiguation .mwe-popups-extract:hover + footer .mwe-popups-read-link{text-decoration:underline}.mwe-popups.mwe-popups-no-image-pointer:before{content:'';position:absolute;border:8px solid transparent;border-top:0;border-bottom:8px solid rgba(0,0,0,0.07000000000000001);top:-8px;left:10px}.mwe-popups.mwe-popups-no-image-pointer:after{content:'';position:absolute;border:11px solid transparent;border-top:0;border-bottom:11px solid #ffffff;top:-7px;left:7px}.mwe-popups.flipped-x.mwe-popups-no-image-pointer:before{left:auto;right:10px}.mwe-popups.flipped-x.mwe-popups-no-image-pointer:after{left:auto;right:7px}.mwe-popups.mwe-popups-image-pointer:before{content:'';position:absolute;border:9px solid transparent;border-top:0;border-bottom:9px solid #a2a9b1;top:-9px;left:9px;z-index:111}.mwe-popups.mwe-popups-image-pointer:after{content:'';position:absolute;border:12px solid transparent;border-top:0;border-bottom:12px solid #ffffff;top:-8px;left:6px;z-index:112}.mwe-popups.mwe-popups-image-pointer.flipped-x:before{content:'';position:absolute;border:9px solid transparent;border-top:0;border-bottom:9px solid #a2a9b1;top:-9px;left:293px}.mwe-popups.mwe-popups-image-pointer.flipped-x:after{content:'';position:absolute;border:12px solid transparent;border-top:0;border-bottom:12px solid #ffffff;top:-8px;left:290px}.mwe-popups.mwe-popups-image-pointer .mwe-popups-extract{padding-top:16px;margin-top:200px}.mwe-popups.mwe-popups-image-pointer > div > a > svg{margin-top:-8px;position:absolute;z-index:113;left:0}.mwe-popups.flipped-x.mwe-popups-is-tall{min-height:242px}.mwe-popups.flipped-x.mwe-popups-is-tall:before{content:'';position:absolute;border:9px solid transparent;border-top:0;border-bottom:9px solid #a2a9b1;top:-9px;left:420px;z-index:111}.mwe-popups.flipped-x.mwe-popups-is-tall > div > a > svg{margin:0;margin-top:-8px;margin-bottom:-7px;position:absolute;z-index:113;right:0}.mwe-popups.flipped-x-y:before{content:'';position:absolute;border:9px solid transparent;border-bottom:0;border-top:9px solid #a2a9b1;bottom:-9px;left:293px;z-index:111}.mwe-popups.flipped-x-y:after{content:'';position:absolute;border:12px solid transparent;border-bottom:0;border-top:12px solid #ffffff;bottom:-8px;left:290px;z-index:112}.mwe-popups.flipped-x-y.mwe-popups-is-tall{min-height:242px}.mwe-popups.flipped-x-y.mwe-popups-is-tall:before{content:'';position:absolute;border:9px solid transparent;border-bottom:0;border-top:9px solid #a2a9b1;bottom:-9px;left:420px}.mwe-popups.flipped-x-y.mwe-popups-is-tall:after{content:'';position:absolute;border:12px solid transparent;border-bottom:0;border-top:12px solid #ffffff;bottom:-8px;left:417px}.mwe-popups.flipped-x-y.mwe-popups-is-tall > div > a > svg{margin:0;margin-bottom:-9px;position:absolute;z-index:113;right:0}.mwe-popups.flipped-y:before{content:'';position:absolute;border:8px solid transparent;border-bottom:0;border-top:8px solid #a2a9b1;bottom:-8px;left:10px}.mwe-popups.flipped-y:after{content:'';position:absolute;border:11px solid transparent;border-bottom:0;border-top:11px solid #ffffff;bottom:-7px;left:7px}.mwe-popups-is-tall polyline{-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}.mwe-popups-is-tall.flipped-x-y polyline{-webkit-transform:translate(0,-8px);-moz-transform:translate(0,-8px);-ms-transform:translate(0,-8px);transform:translate(0,-8px)}.mwe-popups-is-tall.flipped-x polyline{-webkit-transform:translate(0,8px);-moz-transform:translate(0,8px);-ms-transform:translate(0,8px);transform:translate(0,8px)}.rtl .mwe-popups-is-tall polyline{-webkit-transform:translate(-100%,0);-moz-transform:translate(-100%,0);-ms-transform:translate(-100%,0);transform:translate(-100%,0)}.rtl .mwe-popups-is-tall.flipped-x-y polyline{-webkit-transform:translate(-100%,-8px);-moz-transform:translate(-100%,-8px);-ms-transform:translate(-100%,-8px);transform:translate(-100%,-8px)}.rtl .mwe-popups-is-tall.flipped-x polyline{-webkit-transform:translate(-100%,8px);-moz-transform:translate(-100%,8px);-ms-transform:translate(-100%,8px);transform:translate(-100%,8px)}.mwe-popups-settings-icon{display:block;overflow:hidden;font-size:16px;width:1.5em;height:1.5em;padding:3px;float:right;margin:4px 4px 2px 4px;text-indent:-1em;border-radius:2px}.mwe-popups-settings-icon:hover{background-color:#eaecf0}.mwe-popups-settings-icon:active{background-color:#c8ccd1}.mwe-popups .mwe-popups-title{display:block;font-weight:bold;margin:0 16px}#mw-content-text .reference a[href*='#'] *{pointer-events:none}.mwe-popups.mwe-popups-type-reference .mwe-popups-title{margin-top:16px}.mwe-popups.mwe-popups-type-reference .mwe-popups-title .mw-ui-icon{vertical-align:middle}.mwe-popups.mwe-popups-type-reference .mwe-popups-title .mw-ui-icon.mw-ui-icon-element{min-width:1.5em;width:1.5em}.mwe-popups.mwe-popups-type-reference .mwe-popups-title .mw-ui-icon.mw-ui-icon-element:before{margin:0}.mwe-popups.mwe-popups-type-reference .mwe-popups-title .mw-ui-icon.mw-ui-icon-reference-generic{ margin-left:-2px}.mwe-popups.mwe-popups-type-reference .mwe-popups-extract{margin-bottom:55px;max-height:100px;min-height:20px}.mwe-popups.mwe-popups-type-reference .mwe-popups-extract:after{display:none}.mwe-popups.mwe-popups-type-reference .mwe-popups-extract .mwe-popups-fade{position:absolute;width:100%;height:20px;background-color:transparent;background-image:-webkit-linear-gradient(top,rgba(255,255,255,0),#ffffff);background-image:-moz-linear-gradient(top,rgba(255,255,255,0),#ffffff);background-image:linear-gradient(rgba(255,255,255,0),#ffffff);opacity:0;pointer-events:none;-webkit-transition:opacity 250ms ease;-moz-transition:opacity 250ms ease;transition:opacity 250ms ease}.mwe-popups.mwe-popups-type-reference .mwe-popups-extract.mwe-popups-fade-out .mwe-popups-fade{opacity:1}.mwe-popups.mwe-popups-type-reference .mwe-popups-extract .mw-parser-output{max-height:inherit;overflow:auto}.mwe-popups.mwe-popups-type-reference .mwe-popups-read-link{font-size:12px}.mwe-popups-overlay{background-color:rgba(255,255,255,0.9);z-index:999;position:fixed;height:100%;width:100%;top:0;bottom:0;left:0;right:0;display:flex;justify-content:center;align-items:center}#mwe-popups-svg{position:absolute;top:-1000px}</style><style>
.tipsy{padding:5px;position:absolute;z-index:100000;cursor:default}.tipsy-inner{padding:5px 8px 4px 8px; background-color:#fff;border:solid 1px #a7d7f9;color:#000;max-width:15em;border-radius:4px; }.tipsy-arrow{position:absolute;background:url(/w/resources/src/jquery.tipsy/images/tipsy.png?e5f3a) no-repeat top left;width:11px;height:6px} .tipsy-n .tipsy-arrow{top:0;left:50%;margin-left:-5px} .tipsy-nw .tipsy-arrow{top:0;left:10px} .tipsy-ne .tipsy-arrow{top:0;right:10px} .tipsy-s .tipsy-arrow{bottom:0;left:50%;margin-left:-5px;background-position:bottom left} .tipsy-sw .tipsy-arrow{bottom:0;left:10px;background-position:bottom left} .tipsy-se .tipsy-arrow{bottom:0;right:10px;background-position:bottom left} .tipsy-e .tipsy-arrow{top:50%;margin-top:-5px;right:0;width:6px;height:11px;background-position:top right} .tipsy-w .tipsy-arrow{top:50%;margin-top:-5px;left:0;width:6px;height:11px} .tipsy{font-size:0.8em}</style><style>
.mw-mmv-dialog{position:fixed;right:58px;display:none;width:450px;height:350px;background-color:#ffffff;box-shadow:0 2px 2px 0 #aaaaaa;border-radius:2px;z-index:1004}.mw-mmv-dialog .mw-mmv-dialog-down-arrow{right:48px;background-color:#ffffff;width:20px;height:20px;-webkit-transform:rotate(-45deg);-moz-transform:rotate(-45deg);transform:rotate(-45deg);position:fixed}.mw-mmv-dialog .mw-mmv-dialog-copy{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2218.244%22 height=%2222.457%22 viewBox=%220 0 18.244 22.457%22%3E %3Cg fill=%22%2372777d%22 fill-rule=%22evenodd%22%3E %3Cpath d=%22M2.773 1.936C1.21 2.237 0 3.612 0 5.256v13.799c0 1.86 1.54 3.402 3.4 3.402h11.444c1.86 0 3.4-1.542 3.4-3.402v-13.8c0-1.643-1.21-3.018-2.773-3.32.129.332.207.687.207 1.06v.175c0 .591-.184 1.145-.49 1.613a.555.555 0 01.253.473v13.799c0 .355-.241.597-.597.597H3.4c-.355 0-.597-.242-.597-.597v-13.8c0-.214.105-.369.254-.472a2.938 2.938 0 01-.49-1.613v-.176c0-.372.078-.728.206-1.058z%22/%3E %3Cpath d=%22M8.096 0c-.831 0-1.5.669-1.5 1.5v.004h-1.04c-.824 0-1.49.665-1.49 1.49v.176c0 .825.666 1.488 1.49 1.488h7.133c.825 0 1.489-.663 1.489-1.488v-.176c0-.825-.664-1.49-1.489-1.49H11.65V1.5c0-.831-.669-1.5-1.5-1.5z%22/%3E %3C/g%3E %3C/svg%3E");background-size:contain;background-position:right center;background-repeat:no-repeat;cursor:pointer}.mw-mmv-dialog .mw-mmv-dialog-copy:hover{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2218.244%22 height=%2222.457%22 viewBox=%220 0 18.244 22.457%22%3E %3Cg fill=%22%2336c%22 fill-rule=%22evenodd%22%3E %3Cpath d=%22M2.773 1.936C1.21 2.237 0 3.612 0 5.256v13.799c0 1.86 1.54 3.402 3.4 3.402h11.444c1.86 0 3.4-1.542 3.4-3.402v-13.8c0-1.643-1.21-3.018-2.773-3.32.129.332.207.687.207 1.06v.175c0 .591-.184 1.145-.49 1.613a.555.555 0 01.253.473v13.799c0 .355-.241.597-.597.597H3.4c-.355 0-.597-.242-.597-.597v-13.8c0-.214.105-.369.254-.472a2.938 2.938 0 01-.49-1.613v-.176c0-.372.078-.728.206-1.058z%22/%3E %3Cpath d=%22M8.096 0c-.831 0-1.5.669-1.5 1.5v.004h-1.04c-.824 0-1.49.665-1.49 1.49v.176c0 .825.666 1.488 1.49 1.488h7.133c.825 0 1.489-.663 1.489-1.488v-.176c0-.825-.664-1.49-1.489-1.49H11.65V1.5c0-.831-.669-1.5-1.5-1.5z%22/%3E %3C/g%3E %3C/svg%3E");text-decoration:none}.mw-mmv-dialog .mw-mmv-dialog-warning{background-color:#ffd36e}  .mw-mmv-reuse-dialog{height:initial;min-height:300px;bottom:95px}.mw-mmv-reuse-dialog .mw-mmv-reuse-tabs{position:static;box-shadow:none;padding-bottom:0;border:0;border-bottom:1px solid #c8ccd1;border-radius:2px 2px 0 0}.mw-mmv-reuse-dialog .mw-mmv-reuse-tabs .oo-ui-iconedElement-icon.oo-ui-icon-check,.mw-mmv-reuse-dialog .mw-mmv-reuse-tabs .oo-ui-iconElement-icon.oo-ui-icon-check{display:none}.mw-mmv-reuse-dialog .mw-mmv-reuse-tabs .oo-ui-optionWidget{display:inline-block;padding:10px 25px;font-size:1.2em}.mw-mmv-reuse-dialog .mw-mmv-reuse-tabs .oo-ui-optionWidget.oo-ui-optionWidget-highlighted{border-bottom:2px solid #e1f3ff}.mw-mmv-reuse-dialog .mw-mmv-reuse-tabs .oo-ui-optionWidget.oo-ui-optionWidget-selected,.mw-mmv-reuse-dialog .mw-mmv-reuse-tabs .oo-ui-optionWidget.oo-ui-optionWidget-highlighted.oo-ui-optionWidget-selected{border-bottom:3px solid #2a4b8d}.mw-mmv-reuse-dialog .mw-mmv-reuse-tabs .oo-ui-optionWidget:first-child{border-radius:2px 0 0 0}.mw-mmv-reuse-dialog .mw-mmv-reuse-pane{display:none;padding:20px}.mw-mmv-reuse-dialog .mw-mmv-reuse-pane.active{display:block}.mw-mmv-reuse-dialog .mw-mmv-dialog-down-arrow{bottom:105px}.mw-mmv-reuse-dialog .mw-mmv-dialog-warning{padding:15px 30px}  .mw-mmv-download-dialog{height:initial;position:fixed;bottom:135px}.mw-mmv-download-dialog .mw-mmv-download-size .oo-ui-optionWidget{display:inline-block;padding:10px 25px;font-size:16px}.mw-mmv-download-dialog .mw-mmv-download-size .oo-ui-optionWidget.oo-ui-optionWidget-selected{border:0}.mw-mmv-download-dialog .mw-mmv-download-size .oo-ui-optionWidget:first-child{border-radius:2px 0 0 0}.mw-mmv-download-dialog .mw-mmv-dialog-down-arrow{bottom:145px;background-color:#f8f9fa}.mw-mmv-download-dialog .mw-mmv-dialog-warning{padding:10px 30px}  .mw-mmv-options-dialog{top:85px;height:auto;z-index:1004;padding:15px}.mw-mmv-options-dialog.mw-mmv-enable-confirmation-shown{background-color:#00af89;box-shadow:0 2px 0 #00634e}.mw-mmv-options-dialog.mw-mmv-enable-confirmation-shown .mw-mmv-dialog-down-arrow{background-color:#00af89}.mw-mmv-options-dialog.mw-mmv-disable-confirmation-shown,.mw-mmv-options-dialog.mw-mmv-disable-confirmation-shown .mw-mmv-dialog-down-arrow,.mw-mmv-options-dialog.mw-mmv-enable-div-shown .mw-mmv-dialog-down-arrow{background-color:#eaecf0}.mw-mmv-options-dialog .mw-mmv-dialog-down-arrow{top:95px}.mw-mmv-options-dialog .mw-mmv-enable-confirmation,.mw-mmv-options-dialog .mw-mmv-disable-confirmation,.mw-mmv-options-dialog .mw-mmv-options-enable,.mw-mmv-options-dialog .mw-mmv-options-disable{position:relative;display:none}.mw-mmv-options-dialog .mw-mmv-enable-confirmation.mw-mmv-shown,.mw-mmv-options-dialog .mw-mmv-disable-confirmation.mw-mmv-shown,.mw-mmv-options-dialog .mw-mmv-options-enable.mw-mmv-shown,.mw-mmv-options-dialog .mw-mmv-options-disable.mw-mmv-shown{display:block}.mw-mmv-options-dialog .mw-mmv-confirmation-close{display:inline-block;position:absolute;right:0;top:0;width:15px;height:15px;cursor:pointer;opacity:0.75;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2296%22 height=%2296%22 viewBox=%220 0 96 96%22%3E %3Cpath fill=%22%23fff%22 d=%22M96 14L82 0 48 34 14 0 0 14l34 34L0 82l14 14 34-34 34 34 14-14-34-34z%22/%3E %3C/svg%3E");background-size:15px 15px}.mw-mmv-options-dialog .mw-mmv-confirmation-close:hover{opacity:1}.mw-mmv-options-dialog .mw-mmv-disable-confirmation .mw-mmv-options-dialog-header{color:#222;padding:0}.mw-mmv-options-dialog .mw-mmv-disable-confirmation .mw-mmv-options-text-header{color:#222}.mw-mmv-options-dialog .mw-mmv-enable-confirmation .mw-mmv-options-dialog-header{color:#fff;padding:0}.mw-mmv-options-dialog .mw-mmv-enable-confirmation .mw-mmv-options-text-header{color:#fff}.mw-mmv-options-dialog .mw-mmv-disable-confirmation,.mw-mmv-options-dialog .mw-mmv-enable-confirmation{padding:0}.mw-mmv-options-dialog .mw-mmv-disable-confirmation .mw-mmv-options-text,.mw-mmv-options-dialog .mw-mmv-enable-confirmation .mw-mmv-options-text{left:0}.mw-mmv-options-enable .mw-mmv-options-text,.mw-mmv-options-enable .mw-mmv-options-icon{top:70px}.mw-mmv-options-submit{margin-top:10px}.mw-mmv-options-text{left:68px;right:0}.mw-mmv-options-subcontainer .mw-mmv-options-text{margin-left:68px}.mw-mmv-options-icon{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2258.4%22 height=%2252.3%22 viewBox=%220 0 58.4 52.3%22%3E %3Cpath fill=%22%2336c%22 d=%22M0 0v44.7h58.4V0H0zm51.9 35.4H6l13.7-17.7 2-.8 12.9 13.7 6.8-4 10.5 8.8z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M48 4.2l1.9 1.9-5.1 5.1-1.3-1.3-.3 5.3 5.2-.3-1.3-1.3 5.2-5.1 1.8 1.9.4-6.6%22/%3E %3Cpath fill=%22%23eaecf0%22 d=%22M0 44.7h58.4v7.7H0z%22/%3E %3Cpath fill=%22%2336c%22 d=%22M1.9 46.2h13.8v1.6H1.9zm0 3.1h54.7v1.6H1.9z%22/%3E %3C/svg%3E");float:left;width:58px;height:52px}.mw-mmv-options-cancel-button,.mw-mmv-options-submit-button{float:right}.mw-mmv-options-dialog-header{padding-top:0;font-weight:normal;font-size:1.25em;color:#222}.mw-mmv-options-enable .mw-mmv-options-dialog-header{top:35px}.mw-mmv-options-text-header{margin:0;font-size:1em;color:#54595d}.mw-mmv-options-text-body{font-size:0.9em;color:#72777d}.mw-mmv-options-enable-alert{background-color:#eaecf0;color:#222;position:absolute;left:-15px;right:-15px;top:-15px;border-top-right-radius:2px;border-top-left-radius:2px;padding:10px 15px;font-weight:500}  .mw-mmv-image{display:table-cell;width:100%;height:100%;vertical-align:middle;-webkit-user-select:none;-moz-user-select:-moz-none;-ms-user-select:none;user-select:none}.mw-mmv-image.empty{display:none}.mw-mmv-image.error{background-color:#222}.mw-mmv-image.error a,.mw-mmv-image.error a:visited{cursor:pointer;color:#3472e5}.mw-mmv-image .error-box{background:url(/w/extensions/MultimediaViewer/resources/mmv/ui/img/error-media-icon.svg?78f99) no-repeat 0 0;background-size:110px 110px;position:absolute;left:50%;margin-left:-350px;top:50%;margin-top:-100px;padding:0 20px 0 160px;color:#fff;max-width:520px}.mw-mmv-image .error-box .mw-mmv-error-text{font-size:48px}.mw-mmv-image .error-box .mw-mmv-error-description{margin-top:30px;font-size:22px}.mw-mmv-image img{display:block;margin-right:auto;margin-left:auto;cursor:pointer;cursor:zoom-in; }.mw-mmv-image img.blurred{filter:blur(3px);-webkit-filter:blur(3px);opacity:0.8}.mw-mmv-image img.gif,.mw-mmv-image img.png,.mw-mmv-image img.svg,.mw-mmv-image img.tiff,.mw-mmv-image img.tif{background:url(/w/extensions/MultimediaViewer/resources/mmv/ui/checker.png?bdcf5) repeat}.mw-mmv-image img.mw-mmv-dialog-is-open{cursor:default}.metadata-panel-is-open .mw-mmv-image img{cursor:pointer}.mw-mmv-image.empty img{display:none}.metadata-panel-is-open .mw-mmv-image-wrapper{cursor:pointer}  .mw-mmv-download-button,.mw-mmv-reuse-button,.mw-mmv-options-button,.mw-mmv-close,.mw-mmv-fullscreen,.mw-mmv-next-image,.mw-mmv-prev-image{cursor:pointer;position:fixed;background-color:transparent;background-repeat:no-repeat;opacity:0.8;border:0;z-index:1003;-webkit-user-select:none;-moz-user-select:-moz-none;-ms-user-select:none;user-select:none}.mw-mmv-download-button.mw-mmv-dialog-open,.mw-mmv-reuse-button.mw-mmv-dialog-open,.mw-mmv-options-button.mw-mmv-dialog-open,.mw-mmv-close.mw-mmv-dialog-open,.mw-mmv-fullscreen.mw-mmv-dialog-open,.mw-mmv-next-image.mw-mmv-dialog-open,.mw-mmv-prev-image.mw-mmv-dialog-open,.mw-mmv-download-button:hover,.mw-mmv-reuse-button:hover,.mw-mmv-options-button:hover,.mw-mmv-close:hover,.mw-mmv-fullscreen:hover,.mw-mmv-next-image:hover,.mw-mmv-prev-image:hover{text-decoration:none;opacity:1}.mw-mmv-download-button.hidden,.mw-mmv-reuse-button.hidden,.mw-mmv-options-button.hidden,.mw-mmv-close.hidden,.mw-mmv-fullscreen.hidden,.mw-mmv-next-image.hidden,.mw-mmv-prev-image.hidden{display:none}.cursor-hidden .mw-mmv-download-button,.cursor-hidden .mw-mmv-reuse-button,.cursor-hidden .mw-mmv-close,.cursor-hidden .mw-mmv-fullscreen,.cursor-hidden .mw-mmv-next-image,.cursor-hidden .mw-mmv-prev-image{cursor:none}.mw-mmv-download-button,.mw-mmv-reuse-button,.mw-mmv-options-button,.mw-mmv-close,.mw-mmv-fullscreen{right:5px;left:auto;transition:opacity 0.25s;background-position:center;margin-top:14px;width:52px}.mw-mmv-next-image,.mw-mmv-prev-image{top:-999px;width:80px;height:120px;transition:opacity 0.25s,margin 0.25s}.mw-mmv-next-image.disabled,.mw-mmv-prev-image.disabled{display:none;cursor:none}.mw-mmv-close{top:5px;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2223%22 height=%2223%22 viewBox=%220 0 23 23%22%3E %3Cpath fill-opacity=%22.51%22 d=%22M19.65 0a.436.436 0 00-.282.125l-8.031 8L3.493.282C3.336.125 3.103.109 2.962.25L.274 2.938c-.14.141-.125.375.032.531l7.843 7.844-8.021 8.021c-.157.157-.172.422-.031.563l2.687 2.656c.141.141.375.126.532-.031l8.02-8.021 8.21 8.208c.156.157.39.173.531.032l2.688-2.688c.14-.14.125-.374-.032-.531l-8.209-8.209 8.032-8.031c.156-.157.172-.39.03-.531L19.9.094a.311.311 0 00-.25-.093z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M19.65.657a.32.32 0 00-.22.093l-8.093 8.094L3.43.938a.316.316 0 00-.438 0L1.306 2.625a.316.316 0 000 .438l7.906 7.906-8.094 8.094a.316.316 0 000 .437l1.719 1.688a.278.278 0 00.406 0l8.094-8.094 8.281 8.281c.118.118.32.118.438 0l1.687-1.718a.278.278 0 000-.407l-8.281-8.28 8.094-8.095a.316.316 0 000-.437L19.868.75a.32.32 0 00-.219-.093z%22/%3E %3C/svg%3E");height:23px}.mw-mmv-fullscreen{right:3px;top:42px;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2221.125%22 height=%2221.781%22%3E %3Cpath fill-opacity=%22.51%22 d=%22M8.313 0c-.102 0-.188.144-.188.344v3.781c0 .2.086.375.188.375h5.124L4 13.938v-5.97c0-.104-.135-.187-.313-.187H.313c-.178 0-.313.083-.313.188v13.437c0 .2.086.375.188.375h12.624c.102 0 .188-.175.188-.375v-3.781c0-.2-.086-.344-.188-.344H7L17.125 7.188v6.593c0 .11.166.219.344.219h3.343c.178 0 .313-.11.313-.219V.187c0-.012-.027-.019-.031-.03-.005-.016.01-.05 0-.063-.019-.037-.038-.045-.063-.063-.008-.005-.022.005-.031 0C20.987.025 20.983 0 20.969 0H8.312z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M8.781.5c-.075 0-.156.115-.156.281v2.406c0 .167.081.313.156.313h6.375L3.5 15.156v-6.75c0-.078-.146-.125-.313-.125H.781c-.166 0-.281.047-.281.125v12.219c0 .013.024.02.031.031.003.008-.003.024 0 .031.015.017.036.019.063.032.016.015.012.062.03.062h11.75c.076 0 .126-.146.126-.312v-2.407c0-.166-.05-.28-.125-.28H5.094L17.624 5.25v7.594c0 .078.147.156.313.156h2.407c.166 0 .28-.078.28-.156V.624c0-.014-.021-.017-.03-.03-.005-.008.008-.024 0-.032-.01-.02-.02-.017-.031-.03-.008-.005-.023.002-.032 0-.013-.01-.016-.032-.03-.032H8.78z%22/%3E %3C/svg%3E");height:22px}.mw-mmv-options-button{top:79px;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 96 96%22%3E %3Cpath fill-opacity=%22.51%22 d=%22M39.063.038c-.014.004.013.136 0 .14-.014.005-.128-.005-.141 0-.014.005-.128-.005-.14 0-.014.006.012.135 0 .14-.014.007-.128-.005-.141 0-.013.007-.128-.006-.141 0-.013.008.013.134 0 .141-.013.008-.128-.007-.14 0-.013.008-.13-.008-.141 0-.012.009.011.133 0 .141-.012.009-.13-.009-.14 0-.012.009.01.131 0 .14-.012.01-.13-.009-.142 0-.01.01.01.131 0 .141-.01.01-.13-.01-.14 0l-.14.14c-.01.012.008.13 0 .142-.01.01-.133-.012-.141 0-.009.011.008.128 0 .14-.008.012-.133-.012-.141 0-.008.012.008.129 0 .14-.007.013.007.129 0 .141-.007.013-.134-.012-.14 0-.007.013.006.128 0 .141-.006.013-.136.127-.141.14-.006.014.005.128 0 .141-.005.014.004.127 0 .14-.004.015.004.128 0 .142-.004.013-.137-.014-.14 0-.004.013.002.126 0 .14-.004.014.002.127 0 .14-.003.015.001.127 0 .141l-1.407 8.438c-1.472.498-2.851 1.025-4.219 1.687l-7.172-4.921c-.013-.01-.127.008-.14 0-.014-.009-.127-.133-.141-.141-.014-.008-.126.007-.14 0-.015-.007-.127.007-.141 0-.015-.007.015-.134 0-.14-.015-.007-.126.005-.14 0-.016-.006-.126.004-.142 0-.015-.006-.125-.137-.14-.141-.015-.005-.125.003-.14 0-.017-.004-.126.002-.141 0-.016-.003-.125.002-.141 0-.016-.002-.125.001-.14 0h-.141c-.016 0-.125-.14-.14-.141-.017 0-.126.14-.142.14-.015.002-.124 0-.14 0-.016.002-.125-.001-.14 0-.017.003-.126-.002-.141 0-.016.004-.125-.003-.141 0-.015.005-.125-.004-.14 0-.016.005-.126.136-.141.141-.015.006-.126-.005-.14 0-.016.006-.127-.006-.142 0-.014.007.015.134 0 .14-.014.008-.126-.006-.14 0-.014.008-.127-.007-.14 0-.015.009.013.133 0 .142-.014.008-.128-.01-.141 0-.013.009-.128.13-.141.14-.013.01-.128.13-.14.14-.012.011-.13-.01-.141 0-.012.012.011.13 0 .141L8.547 20.006c-.011.012-.13-.011-.14 0-.012.012.01.129 0 .14-.011.013-.131.129-.141.142-.01.013-.132.127-.141.14-.009.014.009.127 0 .14-.008.014-.133-.013-.14 0-.009.015.007.127 0 .141-.008.015.007.127 0 .141-.008.014-.135-.014-.141 0-.007.015.006.126 0 .14-.006.015.005.126 0 .141-.005.016-.136.126-.14.14-.005.016.003.126 0 .142-.005.015.003.125 0 .14-.004.016.001.125 0 .14-.003.017 0 .125 0 .141-.002.016 0 .125 0 .141-.002.016-.141.125-.141.14 0 .017.14.125.14.141v.14c.002.017-.002.126 0 .142.002.015-.002.124 0 .14.003.016-.003.125 0 .14.004.016-.004.126 0 .141.005.016.136.126.14.141.006.015-.005.126 0 .14.007.015-.006.126 0 .141.007.015.135-.014.141 0 .007.015-.007.127 0 .14.008.015-.007.128 0 .142.008.013.133.126.141.14.009.014-.009.128 0 .14l4.922 6.891c-.724 1.484-1.275 3.006-1.828 4.641l-8.297 1.266c-.016.002-.124-.003-.14 0-.017.003-.125-.004-.141 0-.017.004-.125-.005-.141 0-.016.004.016.136 0 .14-.016.005-.125-.005-.14 0-.016.006-.126-.006-.141 0-.016.007-.126-.006-.14 0-.016.007.014.134 0 .14-.016.008-.127-.006-.142 0-.014.009-.126-.007-.14 0-.015.009.014.133 0 .142-.015.008-.127-.009-.14 0-.015.009-.128.13-.141.14-.013.01-.128-.01-.141 0-.013.01.013.13 0 .14-.013.011-.128-.01-.14 0-.013.012.012.13 0 .141-.013.012-.13-.011-.141 0-.012.013.011.129 0 .141-.012.012-.13-.013-.14 0-.012.013.01.128 0 .14-.011.014-.131-.012-.141 0-.01.014.01.128 0 .141-.01.014-.132.127-.141.14-.009.015.008.127 0 .142-.008.014-.133.125-.14.14-.008.015.006.125 0 .14-.008.016-.135-.015-.141 0-.007.016.005.126 0 .141-.006.016.005.125 0 .141-.005.016.004.125 0 .14-.004.017-.137.125-.141.141-.003.016.003.125 0 .14-.003.017.002.125 0 .142-.002.016.002.124 0 .14v17.015c.002.017-.002.125 0 .141.002.017-.003.125 0 .141.003.016-.003.124 0 .14.004.017.137.125.14.141.005.016-.004.125 0 .14.006.017-.005.126 0 .142.006.015-.006.125 0 .14.007.015.135-.015.141 0 .007.015-.007.126 0 .14.008.016.133.127.14.141.009.015-.008.127 0 .141.01.014.132.127.141.14.01.014-.01.128 0 .141.01.013.13-.013.141 0 .01.013-.01.128 0 .14.01.013.13-.011.14 0 .012.013-.01.13 0 .142.012.011.13-.012.141 0 .013.01-.012.129 0 .14.013.01.128-.01.141 0 .013.01-.013.13 0 .14.013.011.127-.01.14 0 .014.01.127.132.141.141.014.009.127-.008.14 0 .015.009-.014.133 0 .141.015.008.127-.008.141 0 .015.007.126-.007.141 0 .015.007-.015.134 0 .14.015.007.125-.006.14 0 .017.006.126-.005.141 0 .016.006.125-.004.141 0 .016.005-.016.137 0 .141.016.005.125-.004.14 0 .017.004.125-.003.141 0 .016.003.124-.002.14 0l8.298 1.407c.55 1.61 1.102 3.14 1.828 4.64l-4.922 6.89c-.009.014.009.128 0 .141-.008.014-.133.127-.14.141-.008.014.007.126 0 .14-.008.015.006.127 0 .141-.007.015-.135-.015-.141 0-.007.015.006.126 0 .14-.006.016.005.126 0 .141-.005.016-.136.126-.14.141-.005.015.003.125 0 .14-.004.016.002.126 0 .141-.003.016.001.125 0 .141-.003.016 0 .125 0 .14-.002.016 0 .125 0 .141-.001.016-.141.125-.141.14 0 .017.14.126.14.141v.141c.002.016-.002.125 0 .14.002.016-.003.126 0 .141.004.016-.004.125 0 .141.004.015-.004.125 0 .14.005.016.136.126.14.141.006.015-.005.126 0 .14.007.016-.006.127 0 .141.008.015.134-.014.141 0 .008.015-.007.127 0 .141.008.014-.008.127 0 .14.008.015.133-.013.141 0 .009.014-.009.128 0 .141.009.013.131.128.14.141.01.013.131.128.141.14.011.012-.01.13 0 .141.011.012.13-.011.14 0L20.22 87.928c.011.011-.012.13 0 .14.011.012.128-.01.14 0 .013.011.128.131.141.141.013.01.127.132.14.141.014.009.128-.009.141 0 .014.008-.014.133 0 .14.014.009.127-.007.14 0 .015.008.127-.007.142 0 .014.008-.015.135 0 .141.014.007.125-.006.14 0 .015.006.126-.005.14 0 .016.005.126.136.141.14.016.005.126-.003.141 0 .016.005.125-.003.14 0 .016.004.125-.001.141 0 .016.003.125 0 .14 0 .017.002.126 0 .142 0 .015.002.124.141.14.141.016 0 .125-.14.14-.14h.141c.016-.002.125.002.141 0 .016-.002.125.002.14 0 .016-.003.126.003.141 0 .016-.004.125.004.14 0 .016-.005.126-.136.142-.14.015-.006.125.005.14 0 .015-.007.126.006.14 0 .016-.007-.014-.135 0-.141.015-.007.127.007.141 0 .014-.008.127.007.141 0 .014-.008.127-.133.14-.141.014-.009.128.009.141 0l6.89-4.922c1.456.69 2.936 1.177 4.5 1.688l1.407 8.437c.003.016-.003.124 0 .14.003.017-.003.125 0 .141.004.017-.004.125 0 .141.004.016.136-.016.14 0 .005.016-.004.125 0 .14.006.016-.005.126 0 .141.007.016-.006.126 0 .14.007.016.135-.014.141 0 .007.016-.007.127 0 .141.008.015-.008.127 0 .141.008.015.133-.014.141 0 .009.015-.009.127 0 .14.009.015.131.128.14.141.01.013-.01.128 0 .141.011.013.13-.013.141 0 .01.013-.01.128 0 .14.012.013.13-.012.14 0 .012.013-.01.13 0 .141.013.012.13-.011.142 0 .012.011-.013.13 0 .14.012.012.127-.01.14 0 .013.011-.013.131 0 .141.013.01.127-.01.14 0 .015.01.127.132.141.141.015.009.127-.008.141 0 .014.008.126.133.14.14.016.007.126-.006.141 0 .016.007-.015.135 0 .141.016.007.125-.005.14 0 .017.006.126-.005.142 0 .015.005.124-.004.14 0 .016.004.125.137.14.141.017.003.125-.003.141 0 .017.003.125-.002.141 0 .017.002.124-.002.14 0h17.156c.018-.002.125.002.142 0 .016-.002.124.003.14 0 .016-.003.125.003.14 0 .017-.004.125-.137.141-.14.017-.005.125.004.141 0 .016-.006.125.005.14 0 .016-.006.126.006.141 0 .016-.007-.015-.135 0-.141.016-.007.126.006.14 0 .016-.008.127-.133.142-.14.014-.009.126.008.14 0 .014-.01.127-.132.14-.142.014-.01.128.01.141 0 .013-.01-.013-.13 0-.14.013-.01.128.01.141 0 .013-.01-.012-.13 0-.14.012-.012.129.01.14 0 .012-.012-.01-.13 0-.141.012-.013.13.012.141 0 .011-.013-.01-.128 0-.141.01-.013.13.013.14 0 .011-.013-.009-.127 0-.14.01-.014.132-.127.142-.141.008-.014-.01-.127 0-.14.008-.015.132.014.14 0 .008-.015-.008-.127 0-.142.007-.014-.007-.125 0-.14.007-.015.134.015.14 0 .007-.015-.006-.125 0-.14.007-.017-.005-.126 0-.141.006-.016-.004-.125 0-.141.005-.016.137.016.141 0 .005-.016-.004-.125 0-.14.004-.017-.003-.125 0-.141.003-.016-.002-.124 0-.14l1.407-8.438c1.542-.547 2.968-1.124 4.359-1.829l7.031 4.922c.013.01.128-.009.14 0 .014.008.128.133.141.141.014.008.127-.007.141 0 .015.007.126-.007.14 0 .015.007-.014.134 0 .14.016.007.126-.005.141 0 .015.006.126-.004.141 0 .015.006.125.137.14.141.016.004.126-.004.141 0 .016.003.125-.003.14 0 .017.003.126-.002.141 0 .016.002.125-.001.141 0h.14c.017 0 .125.14.141.14.017 0 .125-.14.141-.14.016 0 .125.002.14 0 .016-.001.125.002.141 0 .016-.002.125.003.14 0 .017-.003.126.004.141 0 .016-.004.126.005.141 0 .015-.004.126-.135.14-.14.016-.006.126.005.141 0 .015-.006.126.006.141 0 .014-.007-.014-.134 0-.141.014-.007.126.007.14 0 .015-.008.127.008.141 0 .014-.008-.014-.132 0-.14.014-.01.128.008.14 0 .014-.01.129-.132.141-.141.013-.01.129-.13.141-.14.012-.012.13.01.14 0 .012-.012-.01-.13 0-.142L88 76.117c.011-.012.13.011.14 0 .011-.012-.01-.13 0-.141.011-.012.131-.128.141-.14.01-.014.132-.128.14-.141.01-.014-.008-.127 0-.14.009-.015.134.013.141 0 .009-.015-.007-.127 0-.142.008-.014-.007-.126 0-.14.008-.015.134.014.141 0 .006-.015-.006-.126 0-.14.006-.016-.005-.126 0-.141.005-.016.136-.126.14-.141.005-.015-.003-.125 0-.14.004-.016-.003-.126 0-.141.003-.016-.002-.125 0-.14.003-.017 0-.126 0-.142.002-.015 0-.124 0-.14.001-.016.141-.125.141-.14 0-.017-.14-.125-.14-.141v-.141c-.002-.016.002-.125 0-.14-.002-.016.002-.125 0-.141-.003-.016.003-.125 0-.14-.004-.016.004-.126 0-.142-.005-.015-.136-.125-.14-.14-.006-.015.005-.126 0-.14-.007-.016.005-.127 0-.141-.007-.015-.135.014-.141 0-.007-.015.007-.127 0-.141-.008-.014.007-.127 0-.14-.009-.015-.133-.128-.141-.141-.01-.014.009-.128 0-.14L83.5 65.286c.71-1.43 1.274-2.901 1.828-4.5l8.438-1.406c.016-.002.124.003.14 0 .016-.003.125.004.14 0 .017-.004.125.005.141 0 .017-.004-.015-.136 0-.14.016-.005.125.005.141 0 .016-.006.125.005.14 0 .016-.007.126.006.141 0 .016-.007-.015-.134 0-.141.016-.007.126.007.141 0 .015-.008.126.008.14 0 .015-.008-.014-.132 0-.14.015-.01.127.008.141 0 .014-.01.127-.132.14-.141.014-.01.128.01.141 0 .014-.01-.012-.13 0-.14.013-.011.129.01.141 0 .012-.012-.012-.13 0-.142.012-.01.129.012.14 0 .012-.012-.01-.128 0-.14.012-.012.13.012.141 0 .011-.013-.01-.128 0-.14.01-.014.13.012.141 0 .01-.014-.01-.128 0-.141.009-.014.132-.127.14-.141.01-.014-.007-.126 0-.14.009-.015.134-.126.141-.141.007-.016-.007-.126 0-.14.007-.016.135.014.14 0 .007-.016-.005-.126 0-.142.006-.015-.005-.124 0-.14.006-.016-.004-.125 0-.14.005-.017.138-.125.141-.141.004-.017-.003-.125 0-.141.003-.016-.002-.124 0-.14.002-.017-.001-.125 0-.141.002-.017 0-.124 0-.14V39.693c0-.017.002-.124 0-.14-.001-.018.002-.125 0-.142-.002-.016.003-.124 0-.14-.003-.016.004-.125 0-.14-.003-.017-.136-.125-.14-.141-.005-.017.005-.125 0-.141-.006-.016.005-.125 0-.14-.006-.016.006-.126 0-.141-.006-.016-.134.015-.14 0-.007-.016.006-.126 0-.14-.008-.016-.133-.127-.141-.142-.008-.014.008-.126 0-.14-.01-.014-.132-.127-.141-.14-.01-.014.01-.128 0-.141-.01-.013-.13.013-.14 0-.011-.013.01-.128 0-.141-.011-.013-.13.012-.141 0-.011-.012.011-.129 0-.14-.012-.012-.129.01-.14 0-.013-.012.011-.13 0-.141-.013-.011-.129.01-.141 0-.013-.01.013-.13 0-.14-.014-.011-.128.009-.141 0-.014-.01-.127-.132-.14-.142-.015-.008-.127.009-.141 0-.015-.008.014-.132 0-.14-.015-.008-.126.008-.141 0-.015-.007-.125.007-.14 0-.016-.007.015-.134 0-.14-.016-.007-.126.006-.141 0-.016-.007-.125.005-.14 0-.017-.006-.126.004-.141 0-.016-.005.016-.137 0-.141-.017-.005-.125.004-.141 0-.016-.004-.124.003-.14 0-.017-.003-.125.002-.141 0l-8.438-1.407a37.056 37.056 0 00-1.828-4.359l4.922-7.031c.009-.013-.01-.128 0-.14.008-.014.132-.128.14-.142.008-.013-.007-.126 0-.14.008-.015-.006-.126 0-.14.007-.015.135.014.141 0 .006-.016-.006-.126 0-.141.006-.015-.005-.126 0-.141.005-.015.136-.125.14-.14.005-.016-.003-.126 0-.141.004-.016-.002-.125 0-.14.003-.017-.001-.126 0-.142.003-.015 0-.124 0-.14.002-.016 0-.125 0-.14.001-.017.141-.125.141-.141 0-.017-.14-.125-.14-.141v-.14c-.002-.016.002-.125 0-.141-.003-.016.002-.125 0-.14-.004-.017.003-.126 0-.142-.004-.015.004-.125 0-.14-.005-.015-.136-.126-.14-.14-.006-.016.005-.126 0-.141-.007-.015.005-.126 0-.141-.008-.014-.135.014-.141 0-.008-.014.007-.126 0-.14-.008-.015.008-.127 0-.141-.009-.014-.133.013-.141 0-.009-.014.009-.128 0-.14-.01-.014-.131-.129-.14-.142-.01-.012-.131-.128-.141-.14-.011-.012.01-.13 0-.14-.011-.012-.13.01-.141 0L76.469 8.333c-.011-.01.011-.13 0-.14-.012-.011-.129.01-.14 0-.013-.01-.129-.131-.141-.14-.014-.01-.128-.132-.141-.142-.014-.008-.127.009-.14 0-.015-.008.013-.132 0-.14-.015-.008-.127.007-.141 0-.015-.007-.127.007-.141 0-.014-.007.014-.134 0-.14-.015-.007-.126.005-.14 0-.015-.006-.126.005-.141 0-.016-.006-.126-.137-.14-.141-.016-.005-.126.004-.141 0-.016-.004-.126.003-.141 0-.016-.003-.125.002-.14 0-.016-.002-.125.001-.141 0-.016-.002-.125 0-.141 0-.016-.001-.125-.14-.14-.141-.017 0-.125.14-.141.14h-.14c-.017.002-.126-.001-.141 0-.016.003-.125-.002-.141 0-.016.003-.125-.003-.14 0-.016.004-.126-.004-.141 0-.016.005-.126.136-.141.141-.015.005-.126-.005-.14 0-.015.006-.126-.006-.141 0-.015.007.014.134 0 .14-.015.008-.127-.006-.14 0-.015.008-.128-.007-.141 0-.014.009-.127.133-.141.141-.014.01-.128-.009-.14 0L65.5 12.834c-1.394-.7-2.82-1.276-4.36-1.828L59.735 2.71v-.14c-.003-.017.004-.125 0-.14-.004-.017.005-.125 0-.142-.004-.016-.136.016-.14 0-.005-.015.005-.124 0-.14-.006-.016.006-.125 0-.14-.007-.016.006-.126 0-.141-.007-.016-.134.015-.14 0-.008-.016.006-.126 0-.141-.009-.015.007-.126 0-.14-.009-.015-.133.014-.142 0-.008-.015.01-.127 0-.141-.008-.014-.13-.127-.14-.14-.01-.014.01-.128 0-.142-.01-.013-.13.013-.14 0-.011-.012.01-.128 0-.14-.012-.012-.13.012-.141 0-.012-.012.011-.129 0-.14-.013-.012-.129.01-.141 0-.012-.012.013-.13 0-.141-.013-.011-.128.01-.14 0-.014-.01.012-.13 0-.141-.014-.01-.128.01-.141 0-.014-.01-.127-.132-.14-.14-.015-.01-.127.008-.142 0-.014-.009-.125-.134-.14-.141-.015-.007-.125.006-.14 0-.016-.007.015-.135 0-.14-.016-.007-.126.005-.141 0-.016-.006-.125.004-.141 0-.016-.006-.125.004-.14 0-.017-.005-.125-.138-.141-.142-.016-.003-.125.004-.14 0-.017-.002-.125.003-.142 0-.016-.001-.124.002-.14 0H39.485a.436.436 0 00-.141 0c-.014.004-.127-.003-.14 0-.015.004-.128-.004-.142 0zm9.28 33.89c7.791 0 14.204 6.323 14.204 14.203 0 7.903-6.27 14.063-14.203 14.063-7.912 0-14.203-6.271-14.203-14.063 0-7.88 6.412-14.203 14.203-14.203z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M92.345 55.587v-16.5L82.22 37.4c-.75-2.719-1.875-5.344-3.188-7.688l5.907-8.343-11.625-11.72-8.344 5.907c-2.438-1.312-4.97-2.436-7.688-3.186L55.595 2.337h-16.5l-1.688 10.125c-2.718.75-5.343 1.875-7.687 3.188l-8.344-5.906L9.657 21.369l5.907 8.343c-1.313 2.438-2.438 4.97-3.188 7.782L2.345 39.087v16.5l10.031 1.688c.75 2.719 1.875 5.344 3.188 7.781L9.657 73.4l11.625 11.625 8.344-5.906c2.438 1.312 4.969 2.437 7.781 3.187l1.688 10.031h16.5l1.687-10.125c2.72-.75 5.344-1.875 7.688-3.187l8.344 5.906 11.625-11.625-5.907-8.344c1.313-2.437 2.438-4.968 3.188-7.78l10.125-1.595zm-45 9c-9.563 0-17.25-7.78-17.25-17.25 0-9.562 7.781-17.25 17.25-17.25 9.469 0 17.25 7.688 17.25 17.25 0 9.563-7.688 17.25-17.25 17.25z%22/%3E %3C/svg%3E");height:23px}.jq-fullscreened .mw-mmv-fullscreen{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2228.391%22 height=%2229.067%22%3E %3Cpath fill-opacity=%22.51%22 d=%22M15.224 0c-.178 0-.313.083-.313.188v13.437c0 .2.086.375.188.375h12.625c.1 0 .187-.176.187-.375V9.844c0-.2-.086-.344-.187-.344h-5.22l5.845-5.844c.08-.08.047-.265-.094-.406L25.567.562c-.14-.14-.326-.173-.406-.093l-6.25 6.25V.187c0-.104-.135-.187-.312-.187h-3.375z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M15.724.5c-.167 0-.313.047-.313.125v12.219c0 .01.026.02.031.031a.257.257 0 00.031.062c.012.01.017.025.032.032.005.004.025-.003.03 0 .01.005.022.03.032.03h11.719c.075 0 .125-.145.125-.312v-2.375c0-.166-.05-.312-.125-.312h-6.688l6.719-6.75c.06-.06.024-.195-.094-.313L25.536 1.25c-.118-.118-.284-.185-.344-.125l-6.781 6.812V.625c0-.079-.115-.125-.281-.125h-2.407z%22/%3E %3Cpath fill-opacity=%22.51%22 d=%22M.974 15c-.102 0-.188.144-.188.344v3.781c0 .2.086.375.188.375H6.38L.036 25.844c-.082.082-.016.265.125.406l2.656 2.656c.141.141.324.208.407.125l6.562-6.562v6.312c0 .11.135.219.313.219h3.343c.178 0 .344-.11.344-.219V15.188c0-.034-.038-.067-.062-.094a.229.229 0 00-.094-.063c-.013-.006-.017-.031-.031-.031H.974z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M1.411 15.5c-.075 0-.125.115-.125.281v2.406c0 .167.05.313.125.313h6.656l-7 7.031c-.06.06-.024.195.094.313L2.88 27.53c.117.118.252.185.312.125l7.094-7.125v7.313c0 .078.115.156.281.156h2.406c.167 0 .313-.078.313-.156V15.625c0-.026-.037-.042-.063-.063-.009-.02-.019-.017-.03-.03-.009-.005-.024.002-.032 0-.013-.01-.017-.032-.031-.032H1.41z%22/%3E %3C/svg%3E")}.mw-mmv-next-image{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2226%22 height=%2240%22 viewBox=%220 0 26 40%22%3E %3Cpath fill-opacity=%22.51%22 d=%22M6 .007a.403.403 0 00-.281.125L.656 5.226a.395.395 0 000 .563l14.188 14.218L.656 34.227a.395.395 0 000 .562l5.063 5.093a.434.434 0 00.593 0l14.47-14.5 5.093-5.093a.403.403 0 00.125-.282.433.433 0 00-.031-.156.392.392 0 00-.094-.125l-5.094-5.094L6.313.132A.478.478 0 006 .007z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M6 .695c-.077 0-.129.004-.188.062L1.97 4.601a.278.278 0 000 .406L16.28 19.29 1.344 34.226a.395.395 0 000 .563l3.687 3.656a.395.395 0 00.563 0L20.719 23.32l3.75-3.75c.022-.022.046-.037.062-.063.017-.017.02-.042.031-.062.004-.009-.003-.022 0-.031.039-.08.036-.139 0-.219-.008-.02.013-.045 0-.062-.005-.014-.024-.019-.03-.032l-.063-.093-3.688-3.657L6.22.758C6.159.698 6.077.695 6 .695z%22/%3E %3C/svg%3E");background-position:right;right:18px}.mw-mmv-next-image:hover{margin-right:-4px}.mw-mmv-prev-image{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2226%22 height=%2240%22 viewBox=%220 0 26 40%22%3E %3Cpath fill-opacity=%22.51%22 d=%22M20 .007c.102 0 .203.047.281.125l5.063 5.094a.395.395 0 010 .563L11.156 20.007l14.188 14.22a.395.395 0 010 .562l-5.063 5.093a.434.434 0 01-.593 0l-14.47-14.5L.126 20.29A.403.403 0 010 20.007c0-.05.012-.108.031-.156a.392.392 0 01.094-.125l5.094-5.094L19.687.132A.478.478 0 0120 .007z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M20 .695c.077 0 .129.004.188.062l3.843 3.844a.278.278 0 010 .406L9.72 19.29l14.937 14.937a.395.395 0 010 .563l-3.687 3.656a.395.395 0 01-.563 0L5.281 23.32l-3.75-3.75c-.022-.022-.046-.037-.062-.063-.017-.017-.02-.042-.031-.062-.004-.009.003-.022 0-.031-.039-.08-.036-.139 0-.219.008-.02-.013-.045 0-.062.005-.014.024-.019.03-.032l.063-.093L5.22 15.35 19.78.758c.06-.06.142-.063.219-.063z%22/%3E %3C/svg%3E");background-position:left;left:18px}.mw-mmv-prev-image:hover{margin-left:-4px}.mw-mmv-reuse-button{right:3px;bottom:105px;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2224%22 height=%2224%22 viewBox=%220 0 24 24%22%3E %3Cpath fill-opacity=%22.51%22 d=%22M16.051 1.55a.52.52 0 00-.092.062.52.52 0 00-.245.153c.004-.004-.034.004-.03 0 .004-.004-.004.034 0 .03l-.03.031a.52.52 0 000 .03.52.52 0 00-.031 0c.002-.003-.003.035 0 .031l-.03.03c.001-.003-.033.035-.031.031.001-.003-.002.035 0 .031.001-.004-.033.035-.031.03v.061a.52.52 0 00-.03.123v.061a.582.582 0 000 .062v2.233c-4.157.277-6.5 2.397-7.589 4.59-1.141 2.299-1.103 4.542-1.101 4.59v.03a2.328 2.328 0 000 .092.52.52 0 00.06.184.52.52 0 000 .03c0-.004.033.035.031.03-.001-.003.002.035 0 .031a.52.52 0 00.03.031c-.002-.004.035.034.032.03-.003-.003.002.035 0 .031a.52.52 0 00.03.03c-.002-.001.024-.007.03 0 .008.009.002.033 0 .031-.003-.004.004.035 0 .031-.003-.004.035.004.031 0l.03.03a.52.52 0 00.032.031c-.004-.002.034.002.03 0l.03.03c-.003-.001.035.033.031.031-.003-.001.035.002.03 0-.003-.001.035.033.031.031-.003-.002.035.001.031 0h.03a.52.52 0 00.123.03h.061a.452.452 0 00.061 0h.061a.52.52 0 00.123-.03h.03c-.003.001.035-.002.031 0-.004.002.035-.032.03-.03-.003.001.035-.002.031 0-.003.001.035-.033.031-.031-.004.002.034-.033.03-.03-.003.001.035-.003.031 0a.52.52 0 00.03-.031l.031-.031c-.003.004.035-.004.031 0 .008-.007.002-.032 0-.03l.03-.031.031-.03a.52.52 0 00.03-.031S10.79 10.6 15.5 10.975v2.845a.52.52 0 00.03.123.15.15 0 000 .03.16.16 0 000 .031c0-.004.033.034.032.03-.002-.003.001.035 0 .031-.002-.004.032.034.03.03-.002-.003.002.035 0 .031.005.008.032.002.03 0-.001-.003.003.034 0 .031l.031.03c-.002-.003.003.034 0 .031-.002-.003.033.003.03 0l.031.03c-.002-.002.003.034 0 .031-.002-.002.034.003.031 0l.03.031c-.002-.003.004.033 0 .03-.002-.002.035.003.031 0-.003-.002.034.033.03.031-.002-.002.035.002.031 0-.003-.002.004.033 0 .03.01.007.034.002.031 0l.061.031c-.005-.002.036.003.03 0-.005-.002.037.033.031.031-.005-.002.037.002.031 0l.061.03h.061a.52.52 0 00.031 0h.122a.52.52 0 00.184-.06c-.004 0 .035-.002.03 0-.003.001.035-.033.031-.031a.52.52 0 00.061-.03l.031-.031c-.004.003.034-.004.03 0-.003.003.004-.035 0-.031l.031-.03a.52.52 0 00.03 0l5.814-5.722a.52.52 0 00.062-.062c-.003.004.002-.034 0-.03-.003.003.032-.034.03-.03-.002.003.002-.035 0-.031l.03-.031c0 .004.033-.034.031-.03-.001.003.002-.035 0-.031a.071.071 0 010-.03c.004-.009.032-.003.031 0-.002.003.001-.035 0-.031V8.22a.52.52 0 00.03-.122v-.122a.52.52 0 00-.03-.123v-.03c.001.003-.002-.035 0-.031 0 .002-.027.007-.03 0 0 .004-.002-.035 0-.03 0 .003-.002-.035 0-.032.001.004-.033-.034-.031-.03 0 .002.004-.024 0-.03 0 .001-.027.007-.03 0 .001.003-.003-.035 0-.031.001.003-.034-.034-.031-.03.002.003-.003-.035 0-.032a.52.52 0 00-.062-.06l-5.813-5.753a.52.52 0 00-.061-.061c.003.002-.034-.003-.031 0l-.03-.03a.06.06 0 01-.031 0c.003.001-.034-.034-.03-.032a.439.439 0 01-.031-.03c.003.002-.035-.002-.031 0a.071.071 0 01-.03 0c-.008-.003-.003-.031 0-.03.003 0-.035-.002-.031 0 .004 0-.035-.002-.03 0a.52.52 0 00-.123-.031h-.183a.52.52 0 00-.062 0zM.63 3.08a.52.52 0 00-.153.092.52.52 0 00-.184.123c.003-.003-.003.033 0 .03a.52.52 0 00-.03.031c.002-.003-.033.003-.031 0a.52.52 0 000 .03l-.03.031c.002-.003-.003.034 0 .03.001-.003-.033.004-.031 0-.006.011-.002.034 0 .031a4.58 4.58 0 01-.03.062c.002-.006-.003.036 0 .03.001-.005-.033.036-.031.03.001-.005-.002.037 0 .031.001-.005-.032.067-.031.062v.06a.52.52 0 000 .062v16.798a.52.52 0 00.03.123.15.15 0 000 .03.15.15 0 000 .03c0-.003.033.035.031.031-.002-.003.002.035 0 .03l.03.032c-.001-.004.003.034 0 .03.005.007.033.002.031 0-.002-.003.003.034 0 .03-.002-.003.033.035.031.031-.002-.003.002.034 0 .03-.002-.002.033.004.03 0l.031.032c-.003-.003.003.033 0 .03-.003-.003.034.003.03 0l.031.03c-.003-.002.004.034 0 .031-.003-.002.034.003.031 0l.03.03c-.003-.001.035.003.031 0-.003-.001.004.033 0 .032.01.005.033 0 .03 0l.062.03c-.005-.002.036.002.03 0-.005-.002.037.033.031.03-.005-.001.036.002.03 0-.005 0 .068.033.062.031h.061a.52.52 0 00.061 0h18.175a.52.52 0 00.123-.03c-.004.001.034-.002.03 0-.003.001.035-.002.031 0-.004.001.034-.033.03-.03-.003 0 .035-.003.031 0-.003.001.034-.034.03-.031-.003.002.035-.033.031-.031-.003.002.034-.003.031 0l.03-.03c-.003.002.034-.003.031 0-.003.002.003-.034 0-.031l.03-.03c-.002.002.034-.004.031 0-.002.002.003-.034 0-.031l.031-.031c-.003.003.033-.003.03 0-.002.003.003-.034 0-.03-.002.003.033-.035.031-.031-.002.003.002-.034 0-.03-.003.004.034-.037.03-.031l.031-.062c-.002.006.003-.036 0-.03-.002.005.033-.036.031-.03-.002.005.002-.037 0-.031l.03-.062v-.06a.52.52 0 000-.062v-7.283a.52.52 0 00-.03-.123v-.03c.001.004-.002-.035 0-.03l-.03-.032c.001.004-.002-.034 0-.03.001.004-.033-.034-.031-.03 0 .001.004-.024 0-.031.002.003-.033-.004-.03 0 .001.003-.003-.034 0-.03l-.031-.032c.002.004-.003-.033 0-.03.002.003-.034-.003-.031 0l-.03-.03c.002.002-.003-.034 0-.031.002.003-.034-.003-.031 0l-.03-.03c.002.002-.004-.034 0-.031.002.002-.035-.003-.031 0l-.031-.031c.003.002-.034-.002-.03 0 .003.002-.004-.033 0-.03-.011-.007-.034-.002-.031 0a4.144 4.144 0 01-.061-.031c.005.002-.036-.002-.031 0 .006.002-.036-.033-.03-.03.005.001-.037-.002-.031 0 .006 0-.067-.033-.061-.031h-.062a.52.52 0 00-.03 0h-.123a.52.52 0 00-.153.06.52.52 0 00-.091.031c.004-.001-.035.002-.03 0l-.032.03c.004-.001-.034.034-.03.032.003-.003-.034.002-.03 0a.52.52 0 00-.031.06.52.52 0 00-.03 0l-2.663 2.602a.52.52 0 00-.061.06c.002-.003-.002.035 0 .031l-.03.03c.001-.003-.033.035-.031.032.002-.004-.002.034 0 .03.001-.004-.032.035-.03.03 0-.003-.002.035 0 .031 0-.002.002.023 0 .03-.004.009-.032.003-.031 0 .001-.003-.002.035 0 .031 0-.003-.002.035 0 .031a.52.52 0 00-.031.122v1.286H4.24V7.15h4.069a.52.52 0 00.092-.03.52.52 0 00.153-.031c-.004.002.034-.033.03-.03-.003.001.034-.034.03-.031-.003.002.035-.003.031 0l3.948-2.601a.52.52 0 00.06-.03l.031-.031.03-.03c-.004.004.036-.067.032-.062-.005.005.034-.036.03-.03-.004.005.034-.037.03-.031 0 .002-.004-.022 0-.03-.002.003.033-.035.031-.031-.003.006.033-.037.03-.031 0 .003-.004-.018 0-.03a.071.071 0 010-.031c.004-.008.032-.002.031 0-.001.004.002-.035 0-.03 0 .003.002-.035 0-.031a.52.52 0 00.031-.123v-.122a.52.52 0 00-.03-.123c0 .004-.002-.034 0-.03 0 .004-.002-.035 0-.03 0 .001-.028.007-.031 0V3.54c.001.003-.002-.035 0-.031.001.004-.032-.034-.03-.03.001.003-.003-.035 0-.031 0 .002-.027.007-.031 0 .002.003-.003-.034 0-.03l-.03-.031c.001.003-.003-.034 0-.031a.52.52 0 00-.062-.061 3.266 3.266 0 01-.03-.03l-.031-.031c.003.002-.034-.003-.03 0 .003.002-.035-.033-.031-.031l-.031-.03c.004.001-.034-.033-.03-.031.003.002-.035-.002-.031 0 .004.002-.035-.002-.03 0 .003.001-.035-.032-.031-.03.004 0-.035-.002-.031 0 .004 0-.035-.002-.03 0 .003 0-.035-.032-.031-.031h-.061a.52.52 0 00-.031 0H.69a.52.52 0 00-.061 0z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M16.064 2.188v2.873c-8.872.057-8.7 8.606-8.7 8.606s3.163-4.297 8.7-3.442v3.442l5.797-5.734-5.797-5.745zM.624 3.723V20.46H18.75v-7.227l-2.655 2.602v1.97H3.331V6.336H8.08l3.94-2.613H.625z%22/%3E %3C/svg%3E");height:24px}.mw-mmv-download-button{bottom:142px;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2224%22 height=%2224%22 viewBox=%220 0 24 24%22%3E %3Cpath fill-opacity=%22.51%22 d=%22M9.944.006a1.62 1.62 0 00-1.157.469 1.62 1.62 0 00-.468 1.156v7.875h-4s.006.025 0 .031c-.006.007-.031 0-.032 0 0 0 .005.029 0 .032-.004.003-.03 0-.03 0-.004.003-.028-.004-.032 0h-.031V9.6c-.004.003-.028-.004-.032 0 0 0 .008.028 0 .031-.007.004-.03 0-.03 0s-.005.025 0 .031c-.012-.005-.032 0-.032 0h-.031s-.004.025 0 .032h-.032s-.004.024 0 .031c0 0 .005.029 0 .031l-.03.031s-.028-.003-.032 0v.032s.027.003.031 0l-.031.031c-.006-.004.004-.028 0-.031-.007-.005-.031 0-.031 0a.097.097 0 000 .031v.031c-.004.004.004.028 0 .031.004.003-.006.028 0 .032-.011-.005-.032 0-.032 0v.125a2.53 2.53 0 000 .062v.188s.02.005.032 0c-.006.004.004.028 0 .031v.094s.024.004.031 0c.004-.003-.006-.028 0-.032.009-.005.022.006.031 0-.006.005.005.029 0 .032-.006.004-.031 0-.031 0v.031h.031c.004.003-.004.028 0 .031.004.008.032 0 .031 0l.032-.031c.006-.006.031 0 .031 0s.011.037 0 .063c-.002.004-.03-.004-.031 0-.005.006 0 .03 0 .03H4.1l6.031 7.438H3.069a.097.097 0 00-.032 0h-.062v.032c-.004.003-.028-.004-.031 0 0 0 .006.026 0 .03-.006.005-.031 0-.032 0H2.85v.032s.002.027 0 .031-.031 0-.031 0h-.032v.032s.006.023 0 .03c-.005.009-.03 0-.03 0s.003.03 0 .032c-.004.002-.032 0-.032 0h-.031v.031h-.032v.062s.005.025 0 .032c-.004.007-.03 0-.03 0v.063s.003.023 0 .03c-.005.009-.032 0-.032 0v2.97a.705.705 0 000 .062v.125h.031v.063s.024-.01.031 0c.008.01 0 .03 0 .03v.063s.024-.008.032 0c.007.008 0 .031 0 .032h.031c.003.003-.004.027 0 .03.004.004.029-.002.031 0 .001.002-.007.025 0 .032.008.007.04.009.031 0-.008-.008-.007.024 0 .031.008.008.03 0 .032 0 .002.003-.004.028 0 .032.003.003.028-.004.031 0a.097.097 0 000 .03c.008.008.031 0 .031 0 .008.008 0 .032 0 .032h.031c.004.006.032 0 .032 0h.031c.01.008 0 .031 0 .031h.062v.032h.125a1.698 1.698 0 00.063 0h15.594a.704.704 0 00.062 0h.032a.097.097 0 00.03 0h.063v-.032h.062s-.009-.023 0-.031c0 0 .025.006.032 0 0 0 .028.006.031 0h.032s-.008-.024 0-.031c.008-.008.03 0 .03 0 .008-.008 0-.032 0-.031.004-.004.028.003.032 0 .004-.004-.003-.03 0-.032.001 0 .024.008.031 0 .008-.007.009-.04 0-.031-.008.009.024.007.032 0 .007-.007-.001-.03 0-.031.002-.003.027.003.03 0 .004-.004-.003-.028 0-.031l.032-.032c.008-.008.031 0 .031 0v-.031a.097.097 0 000-.031c.006-.007 0-.032 0-.031.008-.01.031 0 .032 0V21.6h.03v-.062a.097.097 0 000-.032v-.031a1.698 1.698 0 000-.063v-2.78a.704.704 0 000-.063v-.125h-.03c-.005-.004.003-.028 0-.031v-.032s-.024.01-.032 0c0 0 .006-.024 0-.031v-.062s-.023.008-.031 0l-.031-.032c-.004-.003.003-.027 0-.031-.004-.004-.029.003-.031 0-.001-.001.007-.024 0-.031-.008-.008-.04-.009-.032 0 .009.008.008-.024 0-.031-.007-.008-.03 0-.031 0-.003-.003.004-.028 0-.032-.004-.003-.028.004-.031 0V18.1c-.008-.008-.032 0-.031 0-.008-.008 0-.031 0-.031h-.032c-.003-.006-.031 0-.031 0-.007-.006-.031 0-.031 0-.01-.008 0-.031 0-.031h-.063v-.032h-.125a1.698 1.698 0 00-.062 0H11.1v-.031l6.625-7.406h.063s.004-.025 0-.031c-.005-.003.005-.028 0-.032l-.032.032s-.012-.043-.031-.063c.005.002.019-.006.031 0 .013.006.02-.005.032 0 .004.001-.004.03 0 .031.011.004.03 0 .03 0 .005-.003-.003-.028 0-.031 0 0 .027.003.032 0v-.031s-.025.004-.031 0l.031-.031v.03c.006.005.031 0 .031 0a.097.097 0 000-.03v-.094c.011.005.032 0 .032 0v-.188a2.81 2.81 0 000-.062v-.125s-.02-.005-.032 0v-.031c-.003-.004.004-.028 0-.032v-.062s-.024-.005-.031 0c-.004.003.006.027 0 .031-.023.016-.063.031-.063.031.006-.007-.007-.022 0-.031.008-.014.024-.017.032-.031.006-.005.031 0 .031 0v-.031h-.031c-.004-.004.004-.028 0-.032 0 0-.02-.005-.031 0v-.031s-.021-.006-.032 0c.005-.007 0-.031 0-.031v-.031s-.024-.005-.031 0V9.63s-.025-.004-.031 0c0 0-.025-.006-.031 0-.003-.002.003-.027 0-.031h-.032v-.031a.097.097 0 00-.031 0 .12.12 0 00-.031 0s-.027.003-.031 0v-.031c-.006-.007-.032 0-.032 0v-.032h-.093a.097.097 0 00-.032 0H13.57L13.757.63a.704.704 0 000-.062V.444h-.031c-.004-.004.004-.028 0-.031V.38s-.024.01-.031 0V.288s-.024.007-.031 0c0 0 .007-.024 0-.032a.097.097 0 00-.032 0c-.003-.003.004-.027 0-.031-.003-.004-.028.003-.031 0 0-.001.007-.024 0-.031-.007-.008-.04-.009-.031 0 .008.008.007-.024 0-.031-.008-.008-.03 0-.031 0-.003-.003.003-.028 0-.032-.004-.003-.028.004-.032 0a.097.097 0 000-.031s-.023.008-.031 0 0-.031 0-.031h-.031c-.003-.006-.031 0-.031 0s-.022.007-.032 0c-.01-.008 0-.031 0-.031h-.062V.006h-.125a1.693 1.693 0 00-.063 0H9.944zM4.069 10.537l-.032-.03c.005.007 0 .03 0 .03s.025.007.032 0zm.093-.875c.004.012-.004.019 0 .032-.008-.009-.024.006-.03 0l.03-.032zm13.594.094c.009-.004.023.035.032.031-.01.006-.023-.005-.032 0 .005-.002-.006-.027 0-.03zm-13.75.094c.009.006.023-.006.032 0 .006.007 0 .031 0 .031s-.017-.021-.032-.031z%22/%3E %3Cpath fill=%22%23fff%22 d=%22M9.71.435c-.6 0-1 .4-1 1v8.5H4.305l6.094 7.5 6.688-7.5H12.71l.187-9.5H9.711zm-6.718 18v2.78h15.594v-2.78H2.992z%22/%3E %3C/svg%3E");height:24px}    .mw-mmv-permission-box{position:relative;width:90%;margin:10px 20px 0}.mw-mmv-permission-box.empty{display:none}.mw-mmv-permission-box h3{margin:10px;padding:0;color:#54595d;font-size:0.95em}.mw-mmv-permission-box .mw-mmv-permission-close{display:none;position:absolute;top:12px;right:8px;width:16px;height:16px;background-image:url(/w/extensions/MultimediaViewer/resources/mmv/ui/img/x_gray.svg?1f162);cursor:pointer}.mw-mmv-permission-box .mw-mmv-permission-text{position:relative;max-height:3.78em;overflow:hidden;margin:0 10px 10px;font-size:0.9em;line-height:1.4;color:#54595d}.mw-mmv-permission-box .mw-mmv-permission-text .mw-mmv-permission-text-fader{position:absolute;top:2.52em;width:100%;height:1.26em;background-image:-moz-linear-gradient(top,rgba(255,255,255,0) 0%,#ffffff 100%);background-image:-webkit-gradient(linear,left top,left bottom,color-stop(0%,rgba(255,255,255,0)),color-stop(100%,#ffffff));background-image:-webkit-linear-gradient(top,rgba(255,255,255,0) 0%,#ffffff 100%);background-image:linear-gradient(to bottom,rgba(255,255,255,0) 0%,#ffffff 100%);text-align:right}.mw-mmv-permission-box .mw-mmv-permission-text .mw-mmv-permission-text-fader a{padding:3px 0 0 1em;background-color:#fff;font-size:1em}.mw-mmv-permission-box .mw-mmv-permission-html{padding:0 15px 15px;display:none;max-height:400px;overflow:auto}.mw-mmv-permission-box.full-size .mw-mmv-permission-close{display:block}.mw-mmv-permission-box.full-size .mw-mmv-permission-text{display:none}.mw-mmv-permission-box.full-size .mw-mmv-permission-html{display:block;border-top:1px solid #f8f9fa}.mw-mmv-progress{background-color:#c8ccd1;background-color:rgba(221,221,221,0.5);width:100%;height:14px;position:absolute;top:-14px}.mw-mmv-progress.empty{display:none}.mw-mmv-progress-percent{width:0;height:14px;background:linear-gradient(-45deg,transparent 33%,rgba(0,0,0,0.1) 33%,rgba(0,0,0,0.1) 66%,transparent 66%),#3366cc;background-size:35px 20px,100% 100%,100% 100%;-webkit-animation:mw-mmv-progress-percent-animation 1.5s linear infinite;-moz-animation:mw-mmv-progress-percent-animation 1.5s linear infinite;animation:mw-mmv-progress-percent-animation 1.5s linear infinite}@-webkit-keyframes mw-mmv-progress-percent-animation{0%{background-position:0 0}100%{background-position:-70px 0}}@-moz-keyframes mw-mmv-progress-percent-animation{0%{background-position:0 0}100%{background-position:-70px 0}}@keyframes mw-mmv-progress-percent-animation{0%{background-position:0 0}100%{background-position:-70px 0}}  .mw-mmv-stripe-button{float:right;margin-top:-9px;-webkit-user-select:none;-moz-user-select:-moz-none;-ms-user-select:none;user-select:none;font-size:1.25em;color:#888888;cursor:pointer;transition:opacity 0.25s}.mw-mmv-stripe-button.empty{display:none}.mw-mmv-stripe-button:before{display:inline-block;width:1em;height:1em;position:relative;top:0.1em;background-size:100% 100%;content:' ';vertical-align:baseline;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%22212 84 600 600.8%22%3E %3Cpath fill=%22%23fff%22 d=%22M422.4 233.6h180v60h-180v-60zm299.2 120v60H422.4v-60h299.2zM422.4 534.4v-60h300v60h-300zM332 233.6h-84l-5.6-8v-67.2c0-24 21.6-44 45.6-44s44 20 44 44v75.2zM812 148c0-35.2-60-64-60-64H276c-35.2 0-64 28.8-64 64v100l16 16h104v420.8h480V148z%22/%3E %3C/svg%3E");margin-right:0.7em}.mw-mmv-stripe-button.has-label:before{margin-right:0.25em}.mw-mmv-stripe-button.mw-mmv-description-page-button,.mw-mmv-stripe-button.mw-mmv-description-page-button:active,.mw-mmv-stripe-button.mw-mmv-description-page-button:visited{border-radius:2px;color:#fff;padding:7px 16px;margin:7px 10px}.mw-mmv-stripe-button.mw-mmv-description-page-button.mw-mmv-repo-button-commons:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 80 80%22%3E %3Cg fill=%22%23fff%22%3E %3Cpath d=%22M39.022 80c-8.302-.252-15.975-4.025-21.384-10.314-4.528-5.283-6.918-11.95-6.918-18.994 0-7.044 2.39-13.71 7.044-19.12.629-.754 2.138-2.264 2.264-2.264l3.9 3.9 3.899 3.899 1.132-1.006c.629-.63 1.132-1.007 1.132-1.007s.754 2.39 2.264 8.05c0 0 0 .126-.126.126 0 0-7.421-2.012-7.924-2.138h-.126s.503-.503 1.006-1.132a11.797 11.797 0 001.006-1.132c0-.126-1.886-2.013-1.886-2.013s-.504.63-1.007 1.132a20.56 20.56 0 00-4.276 10.566c-.126 1.007-.126 1.133 0 1.258h2.767v-1.51c0-1.509 0-1.509.126-1.509.125 0 7.17 4.151 7.17 4.151s0 .126-.126.126c-.126-.126-7.044 3.774-7.044 3.774v-3.02h-2.768v.378c0 .755.252 2.139.503 3.145.63 3.019 2.013 5.786 3.9 8.176.377.377 1.006 1.258 1.006 1.258s2.013-1.887 2.013-2.013c0 0-.503-.503-1.007-1.132-.628-.629-1.006-1.132-1.006-1.132 0-.126 8.05-2.264 8.05-2.138l-2.138 8.05s-.377-.377-1.132-1.006l-1.132-1.132c-.126 0-2.013 1.887-1.887 2.012 0 .126.755.755 1.384 1.132 2.39 1.761 5.157 3.145 8.176 3.774 1.006.252 2.39.377 3.145.503h.377v-2.767H36.38c0-.126 4.151-7.296 4.151-7.296s4.15 7.17 4.15 7.296h-3.018v2.767h.377c.755 0 2.139-.251 3.145-.503 3.019-.629 5.786-2.013 8.176-3.774a23.22 23.22 0 001.384-1.132c.126-.125-1.887-2.012-1.887-2.012l-1.132 1.132a11.797 11.797 0 01-1.132 1.006c0-.126-2.138-8.05-2.138-8.05l8.05 2.138s-.378.378-1.006 1.132c-.63.63-1.007 1.132-1.007 1.132 0 .126 1.887 2.013 2.013 2.013.126 0 .755-.755 1.006-1.258 1.887-2.39 3.145-5.283 3.9-8.176.251-1.006.377-2.39.377-3.145v-.377h-3.522v3.019s-6.918-3.9-7.17-4.151l-.126-.126s7.044-4.15 7.17-4.15c.126 0 .126 0 .126 1.509v1.509s2.641.126 2.767 0v-1.258a20.56 20.56 0 00-4.277-10.566c-.503-.629-1.006-1.132-1.006-1.132l-1.887 1.887s.503.503 1.007 1.132a11.826 11.826 0 011.006 1.132l-.126.126c-.503.126-7.924 2.138-7.924 2.138s-.126 0 0-.126c1.509-5.534 2.138-7.924 2.264-8.05 0 0 .503.377 1.132 1.006l1.132 1.007 1.006-1.007 1.006-1.006s-.628-.503-1.132-.88c-1.132-.755-1.886-1.007-5.534-2.516-2.516-1.007-3.774-1.635-5.283-2.642-3.27-2.264-5.409-5.157-6.793-9.308-.125-.503-.629-2.013-.629-2.013 0-.125-.251 0-2.012.63-2.264 1.131-3.145 1.509-3.396 1.509-.378 0-.504-.126-.252-.378.126-.251.252-.377.629-.88.88-1.132 1.635-2.138 2.516-3.9 1.51-2.767 3.019-6.54 4.276-10.817.378-1.258.88-2.893 1.007-3.27 0-.126 0-.252.125-.252 0 0 3.648 6.164 5.41 9.057 3.396 6.037 4.905 9.182 4.528 9.434-.126.125-.252 0-.88-.63-.881-.88-1.385-1.131-2.265-1.509-.377-.126-1.006-.251-1.384-.251-.377 0-.377 0-.251.503.503 2.012 1.383 3.522 2.515 4.528 1.133 1.132 2.516 1.887 6.038 3.27 2.767 1.133 4.528 2.013 6.038 3.02 1.258.88 2.264 1.635 3.27 2.767 6.29 6.415 9.183 15.094 8.176 24.025-.88 7.799-5.031 14.843-11.195 19.622-3.522 2.642-7.42 4.529-11.698 5.41-2.264.502-4.905.754-7.17.754z%22/%3E %3Cpath d=%22M39.399 60.503c-1.51-.126-2.516-.377-3.648-1.006-1.132-.503-1.887-1.132-2.767-2.013-1.384-1.51-2.39-3.27-2.642-5.409-.125-.754-.125-2.138 0-2.767.126-1.006.378-1.887.755-2.641a9.816 9.816 0 017.547-5.66c.378 0 .63-.126 1.384-.126.755 0 1.132 0 1.384.125a9.816 9.816 0 017.547 5.66c.377.881.629 1.636.754 2.642.126.63.126 2.139 0 2.767-.125 1.007-.377 1.887-.88 2.768-.503 1.132-1.006 1.887-1.761 2.641-.88.88-1.635 1.51-2.767 2.013-1.384.629-2.642 1.006-4.151 1.006h-.755z%22/%3E %3C/g%3E %3C/svg%3E");width:1.3em;height:1.3em;position:relative;top:0.15em;margin:-0.3em 0.4em 0 0}    .mw-mmv-ttf-container{position:relative;overflow:hidden}.mw-mmv-ttf-container.mw-mmv-ttf-small{font-size:90%}.mw-mmv-ttf-container.mw-mmv-ttf-smaller{font-size:80%}.mw-mmv-ttf-container.mw-mmv-ttf-smallest{font-size:65%}.mw-mmv-ttf-container.mw-mmv-ttf-truncated{cursor:pointer}.mw-mmv-ttf-container.mw-mmv-ttf-untruncated{height:auto}.mw-mmv-ttf-container .mw-mmv-ttf-ellipsis{display:block;position:absolute;right:0;bottom:0;height:1em;width:1.5em;text-indent:-9999px;border-radius:2px;background-size:contain;background-position:center center;background-repeat:no-repeat;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 96 96%22%3E %3Cg fill=%22%2372777d%22 transform=%22translate%280 20%29%22%3E %3Cpath d=%22M22 48.3c0 6.1-4.9 11-11 11s-11-4.9-11-11 4.9-11 11-11 11 4.9 11 11zm37 0c0 6.1-4.9 11-11 11s-11-4.9-11-11 4.9-11 11-11 11 4.9 11 11z%22/%3E %3Ccircle cx=%2285%22 cy=%2248.3%22 r=%2211%22/%3E %3C/g%3E %3C/svg%3E");-webkit-box-shadow:-4px 6px 0 0 #ffffff;box-shadow:-4px 6px 0 0 #ffffff}.mw-mmv-ttf-ellipsis-container:hover .mw-mmv-ttf-container .mw-mmv-ttf-ellipsis{background-color:#eaecf0;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 96 96%22%3E %3Cg fill=%22%2372777d%22 transform=%22translate%280 20%29%22%3E %3Cpath d=%22M22 48.3c0 6.1-4.9 11-11 11s-11-4.9-11-11 4.9-11 11-11 11 4.9 11 11zm37 0c0 6.1-4.9 11-11 11s-11-4.9-11-11 4.9-11 11-11 11 4.9 11 11z%22/%3E %3Ccircle cx=%2285%22 cy=%2248.3%22 r=%2211%22/%3E %3C/g%3E %3C/svg%3E")}.mw-mmv-ttf-container .mw-mmv-ttf-ellipsis:hover,.mw-mmv-ttf-ellipsis-container .mw-mmv-ttf-container .mw-mmv-ttf-ellipsis:hover{background-color:#c8ccd1;background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 96 96%22%3E %3Cg fill=%22%2354595d%22 transform=%22translate%280 20%29%22%3E %3Cpath d=%22M22 48.3c0 6.1-4.9 11-11 11s-11-4.9-11-11 4.9-11 11-11 11 4.9 11 11zm37 0c0 6.1-4.9 11-11 11s-11-4.9-11-11 4.9-11 11-11 11 4.9 11 11z%22/%3E %3Ccircle cx=%2285%22 cy=%2248.3%22 r=%2211%22/%3E %3C/g%3E %3C/svg%3E")}.mw-mmv-ttf-container .mw-mmv-ttf-ellipsis:before{display:block;content:'';height:1.6em;width:2em;margin-left:-2em;background-image:-moz-linear-gradient(left,rgba(255,255,255,0) 0%,#ffffff 100%);background-image:-webkit-gradient(linear,left top,right top,color-stop(0%,rgba(255,255,255,0)),color-stop(100%,#ffffff));background-image:-webkit-linear-gradient(left,rgba(255,255,255,0) 0%,#ffffff 100%);background-image:linear-gradient(to right,rgba(255,255,255,0) 0%,#ffffff 100%)}  .mw-mmv-info-box{display:inline-block;overflow:hidden;border:1px solid #c8ccd1;border-radius:2px;background-color:#fff}.mw-mmv-title-contain{position:relative}.mw-mmv-title-para{margin:0 0 10px;padding:10px 20px 0 20px;height:36px;line-height:36px}.mw-mmv-title-para.mw-mmv-ttf-normal{font-size:20px}.mw-mmv-title-para.mw-mmv-title-small{font-size:18px}.mw-mmv-title-para.mw-mmv-title-smaller{height:44px;margin:0 0 20px;padding:6px 20px 0 20px;line-height:22px;font-size:16px}.mw-mmv-title-para.mw-mmv-ttf-untruncated{height:auto}.mw-mmv-title-para .mw-mmv-ttf-ellipsis{right:20px;bottom:3px;background-color:#ffffff}.mw-mmv-title-para .mw-mmv-ttf-ellipsis:before{background-image:-moz-linear-gradient(left,rgba(255,255,255,0) 0%,#ffffff 100%);background-image:-webkit-gradient(linear,left top,right top,color-stop(0%,rgba(255,255,255,0)),color-stop(100%,#ffffff));background-image:-webkit-linear-gradient(left,rgba(255,255,255,0) 0%,#ffffff 100%);background-image:linear-gradient(to right,rgba(255,255,255,0) 0%,#ffffff 100%)}.mw-mmv-credit{margin:0;color:#54595d;padding:0 0 5px;font-size:0.85em}.mw-mmv-credit .metadata-panel-is-closed{height:27px;line-height:27px}.mw-mmv-credit.empty{height:0.85em}.mw-mmv-credit .mw-mmv-ttf-ellipsis{right:5px;bottom:13px;background-color:#f8f9fa}.mw-mmv-credit .mw-mmv-ttf-ellipsis:before{background-image:-moz-linear-gradient(left,rgba(248,249,250,0) 0%,#f8f9fa 100%);background-image:-webkit-gradient(linear,left top,right top,color-stop(0%,rgba(248,249,250,0)),color-stop(100%,#f8f9fa));background-image:-webkit-linear-gradient(left,rgba(248,249,250,0) 0%,#f8f9fa 100%);background-image:linear-gradient(to right,rgba(248,249,250,0) 0%,#f8f9fa 100%)}.mw-mmv-source-author{line-height:1.8em}.mw-mmv-title{display:inline-block}.mw-mmv-image-metadata{width:100%;position:relative;margin-top:-30px;border-top:1px solid #c8ccd1;background-color:#f8f9fa;padding-top:2px}.jq-fullscreened .mw-mmv-image-metadata{height:27px;overflow:hidden}.jq-fullscreened .mw-mmv-untruncated .mw-mmv-image-metadata{height:auto}.mw-mmv-author:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Ctitle%3E user avatar %3C/title%3E %3Cg fill=%22%2372777d%22%3E %3Cpath d=%22M10 11c-5.92 0-8 3-8 5v3h16v-3c0-2-2.08-5-8-5z%22/%3E %3Ccircle cx=%2210%22 cy=%225.5%22 r=%224.5%22/%3E %3C/g%3E %3C/svg%3E");background-position:center center;background-repeat:no-repeat;background-size:contain;display:inline-block;vertical-align:middle;height:16px;width:16px;content:' ';margin-right:7px;margin-bottom:2px}.mw-mmv-image-desc-div{overflow-y:auto;max-height:150px;margin-bottom:15px}.mw-mmv-image-desc-div.empty{display:none}.mw-mmv-image-desc-div,.mw-mmv-image-links-div{display:inline-block;vertical-align:top}.mw-mmv-image-desc{font-size:0.95em;color:#54595d}.mw-mmv-image-links{margin:0 20px}.mw-mmv-image-links li{list-style:none;font-size:0.85em}.mw-mmv-image-links li.empty{display:none}.metadata-panel-is-closed .mw-mmv-image-links li.mw-mmv-license-li{height:28px;line-height:28px}.mw-mmv-image-links li:before{display:inline-block;vertical-align:middle;height:16px;width:16px;content:' ';margin-right:7px;margin-bottom:2px;background-size:contain;background-position:right center;background-repeat:no-repeat}.mw-mmv-image-links li.mw-mmv-license-li:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%225.5 -3.5 64 64%22%3E %3Cpath fill=%22%2372777d%22 d=%22M13 .5v56h38.5C57.45 56.5 62 51.95 62 46V.5H13zM51.5 53H20V4h14v28l7-7 7 7V4h10.5v42c0 3.85-3.15 7-7 7z%22/%3E %3C/svg%3E")}.mw-mmv-image-links li.mw-mmv-license-li.cc-license:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%225.5 -3.5 64 64%22%3E %3Ccircle cx=%2237.785%22 cy=%2228.501%22 r=%2228.836%22 fill=%22none%22 stroke=%22%2372777d%22/%3E %3Cpath fill=%22%2372777d%22 d=%22M37.441-3.5c8.951 0 16.572 3.125 22.857 9.372 3.008 3.009 5.295 6.448 6.857 10.314 1.561 3.867 2.344 7.971 2.344 12.314 0 4.381-.773 8.486-2.314 12.313-1.543 3.828-3.82 7.21-6.828 10.143-3.123 3.085-6.666 5.448-10.629 7.086-3.961 1.638-8.057 2.457-12.285 2.457s-8.276-.808-12.143-2.429c-3.866-1.618-7.333-3.961-10.4-7.027-3.067-3.066-5.4-6.524-7-10.372S5.5 32.767 5.5 28.5c0-4.229.809-8.295 2.428-12.2 1.619-3.905 3.972-7.4 7.057-10.486C21.08-.394 28.565-3.5 37.441-3.5zm.116 5.772c-7.314 0-13.467 2.553-18.458 7.657-2.515 2.553-4.448 5.419-5.8 8.6a25.204 25.204 0 00-2.029 9.972c0 3.429.675 6.734 2.029 9.913 1.353 3.183 3.285 6.021 5.8 8.516 2.514 2.496 5.351 4.399 8.515 5.715a25.652 25.652 0 009.943 1.971c3.428 0 6.75-.665 9.973-1.999 3.219-1.335 6.121-3.257 8.713-5.771 4.99-4.876 7.484-10.99 7.484-18.344 0-3.543-.648-6.895-1.943-10.057-1.293-3.162-3.18-5.98-5.654-8.458-5.146-5.143-11.335-7.715-18.573-7.715zm-.401 20.915l-4.287 2.229c-.458-.951-1.019-1.619-1.685-2-.667-.38-1.286-.571-1.858-.571-2.856 0-4.286 1.885-4.286 5.657 0 1.714.362 3.084 1.085 4.113.724 1.029 1.791 1.544 3.201 1.544 1.867 0 3.181-.915 3.944-2.743l3.942 2c-.838 1.563-2 2.791-3.486 3.686-1.484.896-3.123 1.343-4.914 1.343-2.857 0-5.163-.875-6.915-2.629-1.752-1.752-2.628-4.19-2.628-7.313 0-3.048.886-5.466 2.657-7.257 1.771-1.79 4.009-2.686 6.715-2.686 3.963-.002 6.8 1.541 8.515 4.627zm18.457 0l-4.229 2.229c-.457-.951-1.02-1.619-1.686-2-.668-.38-1.307-.571-1.914-.571-2.857 0-4.287 1.885-4.287 5.657 0 1.714.363 3.084 1.086 4.113.723 1.029 1.789 1.544 3.201 1.544 1.865 0 3.18-.915 3.941-2.743l4 2c-.875 1.563-2.057 2.791-3.541 3.686a9.233 9.233 0 01-4.857 1.343c-2.896 0-5.209-.875-6.941-2.629-1.736-1.752-2.602-4.19-2.602-7.313 0-3.048.885-5.466 2.658-7.257 1.77-1.79 4.008-2.686 6.713-2.686 3.962-.002 6.783 1.541 8.458 4.627z%22/%3E %3C/svg%3E")}.mw-mmv-image-links li.mw-mmv-license-li.pd-license:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%225.5 -3.5 64 64%22%3E %3Cpath fill=%22%2372777d%22 d=%22M54.414 39.014c.915-1.828 1.372-4.114 1.372-5.943h-7.772l6.4 5.943zM53.5 17.071C48.471 7.93 35.671 5.186 26.986 11.586l5.485 5.485c5.943-4.571 14.172 0 14.629 6.858h8.229c0-2.743-.915-5.943-1.829-6.858-.914-1.828.914 1.829 0 0z%22/%3E %3Cpath fill=%22%2372777d%22 d=%22M37.5-3.5c-17.829 0-32 14.171-32 32s14.171 32 32 32 32-14.171 32-32-14.171-32-32-32zm-27.429 32c0-5.486 1.829-10.514 4.572-14.629l6.4 6.4c-2.743 7.772-1.829 17.372 4.114 23.772 5.486 5.486 15.086 6.857 22.4 3.2l4.572 5.028c-4.115 2.743-9.143 4.115-14.172 4.115-15.543-.457-27.886-12.8-27.886-27.886zm17.829-.914l13.714 14.171c-4.114 1.372-8.685 0-11.428-3.657-1.829-3.2-2.286-7.314-2.286-10.514zM57.157 47.7l-38.4-38.857C23.33 3.814 30.186 1.07 37.5 1.07c15.086 0 27.429 12.343 27.429 27.429 0 7.314-3.2 14.171-7.772 19.2z%22/%3E %3C/svg%3E")}.mw-mmv-image-links li.mw-mmv-filename-li:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2219%22 height=%2214%22 viewBox=%220 0 19 14%22%3E %3Cpath fill=%22%2372777d%22 d=%22M13 0H0v14h19V6h-6V0zm4.5 12.3V12H2.4l4.5-5.5.7-.1 4.2 4.5 2.3-1.5 3.4 2.9z%22/%3E %3Cpath fill=%22%2372777d%22 d=%22M18.8 4.5h-4.5V0%22/%3E %3C/svg%3E")}.mw-mmv-image-links li.mw-mmv-datetime-li:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2280%22 height=%2280%22 viewBox=%220 0 80 80%22%3E %3Cpath fill=%22%2372777d%22 d=%22M64.742 15.246C58.14 8.638 49.35 5 39.996 5c-9.348 0-18.137 3.638-24.754 10.246C8.633 21.856 5 30.65 5 40s3.633 18.144 10.242 24.742C21.859 71.352 30.648 75 39.996 75c9.354 0 18.144-3.649 24.746-10.258C71.353 58.144 75 49.35 75 40s-3.647-18.144-10.258-24.754zm-13.283 41.29L36.504 41.604V16.01h5.628v23.234l13.3 13.309-3.973 3.982z%22/%3E %3C/svg%3E")}.mw-mmv-image-links li.mw-mmv-location-li:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2280%22 height=%2280%22 viewBox=%220 0 80 80%22%3E %3Cpath fill=%22%2372777d%22 d=%22M61.615 10.966A30.392 30.392 0 0051.506 4.25a30.615 30.615 0 00-33.178 6.728c-5.783 5.792-8.975 13.166-8.975 20.791 0 7.613 3.19 14.995 8.975 20.781l3.182 3.153c6.386 6.31 11.893 11.758 16.618 19.331L39.966 78l1.85-2.966c4.725-7.573 10.232-13.021 16.607-19.324l3.191-3.17c12.044-12.044 12.044-29.529.001-41.574zM49.477 42.641c-5.25 5.258-13.76 5.258-19.011 0-5.249-5.24-5.249-13.75 0-19 5.251-5.24 13.761-5.24 19.011 0 5.238 5.25 5.238 13.758 0 19z%22/%3E %3C/svg%3E")}.mw-mmv-image-links li.empty:before{background-image:none !important; }.mw-mmv-license-contain,.mw-mmv-license{text-align:right}.mw-mmv-filename-prefix{padding-right:4px;cursor:text}.mw-mmv-title-para,.mw-mmv-credit,.mw-mmv-image-desc{padding-left:20px}.mw-mmv-about-links{font-size:0.85em;padding:20px;width:50%;clear:both}.mw-mmv-label{background-color:#eaecf0;color:#222;margin-left:6px;margin-top:1px;border-radius:2px;padding:2px 5px;font-size:0.9em}.mw-mmv-label:hover{background-color:#c8ccd1}.mw-mmv-image-metadata-column{float:left}.mw-mmv-image-metadata-desc-column{width:66.5%}.mw-mmv-image-metadata-links-column{max-width:33.5%;width:25%;text-align:right;float:right;transition:width 0.2s ease-out}.mw-mmv-untruncated .mw-mmv-image-metadata-links-column{width:33.5%;text-align:left}.mw-mmv-restrictions{display:inline-block;line-height:14px}.mw-mmv-restriction-label{cursor:default;display:inline-block;height:16px}.mw-mmv-restriction-label,.mw-mmv-restriction-label:hover{background-color:#fc6}.mw-mmv-restriction-label-inner{display:inline-block;width:14px;height:14px;text-indent:-9999px;text-align:left;background-size:contain}.mw-mmv-restriction-label-inner:after{float:right;text-indent:0}.mw-mmv-restriction-2257{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 d=%22M64 63.43H45.256L31.704 42.744 17.9 63.43H-8l30.29-31.662L.464.57h18.702l13.087 19.588L45.72.57h17.942L41.71 31.135 64 63.43z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-aus-reserve{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 d=%22M49.024 2.346c-2.179 2.268-2.006 5.62-2.19 8.515-.247 3.166-3.671 4.733-5.458 1.819-2.273-1.17-7.088-3.087-3.768-6.01 1.239-1.461 1.625-2.656-.662-2.304-2.317.615-5.01-2.55-6.605-1.01.67 1.932-3.56.545-3.975 3.091-.499 2.203-1.535 4.134-3.75 2.016-2.504-2.237-4.987 1.97-5.734 2.799.853 1.324-2.909 1.143-.965 2.49-.833 2.001-1.932-3.072-2.565-.021.343 3.524-2.84 6.01-6.048 6.818-2.81.252-4.573 3.223-6.714 3.87-.87-1.04-.65 4.011-.156 5.458C2.787 31.029.2 34.153 2.64 35.504c1.184 2.587 3.321 5.234 3.168 8.13-2.186 3.172 3.68 4.56 5.343 2.11 1.913-2.355 5.297-.565 7.32-2.559 2.039-2.874 6.43-2.899 9.66-3.857 3.368-.274 5.443 1.8 7.18 4.096-.43 2.596 1.588 1.655 2.096-.137.903-1.066 2.574-1.662 1.27.226-.998.99-.486 3.618.272 1.107 2.06-.86-.1 2.63 2.359 2.515-.024 2.905 2.293 4.83 5.199 5.168 2.456-.574 4.634 1.692 6.822-.36 4.33.004 3.583-5.22 6.362-7.349 3.678-3.691 4.851-9.228 4.09-14.26-1.857-1.748-2.183-5.43-4.382-6.134-.06-2.62-2.088-4.984-4.671-6.257-.734-2.389-1.338-5.474-1.686-8.096-4.132-.394-1.933-4.892-4.017-7.5zm-.4 54.03c-.893 2.043.675 7.189 3.696 4.534 1.1-1.99 1.28-5.248-1.923-3.8-.63-.162-1.114-.655-1.773-.734z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-communist{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 fill-rule=%22evenodd%22 d=%22M31.934 0C55.19 15.901 56.025 29.211 53.04 40.015c-.646 2.34-1.731 4.446-3.252 6.19-1.29 1.479-2.938 2.724-4.996 3.683-6.357 2.963-16.17 1.966-29.08-5.537l-2.441 3.147c-.516-.385-1.798-.86-2.783.345-.953 1.166.395 2.498-.1 3.362-.44.767-.583.747-3.388 2.48-5.579 3.445-6.784 8.087-5.137 9.709 1.607 1.582 6.333-.348 9.027-6.834 1.14-2.744.882-2.379 1.752-3.31.734-.787 2.084.31 2.951-.67.92-1.039.372-2.533-.004-3.043l.793-.91c13.014 11.687 26.154 13.391 35.115 8.478l6.147 6.623c.64.664 2.239.07 3.584-1.326 1.344-1.397 1.91-3.057 1.271-3.721l-5.793-5.687c3.32-3.655 5.47-8.657 5.975-14.64C64.472 17.128 48.11 4.708 31.934 0zM22.95 11.512L9.497 25.488l5.82 6.043 6.326-6.57 3.164-3.29 7.996-8.306-9.853-1.853zM38.99 19.5c-.872 0-1.578.276-2.12.83-.543.554-.813 1.286-.813 2.193 0 1.002.276 2.659.83 4.969l.92 3.853c.46 1.91.73 3.537.812 4.881h.78c.153-1.886.394-3.513.724-4.88l.938-3.854c.59-2.452.883-4.126.883-5.022 0-.86-.282-1.567-.848-2.12-.554-.566-1.257-.85-2.105-.85zm.038 19.253c-.802 0-1.48.284-2.034.85a2.804 2.804 0 00-.832 2.033c0 .79.278 1.468.832 2.033.566.554 1.244.83 2.034.83.79 0 1.461-.276 2.015-.83a2.773 2.773 0 00.848-2.033c0-.801-.282-1.479-.848-2.033a2.717 2.717 0 00-2.015-.85z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-costume{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 fill-rule=%22evenodd%22 d=%22M50.477 0c-6.606 4.229-11.73 10.863-19.481 13.187-6.862 2.179-14.184 1.266-21.231.839-3.598-1.417-6.256 1.87-4.78 5.25 1.647 9.517 6.173 18.306 11.46 26.296 4.395 6.31 9.82 12.151 16.6 15.906 5.76 2.912 13.311 3.862 18.784-.206 4.091-2.592 5.26-7.596 6.422-11.969 2.205-9.96.865-20.24-.353-30.234-.829-5.258-2.991-10.171-4.861-15.118-.728-1.345-1.2-3.102-2.56-3.951zm-6.295 13.741c3.165.799 11.212 2.413 6.02 5.54l-13.199 4.313c1.264-5.245.32-11.22 7.18-9.853zm-22.347 8.716c3.165.799 11.212 2.413 6.02 5.54l-13.2 4.312c1.264-5.245.321-11.22 7.18-9.852zM51.762 33.39c5.181 3.66 1.884 9.062.853 12.706-1.71 4.281-11.064 9.595-15.575 7.469-4.524-1.543-7.244-2.464-11.86-6.9-.629-6.497 3.78-3.115 7.055-2.573 5.357 1.752 12.296.678 14.994-4.814 1.566-2.515 1.583-4.709 4.533-5.888z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-currency{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 d=%22M30.69.318c-6.128-.134-11.905 3.47-14.575 8.834l-3.205-.646-1.656 2.336 3.918.789c-.157.413-.244.84-.33 1.267-.086.428-.173.855-.188 1.297l-2.777-.56-1.656 2.336 4.345.875c.461 6.913 5.547 13.127 12.565 14.54 3.883.783 7.825-.017 11.07-1.958l.748-3.705c-2.982 2.66-7.135 3.939-11.303 3.1-5.699-1.149-9.817-5.91-10.408-11.44l17.346 3.496 1.658-2.336-19.023-3.832c.015-.442.101-.868.187-1.295.072-.356.173-.856.33-1.27l20.162 4.063 1.657-2.336-20.768-4.184c2.636-4.806 8.255-7.677 14.025-6.515 4.489.904 7.5 3.624 9.227 7.197l1.65-2.3C41.48 4.325 37.82 1.512 33.33.608a15.04 15.04 0 00-2.64-.29zM48.142 27.07c-.916.016-1.871.15-2.866.405-2.578.658-4.6 1.9-6.068 3.726-1.818 2.29-2.34 4.959-1.563 8.006.184.718.438 1.437.762 2.158.325.721.938 1.838 1.84 3.356l-3.383.863.83 3.254 4.307-1.1c.485.86.84 1.736 1.068 2.63.532 2.08.492 3.968-.12 5.661-.614 1.694-1.523 3.107-2.731 4.242l2.486 3.415c.82-.881 1.78-1.656 2.88-2.327 1.101-.67 2.179-1.14 3.233-1.41a12.695 12.695 0 012.112-.351c.462-.025 1.501.002 3.12.08 1.62.077 2.637.102 3.052.074a10.53 10.53 0 001.914-.3 9.814 9.814 0 002.138-.829c.695-.365 1.644-1.004 2.846-1.92l-2.201-3.3c-1.52 1.261-2.82 2.031-3.904 2.308-.894.228-2.677.363-5.35.406-1.617.023-3.032.192-4.248.502-.952.243-1.968.671-3.043 1.29 1.014-1.836 1.643-3.54 1.889-5.116.245-1.576.206-3.003-.12-4.277a4.998 4.998 0 00-.912-1.827l6.24-1.593-.83-3.252-6.921 1.765c-1.505-2.409-2.464-4.426-2.88-6.052-.377-1.48-.162-2.88.647-4.202.81-1.322 2.026-2.19 3.653-2.605 1.523-.389 2.959-.218 4.308.514 1.35.732 2.36 1.948 3.03 3.65l3.78-1.598c-1.068-2.535-2.65-4.312-4.743-5.33-1.309-.636-2.726-.942-4.252-.916zm-34.989 1.004l-.74 3.627c-1.527-.311-8.78.156-10.004 6.154-1.088 5.336 3.572 7.804 6.91 8.98l-1.222 6.032c-2.369-1.2-2.45-3.15-2.485-4.756L.105 46.988c-.882 6.07 4.032 9.978 6.946 11.014l-.752 3.69 5.234 1.066.74-3.631c4.189.633 9.8-1.81 10.762-6.527 1.102-5.403-3.474-7.934-7.555-9.196l1.127-5.527c1.616.446 2.575 2.271 2.39 3.89l5.454 1.112c1.093-5.36-3.404-9.213-6.857-10.121l.74-3.63-5.18-1.054zm-1.767 8.793l-.934 4.58c-3.68-.808-2.96-4.711.934-4.58zm3.058 11.908c4.546 1.044 4.162 5.533-1.062 5.198l1.062-5.198z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-design{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 d=%22M15.375 0v15.375H0v1h15.375V31.5H0v1h15.375v15.125H0v1h15.375L0 64h64V0L52.87 11.133l-4.243 4.242h-.002V0h-1v15.375H32.5V0h-1v15.375H16.375V0h-1zm1 16.375H31.5V31.5H16.375V16.375zm16.125 0h15.125v.002L32.502 31.5H32.5V16.375zm18.457 15.111v.014h-.014l.014-.014zM16.375 32.5H31.5v.002L16.375 47.625V32.5zm33.568 0h1.014v15.125h-2.332V33.818l1.318-1.318zm-2.318 2.318v12.807H34.818l12.807-12.807zM33.818 48.625h13.807v2.332H32.5v-1.014l1.318-1.318zm14.807 0h2.332v2.332h-2.332v-2.332zM31.5 50.943v.014h-.014l.014-.014z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-fan-art{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 fill-rule=%22evenodd%22 d=%22M21.145 2.008c-.77.02-1.43.58-1.586 1.373L8.879 41.908c-.352 1.777.872 3.473 2.656 3.785 1.784.313 3.511-.865 3.785-2.656l7.508-39.084c.14-.916-.443-1.76-1.35-1.92a1.665 1.665 0 00-.333-.025zm18.43 7.65c-1.161.024-2.37.165-3.634.446-5.145 1.14-10.539 3.35-14.744 7.72l-4.873 25.37c-.17 1.1-.782 2.002-1.6 2.634.45.816.658 1.855.55 3.23 3.81 9.697 13.185 12.934 18.292 12.934 6.76 0 22.29-3.106 21.377-11.328-.913-8.222-8.952-10.05-8.039-14.252.914-4.202 9.502 3.29 12.608 4.022 3.106.73 8.037-11.144.547-18.635-6.555-6.555-12.363-12.307-20.485-12.14zm.347 3.285c1.29-.021 2.636.63 3.6 1.739 1.612 1.854 1.694 4.422.183 5.736-1.512 1.314-4.046.875-5.658-.98s-1.695-4.425-.184-5.739c.552-.48 1.271-.742 2.059-.756zm-10.969 5.094c1.369.05 2.77.621 3.91 1.592 2.42 2.062 3.02 5.331 1.34 7.303-1.68 1.97-5.005 1.897-7.424-.164-2.418-2.062-3.017-5.33-1.338-7.301.822-.965 2.09-1.481 3.512-1.43zm-6.47 12.053a4.953 4.953 0 012.288.525c2.093 1.034 3.107 3.254 2.264 4.96-.843 1.705-3.223 2.25-5.316 1.216-2.093-1.035-3.106-3.257-2.262-4.963.52-1.051 1.66-1.707 3.025-1.738zm1.146 13.113c.087.003.173.01.26.018 2.03.188 3.552 1.683 3.398 3.338-.154 1.654-1.925 2.84-3.955 2.652-2.03-.189-3.55-1.682-3.396-3.336.145-1.572 1.759-2.74 3.693-2.672zM8.906 44.955c-1.095.544-2.222 1.507-3.177 2.906-1.998 2.928 1.04 5.007-.672 8.418C3.345 59.691 0 61.83 0 61.83s11.1-2.634 13.307-8.866c1.343-3.795 1.187-5.702.504-6.815a4.078 4.078 0 01-2.41.315 4.073 4.073 0 01-2.495-1.508z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-ihl{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 d=%22M34.923.061c-.539-.071-1.103.157-1.744.8-1.297 1.301-1.765 3.165-1.483 5.123L5.672 32.068c-1.864-.2-3.625.276-4.865 1.52-2.565 2.573 1.509 3.917 4.875 7.273l5.224 5.207c3.366 3.356 4.711 7.417 7.276 4.845 1.244-1.248 1.717-3.019 1.506-4.889l10.794-10.82c13.061 14.525 16.6 17.131 25.408 26.958.055.062.116.125.176.184 1.85 1.82 5.628 2.136 7.038.723 1.455-1.46 1.133-5.375-.926-7.211-9.848-8.783-12.547-12.23-27.106-25.255l10.86-10.887c1.773.132 3.436-.341 4.622-1.53 2.564-2.573-1.51-3.917-4.876-7.273l-5.223-5.207C37.93 3.19 36.542.275 34.923.061z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-insignia{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 d=%22M30.297 0a.736.736 0 00-.125.016c-.315-.028-1.217.426-.729.648.623-.087 1.127.204 1.266.82.427.425.432 1.249-.283 1.362-.615.126-1.18-.27-1.774-.368-.363-.59.887-.638.678-1.166-.283-.54-1.28-.793-1.441-.027-.236.572-.218 1.27-.703 1.717-.408.514-.335 1.22-.838 1.602-.055.71.859.67 1.187 1.128.347.4.814 1.002 1.412.702.697-.322.39-1.245.903-1.68.748-.556.288.791.511.955.372-.027.965-.115.774.467.26.452.103.558-.819 1.16-.498-.582-1.488-.1-1.336.144.508.164 2.103-.303 2.102.473-.209.715-1.088.808-1.69.57-.62-.11-1.236-.27-1.87-.265-.654-.045-1.28.33-1.66.844-.157.589-.738 1.308-.266 1.878.705.313 1.012-.675 1.572-.898.612-.337 1.361-.244 2.037-.285.674-.01 1.344-.015 2.008.103.262.016.524.026.787.034.263-.008.525-.018.787-.034.664-.118 1.334-.112 2.008-.103.676.041 1.425-.052 2.037.285.56.223.867 1.21 1.572.898.472-.57-.11-1.29-.265-1.878-.38-.515-1.007-.89-1.66-.844-.635-.004-1.25.156-1.872.265-.601.238-1.48.145-1.689-.57 0-.776 1.594-.31 2.102-.473.152-.243-.838-.726-1.336-.144-.923-.602-1.08-.708-.819-1.16-.19-.582.402-.493.774-.467.223-.164-.237-1.511.511-.955.513.435.206 1.358.903 1.68.598.3 1.065-.303 1.412-.702.328-.458 1.242-.418 1.187-1.128-.503-.382-.43-1.088-.838-1.602-.485-.448-.467-1.145-.703-1.717-.16-.766-1.158-.513-1.441.027-.21.528 1.041.576.678 1.166-.594.098-1.16.494-1.774.368-.715-.113-.71-.937-.283-1.362.139-.616.643-.907 1.266-.82.488-.222-.414-.676-.729-.648A.717.717 0 0033.703 0c-.61-.017-.922.763-1.328 1.135-.084.098-.17.197-.232.31-.04.13-.088.255-.143.377a3.023 3.023 0 01-.143-.377c-.063-.113-.148-.212-.232-.31C31.219.763 30.907-.018 30.297 0zm2.213 11.518c-1.72.03-3.463.782-4.815 2.502-2.402 3.057-.436 2.768.729 4.296 1.165 1.53 1.238 4.732-.072 5.97 0 0 3.785-3.713 5.095-3.786 1.31-.073 3.057 1.53 3.057 1.53s-.8-2.113-.436-3.278c.364-1.165 2.767-4.44.22-6.115-1.116-.733-2.44-1.144-3.778-1.12zm-9.405.101c-.187-.013-.391.125-.63.512-.975-.062-1.694-.392-2.707-.025-1.617-.271-2.538 1.2-3.315 2.351-.258.99.71 1.623 1.582 1.536a.959.959 0 01.625-.215c.013-.01.026-.014.04-.023 1.092-.543-.19-2.968 1.603-2.276.53.747.16 3.313-1.221 2.385a1.295 1.295 0 00-.422-.084c-.188.122-.403.19-.625.213-.585.452-.889 1.483-1.586 1.766-1.152.977.179 2.211.182 3.345.026 1.421 1.537 2.645 2.922 2.106 1.545-.068 2.055-1.638.61-2.366-.686-.95-1.826-1.375-2.606-2.216 1.697-.434 3.125.857 4.662 1.275 1.916.203 4.129-1.555 3.636-3.61.12-1.225-1.446-1.99-.992-3.314-.124-.082-.277.023-.379.084-.535-.384-.9-1.409-1.379-1.443zm17.79 0c-.479.035-.844 1.06-1.38 1.444-.101-.061-.252-.167-.376-.084.454 1.324-1.115 2.086-.994 3.312-.493 2.054 1.72 3.812 3.636 3.61 1.537-.418 2.965-1.71 4.662-1.276-.78.842-1.92 1.268-2.605 2.219-1.446.728-.936 2.297.61 2.365 1.384.54 2.895-.685 2.921-2.105.003-1.135 1.334-2.369.182-3.346-.697-.283-1.001-1.314-1.586-1.766a1.398 1.398 0 01-.625-.213c-.129 0-.267.028-.422.084-1.38.928-1.751-1.64-1.22-2.386 1.791-.692.51 1.733 1.603 2.275.013.01.026.017.039.025.246 0 .448.078.625.215.872.088 1.84-.546 1.582-1.535-.777-1.15-1.698-2.623-3.315-2.352-1.013-.367-1.732-.038-2.707.025-.239-.387-.443-.525-.63-.512zM9.742 19.611c-.056 0-.113 0-.17.012-.767.488-1.877.17-2.779.155-.527.56.018 1.03.103 1.543.477.908.072 2.124-.103 3.09-.507.93.744 1.186.412 2.162-.424.407-.6.975-.516 1.67.068.616-.162 1.112-.103 1.726.19.781.263 1.82.83 2.572.356.595.146 1.41.201 2.125.014 1.15.231 2.21-.27 3.328-.368 1.138-.931 2.282-1.171 3.3-.35.8-.865 1.92-1.237 2.882-.304.707-.355 2.178-1.441 1.545-.355-.637.207-1.514.104-2.266.22-.814.082-1.782.412-2.572.156-.946.088-1.983.101-2.988.078-.821-.115-1.186-.72-1.543-.286-.85-.582-1.65-.514-2.594-.181-.707-.078-1.518-.104-2.287-.07-.445.27-1.596-.513-1.09-.728.472-1.533 1.126-1.34 2.08-.21 1.42.961 2.484 1.236 3.79v4.96c.052.606-.113 1.297.104 1.834v2.37c-.233 1.1.555 1.649 1.142 2.411.031 1.832 1.417.066 1.43-.767-.086-.56.734-2.392.62-2.059.032 1.066-.441 2.047-.517 3.088v2.102c-.047.977.558 1.68.774 2.443.653.911.799 2.206 1.598 3.125-.118 1.579 2.086 1.245 2.261.236-.12-.735.232-1.279.207-1.976.085-.849-1.17-.88-1.644-1.42-.222-.794-.499-1.733-.21-2.553.386-.874.431-1.821 1.13-2.37.76-.379.778-1.342.724-2.161-.137-.837.18-1.02.823-.719-.058 1.058-.137 2.123-.102 3.192.191.713-1.859 1.376-1.256 1.855.93.165 1.8 1.307 2.902 1.129 1.207.071 2.59-.383 3.5-.926-.035-1.149.169-2.347-.412-3.398-.518-1.427-.734 1-1.853.31-.687-.386-1.105-.672-.926-1.547-.044-.992.102-2.03-.103-2.984-.153-1.02-1.793-.995-2.676-1.133-.998.443-.484-1.077-.53-.98.7-1.214 1.344-2.584 2.176-3.653.46.344 1.18 1.196 1.903 1.532.689.835 2.046 1.048 2.763.187.693-.538 1.118-1.904 1.432-2.111 1.016-.33.603-1.846-.332-1.975.837-1.114-.746-1.372-1.338-1.646-.448.07-2.044-.785-1.75-.824.6-.402-.108-.84.102-1.647.205-1.115 1.591-1.002 2.472-1.545 1.475-.868-1.064-2.426.617-3.09.102-.811-.57-.842-.72-1.338.086-.593-.113-1.066-.309-1.543-.88-1.325-1.257.172-1.558 1.012-.363 1.159-.854 2.522-1.737 3.31-.198.917-1.203 2.057-1.75.928.226-1.05.127-1.7-.926-1.914-.281-.412-.286-1.306-.103-1.69.55-.672 1.475-.561 1.955-1.235.705-.305.746-1.822 0-1.852-.656-.463-1.455-1.582-2.3-1.607zm44.516 0c-.846.026-1.645 1.143-2.301 1.606-.746.03-.705 1.549 0 1.853.48.675 1.406.564 1.955 1.237.183.382.178 1.277-.103 1.69-1.053.213-1.152.863-.926 1.913-.547 1.129-1.552-.012-1.75-.928-.883-.788-1.374-2.152-1.737-3.31-.3-.84-.678-2.34-1.558-1.014-.196.477-.395.952-.309 1.545-.15.495-.822.526-.72 1.338 1.68.664-.858 2.222.617 3.09.88.542 2.267.427 2.472 1.543.21.808-.497 1.246.102 1.648.294.04-1.302.895-1.75.824-.592.275-2.175.532-1.338 1.647-.935.128-1.348 1.644-.332 1.975.314.207.739 1.573 1.432 2.11.717.862 2.074.647 2.763-.189.722-.335 1.443-1.185 1.903-1.529.831 1.067 1.476 2.434 2.174 3.647-.013 0 .44 1.415-.528.986-.883.138-2.523.112-2.676 1.133-.205.954-.059 1.992-.103 2.984.179.874-.24 1.158-.926 1.545-1.119.69-1.335-1.735-1.853-.309-.58 1.052-.377 2.25-.412 3.399.91.543 2.293.997 3.5.926 1.103.178 1.973-.965 2.902-1.13.603-.479-1.447-1.141-1.256-1.855.035-1.068-.044-2.134-.102-3.191.643-.302.96-.118.823.719-.054.819-.037 1.782.724 2.16.699.55.744 1.496 1.13 2.37.289.82.012 1.76-.21 2.554-.473.54-1.729.57-1.644 1.42-.025.697.327 1.24.207 1.976.175 1.009 2.379 1.343 2.261-.236.8-.919.945-2.214 1.598-3.125.216-.763.82-1.466.774-2.444v-2.101c-.076-1.04-.55-2.022-.516-3.088-.115-.334.705 1.498.62 2.059.012.834 1.398 2.598 1.429.767.587-.763 1.375-1.312 1.142-2.412v-2.371c.217-.537.052-1.227.104-1.832v-4.963c.275-1.305 1.445-2.366 1.236-3.787.193-.955-.612-1.608-1.34-2.08-.784-.507-.443.644-.513 1.09-.026.768.077 1.58-.104 2.287.068.944-.228 1.743-.514 2.594-.605.357-.798.721-.72 1.543.013 1.005-.055 2.042.101 2.988.33.79.191 1.757.412 2.572-.103.752.46 1.629.104 2.266-1.086.632-1.137-.838-1.441-1.545-.372-.962-.887-2.082-1.237-2.883-.24-1.018-.803-2.161-1.172-3.299-.5-1.12-.283-2.18-.27-3.328.056-.714-.154-1.53.202-2.125.567-.753.64-1.791.83-2.572.059-.615-.17-1.112-.103-1.729.083-.694-.092-1.26-.516-1.668-.332-.975.919-1.232.412-2.162-.175-.965-.58-2.181-.103-3.09.085-.512.63-.983.103-1.543-.902.015-2.012.332-2.78-.156-.056-.01-.113-.012-.17-.01zm-34.79 5.39v17.991c0 6.958 5.61 12.6 12.532 12.6 6.921 0 12.532-5.642 12.531-12.6v-17.99zm.452 29.317a1.4 1.4 0 00-.574.133c-1.156.11-1.582 1.782-2.797 1.488-.987-.59-1.445 1.211-.502 1.397.424.535.836 1.127 1.422 1.486-6.91-.338-10.592-1.611-10.592-1.611.423 2.006.636 3.861 0 5.098 0 0 4.892 1.691 13.992 1.691 9.1 0 19.219-1.455 26.717-1.455 7.498 0 9.537 1.455 9.537 1.455-.396-1.401-.685-2.883 0-5.096 0 0-1.983-1.41-9.209-1.453.08-.12.157-.243.229-.369.708-.055.078-1.042-.147-1.379-1.236 1.111-1.955-.82-3.004-1.133-.973-.56-3.512.084-2.474 1.434.51.592.149 1.294-.606.787-1.157-.316-2.51.044-3.197.887-.17-.548-.707-1.28-.947-.307-.332.76-.998.585-.727-.205-.373-1.506-3.195-.419-1.676.746-.47.556-2.604.1-1.392-.863.525-.674-.072-1.72-.659-.656-.553.468-1.04 2.959-1.78 1.464-.308-.645-1.035-2.176-1.723-1.75.148.825 1.202 2.34-.442 1.987-.738.215-1.06-.308-.357-.682.224-1.51-2.644-1.163-2.092.418-.573.485-.69-1.624-1.51-.596-.327 1.097-.903-.834-1.808-.535-.686-.292-2.731.87-2.3-.465 1.2-.914-.255-1.946-1.382-1.916zm9.271 2.805l-.003.021zm5.616 0l.002.021z%22/%3E %3Cpath fill=%22none%22 d=%22M616.063-229.45s-19.799-14.141-92.63-14.141c-72.833 0-171.12 14.142-259.509 14.142-88.388 0-135.893-16.438-135.893-16.438 6.175-12.009 4.107-30.012 0-49.498 0 0 47.505 16.438 135.893 16.438s186.676-14.142 259.508-14.142 92.631 14.142 92.631 14.142c-6.654 21.49-3.849 35.885 0 49.498z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-ita-mibac{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 d=%22M30.333 0c-2.038.777-5.011.9-6.296 1.932C21.898.845 22.13 3.129 21.941 3.44c-2.834-1.096.312 3.695-2.301 1.53-1.463 1.483-2.776-2.772-2.958.5-.336 1.576-1.55 2.906-2.411 1.237-1.258-.47-1.294-3.868-2.414-1.078-.23 3.548-6.552.574-5.698 4.106 2.332 1.41-.494 3.275-1.086 3.73 2.747 1.525-.643 5.362 3.002 5.757 2.684-.276.144 1.433 1.237 2.36 2.98-.145 4.12-5.818 7.549-3.489 2.196 1.381 5.189 2.538 5.098 5.507.896 1.315.17 3.456 1.843 3.937 1.111.681 1.67 2.425 1.941 2.707 3.367.791 4.577 4.81 7.552 6.402 2.06.69 5.108.291 5.566 3.31 1.845-.877.92 2.146 2.694.887 1.865.923 1.107 4.238 4.088 3.788 1.297 1.363 3.28 5.757 2.276 7.507-1.345.623-.872 2.94-2.03 3.698 1.408 2.932 4.08-1.39 4.459-3.12.56-2.335 4.26-1.664 1.986-4.607-1.224-1.336-3.36-1.734-1.794-3.904.453-2.184 2.617-2.798 3.993-1.455 2.022-.471 3.391 5.54 4.394 1.738-.419-3.44-4.418-4.727-6.968-6.335-1.781-.735-6.323-2.15-3.646-4.197-.839-2.147-4.687.465-6.298-1.895-2.546-1.748-4.23-4.364-4.698-7.426-1.01-3.574-6.77-3.743-6.716-7.992-1.055-1.63 2.211-1.957.34-3.497-.954-1.05.72-3.765.655-2.356 2.433-.589 3.195-2.72 5.615-1.48 1.4 1.723-.621-1.31-.45-1.264-1.293-1.154.955-1.941-.826-2.237-.032-1.179 2.253-2.251-.193-2.234-1.877.062-6.639-.972-5.353-3.36L30.333 0zM16.641 37.157c-.972 1.768-3.476 2.719-4.84 2.582.016 1.888 2.138 3.61 1.249 5.37.696 1.788-1.47 6.907 2.068 5.866.59-2.577 3.66-.06 3.51-2.975-.596-2.899 1.494-6.423-.452-8.816.512-1.086-1.026-1.395-1.535-2.027zm2.325 2.013c-1.444-.19-.416.642 0 0zm26.568 15.48c-2.47 1.015-5.208 1.696-7.86 1.785-1.738-.474-2.625-1.488-4.233-.351-2.767-1.675-2.33 3.99.588 3.14 2.893 1.72 6.007 3.487 8.96 4.776 2.757-1.698-1.014-5.466 1.987-7.524.253-.43 1.383-1.423.558-1.826z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-nazi{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 fill-rule=%22evenodd%22 d=%22M32.008 0L12.805 19.195l6.398 6.399L38.406 6.398 32.008 0zm12.797 12.797l-6.399 6.398 19.196 19.203L64 32 44.805 12.797zM31.992 19.5c-.893 0-1.616.278-2.172.832-.555.554-.836 1.288-.836 2.195 0 1.002.284 2.655.852 4.965l.945 3.86c.471 1.91.744 3.529.828 4.873h.797c.157-1.886.404-3.506.742-4.873l.961-3.86c.604-2.451.907-4.122.907-5.017 0-.86-.288-1.568-.868-2.121-.567-.566-1.286-.854-2.156-.854zM6.398 25.602L0 32l19.203 19.203 6.39-6.398L6.399 25.602zm38.407 12.796L25.602 57.602 31.992 64l19.211-19.203-6.398-6.399zm-12.774.36c-.82 0-1.518.28-2.086.846-.567.565-.851 1.247-.851 2.037s.284 1.461.851 2.027c.58.554 1.277.832 2.086.832.81 0 1.495-.278 2.063-.832.58-.566.867-1.238.867-2.027a2.71 2.71 0 00-.867-2.037 2.81 2.81 0 00-2.063-.846z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-personality{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 d=%22M0 0v64h64V0H0zm6 6h52v52h-6.06a25.206 60.413 0 00-10.19-18.945c-2.754 1.99-6.12 3.181-9.758 3.181-3.624 0-6.978-1.18-9.726-3.158A25.206 60.413 0 0012.06 58H6V6zm25.992 6C24.541 12 18.5 18.04 18.5 25.492c0 7.452 6.04 13.492 13.492 13.492s13.492-6.04 13.492-13.492c0-7.451-6.04-13.492-13.492-13.492z%22/%3E %3C/svg%3E")}.mw-mmv-restriction-trademarked:after{content:'\002122'}.mw-mmv-restriction-default{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22 viewBox=%220 0 64 64%22%3E %3Cpath fill=%22%23222%22 fill-rule=%22evenodd%22 d=%22M32.166 3.814A3.556 3.556 0 0028.92 5.59L.477 54.855a3.556 3.556 0 003.08 5.334h56.886a3.556 3.556 0 003.08-5.334L35.08 5.59a3.556 3.556 0 00-2.914-1.776zm-.176 18.377c.938 0 1.712.313 2.324.938.625.612.938 1.393.938 2.344 0 .99-.326 2.838-.977 5.547l-1.035 4.257c-.364 1.51-.631 3.308-.8 5.391h-.86c-.091-1.484-.39-3.281-.898-5.39l-1.016-4.258c-.612-2.553-.918-4.382-.918-5.489 0-1.002.3-1.81.898-2.422.6-.612 1.38-.918 2.344-.918zm.04 21.27a3 3 0 012.226.937c.625.612.937 1.361.937 2.247 0 .872-.312 1.62-.937 2.246a3.032 3.032 0 01-2.227.918 3.097 3.097 0 01-2.246-.918 3.097 3.097 0 01-.918-2.246c0-.873.306-1.622.918-2.247.612-.625 1.36-.937 2.246-.937z%22/%3E %3C/svg%3E")}.mw-mmv-permission-link{cursor:pointer}.jq-fullscreened .mw-mmv-permission-link{display:none}.mw-mmv-optout-link.pending{cursor:wait;color:#54595d}  .mw-mmv-post-image{-webkit-animation:mw-mmv-appear-animation 0.5s ease 0s 1 normal forwards;-moz-animation:mw-mmv-appear-animation 0.5s ease 0s 1 normal forwards;animation:mw-mmv-appear-animation 0.5s ease 0s 1 normal forwards;transition:box-shadow 0.25s}.mw-mmv-post-image.invite{-webkit-animation:mw-mmv-invite-animation 0.9s ease 0.2s 1 normal forwards;-moz-animation:mw-mmv-invite-animation 0.9s ease 0.2s 1 normal forwards;animation:mw-mmv-invite-animation 0.9s ease 0.2s 1 normal forwards}.jq-fullscreened .mw-mmv-post-image{-webkit-animation:none;-moz-animation:none;animation:none}.mw-mmv-post-image.mw-mmv-untruncated,.jq-fullscreened .mw-mmv-post-image{box-shadow:0 -4px 0 rgba(0,0,0,0.2)}@-webkit-keyframes mw-mmv-appear-animation{0%{opacity:0.6}50%{opacity:0.9}100%{opacity:1}}@-moz-keyframes mw-mmv-appear-animation{0%{opacity:0.6}50%{opacity:0.9}100%{opacity:1}}@keyframes mw-mmv-appear-animation{0%{opacity:0.6}50%{opacity:0.9}100%{opacity:1}}@-webkit-keyframes mw-mmv-invite-animation{0%{margin-top:0}30%{margin-top:-15px}85%{margin-top:0}}@-moz-keyframes mw-mmv-invite-animation{0%{margin-top:0}30%{margin-top:-15px}85%{margin-top:0}}@keyframes mw-mmv-invite-animation{0%{margin-top:0}30%{margin-top:-15px}85%{margin-top:0}}  .mw-mmv-wrapper{top:0;left:0;right:0;z-index:1001;position:absolute;bottom:auto}.skin-monobook .mw-mmv-wrapper{font-size:medium}.mw-mmv-main{width:100%;height:auto;position:relative}.mw-mmv-main .jq-fullscreened{background-color:#000}.mw-mmv-image-wrapper{position:fixed;top:0;bottom:86px;left:0;right:0;overflow-y:hidden}.mw-mmv-image-inner-wrapper{display:table;width:100%;height:100%}.mw-mmv-pre-image{position:absolute;top:0;height:32px;width:100%;z-index:1}.mw-mmv-post-image{position:absolute;width:100%;bottom:auto;height:auto;color:#222;background-color:#ffffff;min-height:57px;z-index:1005}.mw-mmv-above-fold{width:100%;height:56px;min-height:56px;position:relative;padding-bottom:30px}.mw-mmv-untruncated .mw-mmv-above-fold{height:auto}.cursor-hidden{cursor:none}.mw-mmv-main.jq-fullscreened{background-color:#000}.jq-fullscreened .mw-mmv-image-wrapper,.jq-fullscreened .mw-mmv-post-image{bottom:0}.jq-fullscreened .mw-mmv-post-image{position:fixed;min-height:0;opacity:0;transition:opacity 0.25s}.jq-fullscreened .mw-mmv-post-image:hover{opacity:1}</style><meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="./k-means clustering - Wikipedia_files/load(2).php">
<meta name="generator" content="MediaWiki 1.35.0-wmf.5">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-crossorigin">
<meta name="referrer" content="origin-when-cross-origin">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png">
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/K-means_clustering">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit">
<link rel="edit" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit">
<link rel="apple-touch-icon" href="https://en.wikipedia.org/static/apple-touch/wikipedia.png">
<link rel="shortcut icon" href="https://en.wikipedia.org/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="https://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://en.wikipedia.org/w/api.php?action=rsd">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/">
<link rel="canonical" href="https://en.wikipedia.org/wiki/K-means_clustering">
<link rel="dns-prefetch" href="https://login.wikimedia.org/">
<link rel="dns-prefetch" href="https://meta.wikimedia.org/">
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-K-means_clustering rootpage-K-means_clustering skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><div id="centralNotice"></div><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en"><i>k</i>-means clustering</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="https://en.wikipedia.org/wiki/K-means_clustering#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="https://en.wikipedia.org/wiki/K-means_clustering#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Vector quantization algorithm minimizing the sum of squared deviations</div>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br><a href="https://en.wikipedia.org/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="https://en.wikipedia.org/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="./k-means clustering - Wikipedia_files/220px-Kernel_Machine.svg.png" decoding="async" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233"></a></td></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame1"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems<a class="NavToggle" id="NavToggle1" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="https://en.wikipedia.org/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="https://en.wikipedia.org/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="https://en.wikipedia.org/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame2"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="padding:0.1em 0;line-height:1.2em;"><a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&nbsp; <b><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div><a class="NavToggle" id="NavToggle2" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="https://en.wikipedia.org/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame3"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a><a class="NavToggle" id="NavToggle3" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="https://en.wikipedia.org/wiki/CURE_data_clustering_algorithm" class="mw-redirect" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a class="mw-selflink selflink"><i>k</i>-means</a></li>
<li><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectationmaximization algorithm">Expectationmaximization (EM)</a></li>
<li><br><a href="https://en.wikipedia.org/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="https://en.wikipedia.org/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame4"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a><a class="NavToggle" id="NavToggle4" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">CCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame5"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a><a class="NavToggle" id="NavToggle5" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="https://en.wikipedia.org/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame6"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a><a class="NavToggle" id="NavToggle6" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame7"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a><a class="NavToggle" id="NavToggle7" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="https://en.wikipedia.org/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame8"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a><a class="NavToggle" id="NavToggle8" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="Stateactionrewardstateaction">SARSA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame9"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory<a class="NavToggle" id="NavToggle9" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma" class="mw-redirect" title="Biasvariance dilemma">Biasvariance dilemma</a></li>
<li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="VapnikChervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame10"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues<a class="NavToggle" id="NavToggle10" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a rel="nofollow" class="external text" href="https://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame11"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a><a class="NavToggle" id="NavToggle11" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame12"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles<a class="NavToggle" id="NavToggle12" href="https://en.wikipedia.org/wiki/K-means_clustering#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="https://en.wikipedia.org/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="https://en.wikipedia.org/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="https://en.wikipedia.org/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table><p><b><i>k</i>-means clustering</b> is a method of <a href="https://en.wikipedia.org/wiki/Vector_quantization" title="Vector quantization">vector quantization</a>, originally from <a href="https://en.wikipedia.org/wiki/Signal_processing" title="Signal processing">signal processing</a>, that is popular for <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> in <a href="https://en.wikipedia.org/wiki/Data_mining" title="Data mining">data mining</a>. <i>k</i>-means clustering aims to <a href="https://en.wikipedia.org/wiki/Partition_of_a_set" title="Partition of a set">partition</a> <i>n</i> observations into <i>k</i> clusters in which each observation belongs to the <a href="https://en.wikipedia.org/wiki/Cluster_(statistics)" class="mw-redirect" title="Cluster (statistics)">cluster</a> with the nearest <a href="https://en.wikipedia.org/wiki/Mean" title="Mean">mean</a>, serving as a prototype of the cluster. This results in a partitioning of the data space into <a href="https://en.wikipedia.org/wiki/Voronoi_cell" class="mw-redirect" title="Voronoi cell">Voronoi cells</a>. <i>k</i>-Means minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult <a href="https://en.wikipedia.org/wiki/Weber_problem" title="Weber problem">Weber problem</a>: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. Better Euclidean solutions can for example be found using <a href="https://en.wikipedia.org/wiki/K-medians_clustering" title="K-medians clustering">k-medians</a> and <a href="https://en.wikipedia.org/wiki/K-medoids" title="K-medoids">k-medoids</a>.
</p><p>The problem is computationally difficult (<a href="https://en.wikipedia.org/wiki/NP-hardness" title="NP-hardness">NP-hard</a>); however, efficient <a href="https://en.wikipedia.org/wiki/Heuristic_algorithm" class="mw-redirect" title="Heuristic algorithm">heuristic algorithms</a> converge quickly to a <a href="https://en.wikipedia.org/wiki/Local_optimum" title="Local optimum">local optimum</a>. These are usually similar to the <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a> for <a href="https://en.wikipedia.org/wiki/Mixture_model" title="Mixture model">mixtures</a> of <a href="https://en.wikipedia.org/wiki/Gaussian_distribution" class="mw-redirect" title="Gaussian distribution">Gaussian distributions</a> via an iterative refinement approach employed by both <i>k-means</i> and <i>Gaussian mixture modeling</i>. They both use cluster centers to model the data; however, <i>k</i>-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.
</p><p>The algorithm has a loose relationship to the <a href="https://en.wikipedia.org/wiki/K-nearest_neighbor" class="mw-redirect" title="K-nearest neighbor"><i>k</i>-nearest neighbor classifier</a>, a popular <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">machine learning</a> technique for classification that is often confused with <i>k</i>-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by <i>k</i>-means classifies new data into the existing clusters. This is known as <a href="https://en.wikipedia.org/wiki/Nearest_centroid_classifier" title="Nearest centroid classifier">nearest centroid classifier</a> or <a href="https://en.wikipedia.org/wiki/Rocchio_algorithm" title="Rocchio algorithm">Rocchio algorithm</a>.
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none"><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Description"><span class="tocnumber">1</span> <span class="toctext">Description</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="https://en.wikipedia.org/wiki/K-means_clustering#History"><span class="tocnumber">2</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Algorithms"><span class="tocnumber">3</span> <span class="toctext">Algorithms</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm_(naive_k-means)"><span class="tocnumber">3.1</span> <span class="toctext">Standard algorithm (naive k-means)</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods"><span class="tocnumber">3.1.1</span> <span class="toctext">Initialization methods</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-6"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Complexity"><span class="tocnumber">3.2</span> <span class="toctext">Complexity</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Variations"><span class="tocnumber">3.3</span> <span class="toctext">Variations</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Hartigan%E2%80%93Wong_method"><span class="tocnumber">3.4</span> <span class="toctext">HartiganWong method</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-9"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Discussion"><span class="tocnumber">4</span> <span class="toctext">Discussion</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Applications"><span class="tocnumber">5</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Vector_quantization"><span class="tocnumber">5.1</span> <span class="toctext">Vector quantization</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Cluster_analysis"><span class="tocnumber">5.2</span> <span class="toctext">Cluster analysis</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Feature_learning"><span class="tocnumber">5.3</span> <span class="toctext">Feature learning</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Relation_to_other_algorithms"><span class="tocnumber">6</span> <span class="toctext">Relation to other algorithms</span></a>
<ul>
<li class="toclevel-2 tocsection-15"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Gaussian_mixture_model"><span class="tocnumber">6.1</span> <span class="toctext">Gaussian mixture model</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="https://en.wikipedia.org/wiki/K-means_clustering#K-SVD"><span class="tocnumber">6.2</span> <span class="toctext">K-SVD</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Principal_component_analysis"><span class="tocnumber">6.3</span> <span class="toctext">Principal component analysis</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Mean_shift_clustering"><span class="tocnumber">6.4</span> <span class="toctext">Mean shift clustering</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Independent_component_analysis"><span class="tocnumber">6.5</span> <span class="toctext">Independent component analysis</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Bilateral_filtering"><span class="tocnumber">6.6</span> <span class="toctext">Bilateral filtering</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-21"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Similar_problems"><span class="tocnumber">7</span> <span class="toctext">Similar problems</span></a></li>
<li class="toclevel-1 tocsection-22"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Software_implementations"><span class="tocnumber">8</span> <span class="toctext">Software implementations</span></a>
<ul>
<li class="toclevel-2 tocsection-23"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Free_Software/Open_Source"><span class="tocnumber">8.1</span> <span class="toctext">Free Software/Open Source</span></a></li>
<li class="toclevel-2 tocsection-24"><a href="https://en.wikipedia.org/wiki/K-means_clustering#Proprietary"><span class="tocnumber">8.2</span> <span class="toctext">Proprietary</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-25"><a href="https://en.wikipedia.org/wiki/K-means_clustering#See_also"><span class="tocnumber">9</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-26"><a href="https://en.wikipedia.org/wiki/K-means_clustering#References"><span class="tocnumber">10</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Description">Description</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=1" title="Edit section: Description">edit</a><span class="mw-editsection-bracket">]</span></span></h2><p>
Given a set of observations (<b>x</b><sub>1</sub>, <b>x</b><sub>2</sub>, ..., <b>x</b><sub><i>n</i></sub>), where each observation is a <i>d</i>-dimensional real vector, <i>k</i>-means clustering aims to partition the <i>n</i> observations into <i>k</i> (&nbsp;<i>n</i>) sets <b>S</b>&nbsp;=&nbsp;{<i>S</i><sub>1</sub>,&nbsp;<i>S</i><sub>2</sub>,&nbsp;...,&nbsp;<i>S<sub>k</sub></i>} so as to minimize the within-cluster sum of squares (WCSS) (i.e. <a href="https://en.wikipedia.org/wiki/Variance" title="Variance">variance</a>). Formally, the objective is to find:</p><center><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}={\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}|S_{i}|\operatorname {Var} S_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <munder>
            <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
              <mi mathvariant="normal">a</mi>
              <mi mathvariant="normal">r</mi>
              <mi mathvariant="normal">g</mi>
              <mspace width="thinmathspace"></mspace>
              <mi mathvariant="normal">m</mi>
              <mi mathvariant="normal">i</mi>
              <mi mathvariant="normal">n</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">S</mi>
            </mrow>
          </munder>
        </mrow>
        <munderover>
          <mo><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </munderover>
        <munder>
          <mo><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mo><!--  --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </munder>
        <msup>
          <mrow>
            <mo symmetric="true"></mo>
            <mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">x</mi>
              </mrow>
              <mo><!--  --></mo>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic"><!--  --></mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
            </mrow>
            <mo symmetric="true"></mo>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <munder>
            <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
              <mi mathvariant="normal">a</mi>
              <mi mathvariant="normal">r</mi>
              <mi mathvariant="normal">g</mi>
              <mspace width="thinmathspace"></mspace>
              <mi mathvariant="normal">m</mi>
              <mi mathvariant="normal">i</mi>
              <mi mathvariant="normal">n</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">S</mi>
            </mrow>
          </munder>
        </mrow>
        <munderover>
          <mo><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>Var</mi>
        <mo><!--  --></mo>
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}={\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}|S_{i}|\operatorname {Var} S_{i}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/8dc15ec63e0676fc07e790f61efd89484a6b7922" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:50.878ex; height:7.676ex;" alt="{\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}={\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}|S_{i}|\operatorname {Var} S_{i}}"></span></center><p>where <i><b></b><sub>i</sub></i> is the mean of points in <i>S<sub>i</sub></i>. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:</p><center><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\,{\frac {1}{2|S_{i}|}}\,\sum _{\mathbf {x} ,\mathbf {y} \in S_{i}}\left\|\mathbf {x} -\mathbf {y} \right\|^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <munder>
            <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
              <mi mathvariant="normal">a</mi>
              <mi mathvariant="normal">r</mi>
              <mi mathvariant="normal">g</mi>
              <mspace width="thinmathspace"></mspace>
              <mi mathvariant="normal">m</mi>
              <mi mathvariant="normal">i</mi>
              <mi mathvariant="normal">n</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">S</mi>
            </mrow>
          </munder>
        </mrow>
        <munderover>
          <mo><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </munderover>
        <mspace width="thinmathspace"></mspace>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mrow>
              <mn>2</mn>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">|</mo>
              </mrow>
              <msub>
                <mi>S</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">|</mo>
              </mrow>
            </mrow>
          </mfrac>
        </mrow>
        <mspace width="thinmathspace"></mspace>
        <munder>
          <mo><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mo>,</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">y</mi>
            </mrow>
            <mo><!--  --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </munder>
        <msup>
          <mrow>
            <mo symmetric="true"></mo>
            <mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">x</mi>
              </mrow>
              <mo><!--  --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">y</mi>
              </mrow>
            </mrow>
            <mo symmetric="true"></mo>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\,{\frac {1}{2|S_{i}|}}\,\sum _{\mathbf {x} ,\mathbf {y} \in S_{i}}\left\|\mathbf {x} -\mathbf {y} \right\|^{2}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/9fb2388a00fcf4f1df3117883fccd0c4028da33d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:32.916ex; height:7.676ex;" alt="{\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\,{\frac {1}{2|S_{i}|}}\,\sum _{\mathbf {x} ,\mathbf {y} \in S_{i}}\left\|\mathbf {x} -\mathbf {y} \right\|^{2}}"></span></center><p>The equivalence can be deduced from identity <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}=\sum _{\mathbf {x} \neq \mathbf {y} \in S_{i}}(\mathbf {x} -{\boldsymbol {\mu }}_{i})({\boldsymbol {\mu }}_{i}-\mathbf {y} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mo><!--  --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </munder>
        <msup>
          <mrow>
            <mo symmetric="true"></mo>
            <mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">x</mi>
              </mrow>
              <mo><!--  --></mo>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic"><!--  --></mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
            </mrow>
            <mo symmetric="true"></mo>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>=</mo>
        <munder>
          <mo><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mo><!--  --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">y</mi>
            </mrow>
            <mo><!--  --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </munder>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo><!--  --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold-italic"><!--  --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold-italic"><!--  --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo><!--  --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">y</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}=\sum _{\mathbf {x} \neq \mathbf {y} \in S_{i}}(\mathbf {x} -{\boldsymbol {\mu }}_{i})({\boldsymbol {\mu }}_{i}-\mathbf {y} )}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/009145430778f69c28e5852d434f0d07e5ca896d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.505ex; width:40.302ex; height:6.009ex;" alt="{\displaystyle \sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}=\sum _{\mathbf {x} \neq \mathbf {y} \in S_{i}}(\mathbf {x} -{\boldsymbol {\mu }}_{i})({\boldsymbol {\mu }}_{i}-\mathbf {y} )}"></span>. Because the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in <i>different</i> clusters (between-cluster sum of squares, BCSS),<sup id="cite_ref-:12_1-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-:12-1">[1]</a></sup> which follows from the <a href="https://en.wikipedia.org/wiki/Law_of_total_variance" title="Law of total variance">law of total variance</a>.
</p><h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=2" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The term "<i>k</i>-means" was first used by James MacQueen in 1967,<sup id="cite_ref-macqueen19672_2-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-macqueen19672-2">[2]</a></sup> though the idea goes back to <a href="https://en.wikipedia.org/wiki/Hugo_Steinhaus" title="Hugo Steinhaus">Hugo Steinhaus</a> in 1956.<sup id="cite_ref-3" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-3">[3]</a></sup> The standard algorithm was first proposed by Stuart Lloyd of <a href="https://en.wikipedia.org/wiki/Bell_Labs" title="Bell Labs">Bell Labs</a> in 1957 as a technique for <a href="https://en.wikipedia.org/wiki/Pulse-code_modulation" title="Pulse-code modulation">pulse-code modulation</a>, though it wasn't published as a journal article until 1982.<sup id="cite_ref-lloyd19572_4-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-lloyd19572-4">[4]</a></sup> In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as Lloyd-Forgy.<sup id="cite_ref-forgy652_5-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-forgy652-5">[5]</a></sup>
</p>
<h2><span class="mw-headline" id="Algorithms">Algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=3" title="Edit section: Algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span id="Standard_algorithm_.28naive_k-means.29"></span><span class="mw-headline" id="Standard_algorithm_(naive_k-means)">Standard algorithm (naive k-means)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=4" title="Edit section: Standard algorithm (naive k-means)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="https://en.wikipedia.org/wiki/File:K-means_convergence.gif" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/K-means_convergence.gif" decoding="async" width="220" height="214" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/330px-K-means_convergence.gif 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/440px-K-means_convergence.gif 2x" data-file-width="637" data-file-height="619"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:K-means_convergence.gif" class="internal" title="Enlarge"></a></div>Convergence of <i>k</i>-means</div></div></div>
<p>The most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called "the <i>k</i>-means algorithm"; it is also referred to as <a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm" title="Lloyd&#39;s algorithm">Lloyd's algorithm</a>, particularly in the computer science community. It is sometimes also referred to as "naive <i>k</i>-means", because there exist much faster alternatives.<sup id="cite_ref-6" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-6">[6]</a></sup>
</p><p>Given an initial set of <i>k</i> means <i>m</i><sub>1</sub><sup>(1)</sup>,...,<i>m<sub>k</sub></i><sup>(1)</sup> (see below), the algorithm proceeds by alternating between two steps:<sup id="cite_ref-7" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-7">[7]</a></sup>
</p>
<dl><dd><b>Assignment step</b>: Assign each observation to the cluster whose mean has the least squared <a href="https://en.wikipedia.org/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a>, this is intuitively the "nearest" mean.<sup id="cite_ref-8" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-8">[8]</a></sup> (Mathematically, this means partitioning the observations according to the <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" title="Voronoi diagram">Voronoi diagram</a> generated by the means).
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S_{i}^{(t)}={\big \{}x_{p}:{\big \|}x_{p}-m_{i}^{(t)}{\big \|}^{2}\leq {\big \|}x_{p}-m_{j}^{(t)}{\big \|}^{2}\ \forall j,1\leq j\leq k{\big \}},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo maxsize="1.2em" minsize="1.2em">{</mo>
          </mrow>
        </mrow>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
          </mrow>
        </msub>
        <mo>:</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo symmetric="true" maxsize="1.2em" minsize="1.2em"></mo>
          </mrow>
        </mrow>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
          </mrow>
        </msub>
        <mo><!--  --></mo>
        <msubsup>
          <mi>m</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mo symmetric="true" maxsize="1.2em" minsize="1.2em"></mo>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo><!--  --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo symmetric="true" maxsize="1.2em" minsize="1.2em"></mo>
          </mrow>
        </mrow>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
          </mrow>
        </msub>
        <mo><!--  --></mo>
        <msubsup>
          <mi>m</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mo symmetric="true" maxsize="1.2em" minsize="1.2em"></mo>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mtext>&nbsp;</mtext>
        <mi mathvariant="normal"><!--  --></mi>
        <mi>j</mi>
        <mo>,</mo>
        <mn>1</mn>
        <mo><!--  --></mo>
        <mi>j</mi>
        <mo><!--  --></mo>
        <mi>k</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo maxsize="1.2em" minsize="1.2em">}</mo>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S_{i}^{(t)}={\big \{}x_{p}:{\big \|}x_{p}-m_{i}^{(t)}{\big \|}^{2}\leq {\big \|}x_{p}-m_{j}^{(t)}{\big \|}^{2}\ \forall j,1\leq j\leq k{\big \}},}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/145a262c93066470be0e062683d64340a1b20121" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.338ex; width:56.409ex; height:4.009ex;" alt="S_{i}^{(t)}={\big \{}x_{p}:{\big \|}x_{p}-m_{i}^{(t)}{\big \|}^{2}\leq {\big \|}x_{p}-m_{j}^{(t)}{\big \|}^{2}\ \forall j,1\leq j\leq k{\big \}},"></span></dd>
<dd>where each <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x_{p}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{p}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/0bec554743fa797409a83ad8d00b4d35e110a50a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.389ex; height:2.343ex;" alt="x_{p}"></span> is assigned to exactly one <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S^{(t)}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S^{(t)}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/4cc468cbde601ba30acbde7fb11bd9bf8b04b8c8" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:3.627ex; height:2.843ex;" alt="S^{(t)}"></span>, even if it could be assigned to two or more of them.</dd></dl></dd>
<dd><b>Update step</b>: Calculate the new means (<a href="https://en.wikipedia.org/wiki/Centroids" class="mw-redirect" title="Centroids">centroids</a>) of the observations in the new clusters.
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle m_{i}^{(t+1)}={\frac {1}{\left|S_{i}^{(t)}\right|}}\sum _{x_{j}\in S_{i}^{(t)}}x_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>m</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo>+</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mrow>
              <mo>|</mo>
              <msubsup>
                <mi>S</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>t</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo>|</mo>
            </mrow>
          </mfrac>
        </mrow>
        <munder>
          <mo><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </msub>
            <mo><!--  --></mo>
            <msubsup>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>t</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msubsup>
          </mrow>
        </munder>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle m_{i}^{(t+1)}={\frac {1}{\left|S_{i}^{(t)}\right|}}\sum _{x_{j}\in S_{i}^{(t)}}x_{j}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/964165b4792e24484c3ccde9b3e00c478c96fbd6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -4.505ex; width:23.627ex; height:7.843ex;" alt="{\displaystyle m_{i}^{(t+1)}={\frac {1}{\left|S_{i}^{(t)}\right|}}\sum _{x_{j}\in S_{i}^{(t)}}x_{j}}"></span></dd></dl></dd></dl>
<p>The algorithm has converged when the assignments no longer change. The algorithm does not guarantee to find the optimum.<sup id="cite_ref-hartigan19792_9-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hartigan19792-9">[9]</a></sup>
</p><p>The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may stop the algorithm from converging. Various modifications of <i>k</i>-means such as spherical <i>k</i>-means and <a href="https://en.wikipedia.org/wiki/K-medoids" title="K-medoids"><i>k</i>-medoids</a> have been proposed to allow using other distance measures.
</p>
<h4><span class="mw-headline" id="Initialization_methods">Initialization methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=5" title="Edit section: Initialization methods">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Commonly used initialization methods are Forgy and Random Partition.<sup id="cite_ref-hamerly4_10-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hamerly4-10">[10]</a></sup> The Forgy method randomly chooses <i>k</i> observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al.,<sup id="cite_ref-hamerly4_10-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hamerly4-10">[10]</a></sup> the Random Partition method is generally preferable for algorithms such as the <i>k</i>-harmonic means and fuzzy <i>k</i>-means. For expectation maximization and standard <i>k</i>-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al.,<sup id="cite_ref-11" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-11">[11]</a></sup> however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad's approach<sup id="cite_ref-12" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-12">[12]</a></sup> performs "consistently" in "the best group" and <a href="https://en.wikipedia.org/wiki/K-means%2B%2B" title="K-means++"><i>k</i>-means++</a> performs "generally well".
</p>
<ul class="gallery mw-gallery-traditional center">
	<li class="gallerycaption">Demonstration of the standard algorithm</li>
		<li class="gallerybox" style="width: 185px"><div style="width: 185px">
			<div class="thumb" style="width: 180px;"><div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:K_Means_Example_Step_1.svg" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/124px-K_Means_Example_Step_1.svg.png" decoding="async" width="124" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/5e/K_Means_Example_Step_1.svg/187px-K_Means_Example_Step_1.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/5e/K_Means_Example_Step_1.svg/249px-K_Means_Example_Step_1.svg.png 2x" data-file-width="197" data-file-height="190"></a></div></div>
			<div class="gallerytext">
<p>1. <i>k</i> initial "means" (in this case <i>k</i>=3) are randomly generated within the data domain (shown in color).
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 185px"><div style="width: 185px">
			<div class="thumb" style="width: 180px;"><div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:K_Means_Example_Step_2.svg" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/139px-K_Means_Example_Step_2.svg.png" decoding="async" width="139" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/a/a5/K_Means_Example_Step_2.svg/209px-K_Means_Example_Step_2.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a5/K_Means_Example_Step_2.svg/278px-K_Means_Example_Step_2.svg.png 2x" data-file-width="197" data-file-height="170"></a></div></div>
			<div class="gallerytext">
<p>2. <i>k</i> clusters are created by associating every observation with the nearest mean. The partitions here represent the <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" title="Voronoi diagram">Voronoi diagram</a> generated by the means.
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 185px"><div style="width: 185px">
			<div class="thumb" style="width: 180px;"><div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:K_Means_Example_Step_3.svg" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/139px-K_Means_Example_Step_3.svg.png" decoding="async" width="139" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/K_Means_Example_Step_3.svg/209px-K_Means_Example_Step_3.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3e/K_Means_Example_Step_3.svg/278px-K_Means_Example_Step_3.svg.png 2x" data-file-width="197" data-file-height="170"></a></div></div>
			<div class="gallerytext">
<p>3. The <a href="https://en.wikipedia.org/wiki/Centroid" title="Centroid">centroid</a> of each of the <i>k</i> clusters becomes the new mean.
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 185px"><div style="width: 185px">
			<div class="thumb" style="width: 180px;"><div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:K_Means_Example_Step_4.svg" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/139px-K_Means_Example_Step_4.svg.png" decoding="async" width="139" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/K_Means_Example_Step_4.svg/209px-K_Means_Example_Step_4.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d2/K_Means_Example_Step_4.svg/278px-K_Means_Example_Step_4.svg.png 2x" data-file-width="197" data-file-height="170"></a></div></div>
			<div class="gallerytext">
<p>4. Steps 2 and 3 are repeated until convergence has been reached.
</p>
			</div>
		</div></li>
</ul>
<p>The algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is <span class="texhtml">2<sup>(<var>n</var>)</sup></span>.<sup id="cite_ref-13" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-13">[13]</a></sup> These point sets do not seem to arise in practice: this is corroborated by the fact that the <a href="https://en.wikipedia.org/wiki/Smoothed_analysis" title="Smoothed analysis">smoothed</a> running time of <i>k</i>-means is polynomial.<sup id="cite_ref-Arthur,_David;_Manthey,_B.;_Roeglin,_H._20092_14-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-Arthur,_David;_Manthey,_B.;_Roeglin,_H._20092-14">[14]</a></sup>
</p><p>The "assignment" step is referred to as the "expectation step", while the "update step" is a maximization step, making this algorithm a variant of the <i>generalized</i> <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a>.
</p>
<h3><span class="mw-headline" id="Complexity">Complexity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=6" title="Edit section: Complexity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Finding the optimal solution to the <i>k</i>-means clustering problem for observations in <i>d</i> dimensions is:
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/NP-hard" class="mw-redirect" title="NP-hard">NP-hard</a> in general Euclidean space (of <i>d</i> dimensions) even for two clusters,<sup id="cite_ref-15" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-15">[15]</a></sup><sup id="cite_ref-16" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-16">[16]</a></sup><sup id="cite_ref-17" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-17">[17]</a></sup><sup id="cite_ref-18" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-18">[18]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/NP-hard" class="mw-redirect" title="NP-hard">NP-hard</a> for a general number of clusters <i>k</i> even in the plane,<sup id="cite_ref-19" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-19">[19]</a></sup></li>
<li>if <i>k</i> and <i>d</i> (the dimension) are fixed, the problem can be exactly solved in time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle O(n^{dk+1})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>O</mi>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
            <mi>k</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle O(n^{dk+1})}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/4c9b1d6a4a1b80be55aeb2a28076d9b6861d526c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:9.026ex; height:3.176ex;" alt="{\displaystyle O(n^{dk+1})}"></span>, where <i>n</i> is the number of entities to be clustered.<sup id="cite_ref-20" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-20">[20]</a></sup></li></ul>
<p>Thus, a variety of <a href="https://en.wikipedia.org/wiki/Heuristic_algorithm" class="mw-redirect" title="Heuristic algorithm">heuristic algorithms</a> such as Lloyd's algorithm given above are generally used.
</p><p>The running time of Lloyd's algorithm (and most variants) is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle O(nkdi)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>O</mi>
        <mo stretchy="false">(</mo>
        <mi>n</mi>
        <mi>k</mi>
        <mi>d</mi>
        <mi>i</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle O(nkdi)}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/7a1d9d1a265e27b3a21e050f1730aea5eb28e577" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.207ex; height:2.843ex;" alt="O(nkdi)"></span>,<sup id="cite_ref-hartigan19792_9-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hartigan19792-9">[9]</a></sup><sup id="cite_ref-21" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-21">[21]</a></sup> where:
</p>
<ul><li><i>n</i> is the number of <i>d</i>-dimensional vectors (to be clustered)</li>
<li><i>k</i> the number of clusters</li>
<li><i>i</i> the number of iterations needed until convergence.</li></ul>
<p>On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd's algorithm is therefore often considered to be of "linear" complexity in practice, although it is in the <a href="https://en.wikipedia.org/wiki/Worst-case_complexity" title="Worst-case complexity">worst case</a> superpolynomial when performed until convergence.<sup id="cite_ref-:02_22-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-:02-22">[22]</a></sup>
</p>
<ul><li>In the worst-case, Lloyd's algorithm needs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle i=2^{\Omega ({\sqrt {n}})}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
        <mo>=</mo>
        <msup>
          <mn>2</mn>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal"><!--  --></mi>
            <mo stretchy="false">(</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <msqrt>
                <mi>n</mi>
              </msqrt>
            </mrow>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i=2^{\Omega ({\sqrt {n}})}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/29ada99014383080debb79cc0308a7f661104313" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:10.117ex; height:2.843ex;" alt="{\displaystyle i=2^{\Omega ({\sqrt {n}})}}"></span> iterations, so that the worst-case complexity of Lloyd's algorithm is <a href="https://en.wikipedia.org/wiki/Time_complexity#Superpolynomial_time" title="Time complexity">superpolynomial</a>.<sup id="cite_ref-:02_22-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-:02-22">[22]</a></sup></li>
<li>Lloyd's <i>k</i>-means algorithm has polynomial smoothed running time. It is shown that<sup id="cite_ref-Arthur,_David;_Manthey,_B.;_Roeglin,_H._20092_14-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-Arthur,_David;_Manthey,_B.;_Roeglin,_H._20092-14">[14]</a></sup> for arbitrary set of <i>n</i> points in <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle [0,1]^{d}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">[</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <msup>
          <mo stretchy="false">]</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle [0,1]^{d}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/e13ae4917276744b214714a20b3cb8ee305e309d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.745ex; height:3.176ex;" alt="[0,1]^{d}"></span>, if each point is independently perturbed by a normal distribution with mean <span class="texhtml">0</span> and variance <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \sigma ^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi><!--  --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sigma ^{2}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/53a5c55e536acf250c1d3e0f754be5692b843ef5" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.385ex; height:2.676ex;" alt="\sigma ^{2}"></span>, then the expected running time of <span class="texhtml mvar" style="font-style:italic;">k</span>-means algorithm is bounded by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle O(n^{34}k^{34}d^{8}\log ^{4}(n)/\sigma ^{6})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>O</mi>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>34</mn>
          </mrow>
        </msup>
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>34</mn>
          </mrow>
        </msup>
        <msup>
          <mi>d</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>8</mn>
          </mrow>
        </msup>
        <msup>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>4</mn>
          </mrow>
        </msup>
        <mo><!--  --></mo>
        <mo stretchy="false">(</mo>
        <mi>n</mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <msup>
          <mi><!--  --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>6</mn>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle O(n^{34}k^{34}d^{8}\log ^{4}(n)/\sigma ^{6})}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/245c9d99b5cc1788e4e67fdf449d1cfcb160bf31" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:23.378ex; height:3.176ex;" alt="{\displaystyle O(n^{34}k^{34}d^{8}\log ^{4}(n)/\sigma ^{6})}"></span>, which is a polynomial in <span class="texhtml mvar" style="font-style:italic;">n</span>, <span class="texhtml mvar" style="font-style:italic;">k</span>, <span class="texhtml mvar" style="font-style:italic;">d</span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle 1/\sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mn>1</mn>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi><!--  --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 1/\sigma }</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/00d5187486468042e9692b18c216b60679aafef3" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:3.655ex; height:2.843ex;" alt="1/\sigma "></span>.</li>
<li>Better bounds are proven for simple cases. For example, in <sup id="cite_ref-23" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-23">[23]</a></sup> it is shown that the running time of <i>k</i>-means algorithm is bounded by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle O(dn^{4}M^{2})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>O</mi>
        <mo stretchy="false">(</mo>
        <mi>d</mi>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>4</mn>
          </mrow>
        </msup>
        <msup>
          <mi>M</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle O(dn^{4}M^{2})}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/8892c255d524bfb2ef96555e56746c603945dddd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:10.8ex; height:3.176ex;" alt="O(dn^{4}M^{2})"></span> for <span class="texhtml mvar" style="font-style:italic;">n</span> points in an <a href="https://en.wikipedia.org/wiki/Integer_lattice" title="Integer lattice">integer lattice</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \{1,\dots ,M\}^{d}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">{</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo><!--  --></mo>
        <mo>,</mo>
        <mi>M</mi>
        <msup>
          <mo fence="false" stretchy="false">}</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \{1,\dots ,M\}^{d}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/b357149a0b365e24b8636c26bec1e899fe5e64e7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:12.2ex; height:3.176ex;" alt="\{1,\dots ,M\}^{d}"></span>.</li></ul>
<p>Lloyd's algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naive implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd's algorithm.<sup id="cite_ref-hartigan19792_9-2" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hartigan19792-9">[9]</a></sup><sup id="cite_ref-phillips2_24-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-phillips2-24">[24]</a></sup><sup id="cite_ref-elkan2_25-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-elkan2-25">[25]</a></sup><sup id="cite_ref-hamerly22_26-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hamerly22-26">[26]</a></sup><sup id="cite_ref-hamerly32_27-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hamerly32-27">[27]</a></sup>
</p>
<h3><span class="mw-headline" id="Variations">Variations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=7" title="Edit section: Variations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><a href="https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization" title="Jenks natural breaks optimization">Jenks natural breaks optimization</a>: <i>k</i>-means applied to univariate data</li>
<li><a href="https://en.wikipedia.org/wiki/K-medians_clustering" title="K-medians clustering"><i>k</i>-medians clustering</a> uses the median in each dimension instead of the mean, and this way minimizes <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle L_{1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>L</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L_{1}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/0e79dc1b001f8b923df475ed14de023cbc456013" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.637ex; height:2.509ex;" alt="L_{1}"></span> norm (<a href="https://en.wikipedia.org/wiki/Taxicab_geometry" title="Taxicab geometry">Taxicab geometry</a>).</li>
<li><a href="https://en.wikipedia.org/wiki/K-medoids" title="K-medoids"><i>k</i>-medoids</a> (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for <i>arbitrary</i> distance functions.</li>
<li><a href="https://en.wikipedia.org/wiki/Fuzzy_clustering#Fuzzy_c-means_clustering" title="Fuzzy clustering">Fuzzy C-Means Clustering</a> is a soft version of <i>k</i>-means, where each data point has a fuzzy degree of belonging to each cluster.</li>
<li><a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model" title="Mixture model">Gaussian mixture</a> models trained with <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a> (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.</li>
<li><a href="https://en.wikipedia.org/wiki/K-means%2B%2B" title="K-means++"><i>k</i>-means++</a> chooses initial centers in a way that gives a provable upper bound on the WCSS objective.</li>
<li>The filtering algorithm uses <a href="https://en.wikipedia.org/wiki/Kd-tree" class="mw-redirect" title="Kd-tree">kd-trees</a> to speed up each <i>k</i>-means step.<sup id="cite_ref-28" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-28">[28]</a></sup></li>
<li>Some methods attempt to speed up each <i>k</i>-means step using the <a href="https://en.wikipedia.org/wiki/Triangle_inequality" title="Triangle inequality">triangle inequality</a>.<sup id="cite_ref-phillips2_24-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-phillips2-24">[24]</a></sup><sup id="cite_ref-elkan2_25-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-elkan2-25">[25]</a></sup><sup id="cite_ref-hamerly22_26-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hamerly22-26">[26]</a></sup><sup id="cite_ref-29" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-29">[29]</a></sup><sup id="cite_ref-hamerly32_27-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hamerly32-27">[27]</a></sup></li>
<li>Escape local optima by swapping points between clusters.<sup id="cite_ref-hartigan19792_9-3" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hartigan19792-9">[9]</a></sup></li>
<li>The Spherical <i>k</i>-means clustering algorithm is suitable for textual data.<sup id="cite_ref-30" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-30">[30]</a></sup></li>
<li>Hierarchical variants such as Bisecting <i>k</i>-means,<sup id="cite_ref-31" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-31">[31]</a></sup> <a href="https://en.wikipedia.org/wiki/X-means_clustering" class="mw-redirect" title="X-means clustering">X-means clustering</a><sup id="cite_ref-32" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-32">[32]</a></sup> and G-means clustering<sup id="cite_ref-33" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-33">[33]</a></sup> <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering#Divisive_clustering" title="Hierarchical clustering">repeatedly split clusters to build a hierarchy</a>, and can also try to automatically determine the optimal number of clusters in a dataset.</li>
<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation" title="Cluster analysis">Internal cluster evaluation</a> measures such as <a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" title="Silhouette (clustering)">cluster silhouette</a> can be helpful at <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set" title="Determining the number of clusters in a data set">determining the number of clusters</a>.</li>
<li>Minkowski weighted <i>k</i>-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features.<sup id="cite_ref-34" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-34">[34]</a></sup> These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.<sup id="cite_ref-35" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-35">[35]</a></sup></li>
<li>Mini-batch <i>k</i>-means: <i>k</i>-means variation using "mini batch" samples for data sets that do not fit into memory.<sup id="cite_ref-36" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-36">[36]</a></sup></li></ul>
<h3><span id="Hartigan.E2.80.93Wong_method"></span><span class="mw-headline" id="HartiganWong_method">HartiganWong method</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=8" title="Edit section: HartiganWong method">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Hartigan and Wong's method<sup id="cite_ref-hartigan19792_9-4" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-hartigan19792-9">[9]</a></sup> provides a more sophisticated though more computationally expensive way to perform <i>k</i>-means. It is still a heuristic method.
</p><p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \varphi (S_{j})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi><!--  --></mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \varphi (S_{j})}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/f47e542ba2c984ea0ee647ff3a5e24f82e170832" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:5.664ex; height:3.009ex;" alt="{\displaystyle \varphi (S_{j})}"></span> is the individual cost of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S_{j}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/222db49df2eefdb67737ba2d2dbd221a1bae0bf0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.335ex; height:2.843ex;" alt="S_j"></span> defined by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \sum _{x\in S_{j}}(x-\mu _{j})^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
            <mo><!--  --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </msub>
          </mrow>
        </munder>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo><!--  --></mo>
        <msub>
          <mi><!--  --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{x\in S_{j}}(x-\mu _{j})^{2}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/64fd8222fb8564702fa8b9e19287fc86a755b91f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.671ex; width:13.103ex; height:6.176ex;" alt="{\displaystyle \sum _{x\in S_{j}}(x-\mu _{j})^{2}}"></span>, with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mu _{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi><!--  --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mu _{j}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/4b2800dcde32ff75ad8aecdf9c4c4e2d7fad58db" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.311ex; height:2.343ex;" alt="\mu _{j}"></span> the center of the cluster.
</p><p><b>Assignment step:</b> Hartigan and Wong's method starts by partitioning the points into random clusters <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \{S_{j}\}_{j\in \{1,\cdots k\}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">{</mo>
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mo fence="false" stretchy="false">}</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo><!--  --></mo>
            <mo fence="false" stretchy="false">{</mo>
            <mn>1</mn>
            <mo>,</mo>
            <mo><!--  --></mo>
            <mi>k</mi>
            <mo fence="false" stretchy="false">}</mo>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \{S_{j}\}_{j\in \{1,\cdots k\}}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/017873dc944d0ebdd0a6013d78f01747cf65c32d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.171ex; width:12.371ex; height:3.176ex;" alt="{\displaystyle \{S_{j}\}_{j\in \{1,\cdots k\}}}"></span>.
</p><p><b>Update step</b>: Next it determines the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle n,m\in \{1,\ldots ,k\}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>n</mi>
        <mo>,</mo>
        <mi>m</mi>
        <mo><!--  --></mo>
        <mo fence="false" stretchy="false">{</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo><!--  --></mo>
        <mo>,</mo>
        <mi>k</mi>
        <mo fence="false" stretchy="false">}</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle n,m\in \{1,\ldots ,k\}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/12f2f36c284af7fa7bf821f6c4f48236126493b9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:17.187ex; height:2.843ex;" alt="{\displaystyle n,m\in \{1,\ldots ,k\}}"></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x\in S_{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
        <mo><!--  --></mo>
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x\in S_{n}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/1ca9416a8d97792a8d6804ad4c77b84ec9e8cb07" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:6.814ex; height:2.509ex;" alt="{\displaystyle x\in S_{n}}"></span> for which the following function reaches a minimum
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \Delta (m,n,x)=\varphi (S_{n})+\varphi (S_{m})-\varphi (S_{n}\smallsetminus \{x\})-\varphi (S_{m}\cup \{x\}).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal"><!--  --></mi>
        <mo stretchy="false">(</mo>
        <mi>m</mi>
        <mo>,</mo>
        <mi>n</mi>
        <mo>,</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi><!--  --></mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mi><!--  --></mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo><!--  --></mo>
        <mi><!--  --></mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo><!--  --></mo>
        <mo fence="false" stretchy="false">{</mo>
        <mi>x</mi>
        <mo fence="false" stretchy="false">}</mo>
        <mo stretchy="false">)</mo>
        <mo><!--  --></mo>
        <mi><!--  --></mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
        <mo><!--  --></mo>
        <mo fence="false" stretchy="false">{</mo>
        <mi>x</mi>
        <mo fence="false" stretchy="false">}</mo>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Delta (m,n,x)=\varphi (S_{n})+\varphi (S_{m})-\varphi (S_{n}\smallsetminus \{x\})-\varphi (S_{m}\cup \{x\}).}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/0452cd1da1795fef15cf5b1b48dfd0ec03504bcd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:60.381ex; height:2.843ex;" alt="{\displaystyle \Delta (m,n,x)=\varphi (S_{n})+\varphi (S_{m})-\varphi (S_{n}\smallsetminus \{x\})-\varphi (S_{m}\cup \{x\}).}"></span></dd></dl>
<p>For the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x,n,m}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
        <mo>,</mo>
        <mi>n</mi>
        <mo>,</mo>
        <mi>m</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x,n,m}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/72375561d9cbce169d8dade4f7e4534091311fb7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:6.833ex; height:2.009ex;" alt="{\displaystyle x,n,m}"></span> that reach this minimum, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/87f9e315fd7e2ba406057a97300593c4802b53e4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="x"></span> moves from the cluster <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S_{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S_{n}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/9f049ac28d4ac8097b625f9d71c1f22b2ebd1bc4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.643ex; height:2.509ex;" alt="S_{n}"></span> to the cluster <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S_{m}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S_{m}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/e6749c5ba85b0fce4b16adf795e6f53d197b7991" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:3.1ex; height:2.509ex;" alt="S_m"></span>.
</p><p><b>Termination</b>: The algorithm terminates once <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \Delta (m,n,x)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal"><!--  --></mi>
        <mo stretchy="false">(</mo>
        <mi>m</mi>
        <mo>,</mo>
        <mi>n</mi>
        <mo>,</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Delta (m,n,x)}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/6c3f23ea38178a1def0bb9fcbeaa49e141bf8b4d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:10.578ex; height:2.843ex;" alt="{\displaystyle \Delta (m,n,x)}"></span> is larger than zero for all <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x,n,m}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
        <mo>,</mo>
        <mi>n</mi>
        <mo>,</mo>
        <mi>m</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x,n,m}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/72375561d9cbce169d8dade4f7e4534091311fb7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:6.833ex; height:2.009ex;" alt="{\displaystyle x,n,m}"></span>.
</p><p>The algorithm can be sped up by immediately moving <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/87f9e315fd7e2ba406057a97300593c4802b53e4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="x"></span> from the cluster <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S_{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S_{n}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/9f049ac28d4ac8097b625f9d71c1f22b2ebd1bc4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.643ex; height:2.509ex;" alt="S_{n}"></span> to the cluster <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S_{m}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S_{m}}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/e6749c5ba85b0fce4b16adf795e6f53d197b7991" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:3.1ex; height:2.509ex;" alt="S_m"></span> as soon as an <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x,n,m}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
        <mo>,</mo>
        <mi>n</mi>
        <mo>,</mo>
        <mi>m</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x,n,m}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/72375561d9cbce169d8dade4f7e4534091311fb7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:6.833ex; height:2.009ex;" alt="{\displaystyle x,n,m}"></span> have been found for which <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \Delta (m,n,x)&lt;0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal"><!--  --></mi>
        <mo stretchy="false">(</mo>
        <mi>m</mi>
        <mo>,</mo>
        <mi>n</mi>
        <mo>,</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>&lt;</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Delta (m,n,x)&lt;0}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/1398d52bc97890a782db634e9053d3d873c210c7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:14.839ex; height:2.843ex;" alt="{\displaystyle \Delta (m,n,x)&lt;0}"></span>. This speed up can make the cost of the final result higher.
</p><p>The function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \Delta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal"><!--  --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Delta }</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/32769037c408874e1890f77554c65f39c523ebe2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.936ex; height:2.176ex;" alt="\Delta "></span> can be relatively efficiently evaluated by making use of the equality<sup id="cite_ref-:22_37-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-:22-37">[37]</a></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \Delta (x,n,m)={\frac {\mid S_{n}\mid }{\mid S_{n}\mid -1}}\cdot \lVert \mu _{n}-x\rVert ^{2}-{\frac {\mid S_{m}\mid }{\mid S_{m}\mid +1}}\cdot \lVert \mu _{m}-x\rVert ^{2}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal"><!--  --></mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>n</mi>
        <mo>,</mo>
        <mi>m</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mo stretchy="false"><!--  --></mo>
              <msub>
                <mi>S</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>n</mi>
                </mrow>
              </msub>
              <mo stretchy="false"><!--  --></mo>
            </mrow>
            <mrow>
              <mo stretchy="false"><!--  --></mo>
              <msub>
                <mi>S</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>n</mi>
                </mrow>
              </msub>
              <mo><!--  --></mo>
              <mo><!--  --></mo>
              <mn>1</mn>
            </mrow>
          </mfrac>
        </mrow>
        <mo><!--  --></mo>
        <mo fence="false" stretchy="false"><!--  --></mo>
        <msub>
          <mi><!--  --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo><!--  --></mo>
        <mi>x</mi>
        <msup>
          <mo fence="false" stretchy="false"><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo><!--  --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mo stretchy="false"><!--  --></mo>
              <msub>
                <mi>S</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>m</mi>
                </mrow>
              </msub>
              <mo stretchy="false"><!--  --></mo>
            </mrow>
            <mrow>
              <mo stretchy="false"><!--  --></mo>
              <msub>
                <mi>S</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>m</mi>
                </mrow>
              </msub>
              <mo><!--  --></mo>
              <mo>+</mo>
              <mn>1</mn>
            </mrow>
          </mfrac>
        </mrow>
        <mo><!--  --></mo>
        <mo fence="false" stretchy="false"><!--  --></mo>
        <msub>
          <mi><!--  --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </msub>
        <mo><!--  --></mo>
        <mi>x</mi>
        <msup>
          <mo fence="false" stretchy="false"><!--  --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Delta (x,n,m)={\frac {\mid S_{n}\mid }{\mid S_{n}\mid -1}}\cdot \lVert \mu _{n}-x\rVert ^{2}-{\frac {\mid S_{m}\mid }{\mid S_{m}\mid +1}}\cdot \lVert \mu _{m}-x\rVert ^{2}.}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/d0eecaef88f3b960451600a4e229bb02d67f5477" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:59.842ex; height:6.509ex;" alt="{\displaystyle \Delta (x,n,m)={\frac {\mid S_{n}\mid }{\mid S_{n}\mid -1}}\cdot \lVert \mu _{n}-x\rVert ^{2}-{\frac {\mid S_{m}\mid }{\mid S_{m}\mid +1}}\cdot \lVert \mu _{m}-x\rVert ^{2}.}"></span></dd></dl>
<h2><span class="mw-headline" id="Discussion">Discussion</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=9" title="Edit section: Discussion">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:652px;"><a href="https://en.wikipedia.org/wiki/File:K-means_convergence_to_a_local_minimum.png" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/650px-K-means_convergence_to_a_local_minimum.png" decoding="async" width="650" height="78" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/7c/K-means_convergence_to_a_local_minimum.png/975px-K-means_convergence_to_a_local_minimum.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/7c/K-means_convergence_to_a_local_minimum.png/1300px-K-means_convergence_to_a_local_minimum.png 2x" data-file-width="2914" data-file-height="349"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:K-means_convergence_to_a_local_minimum.png" class="internal" title="Enlarge"></a></div>A typical example of the <i>k</i>-means convergence to a local minimum. In this example, the result of <i>k</i>-means clustering (the right figure) contradicts the obvious cluster structure of the data set. The small circles are the data points, the four ray stars are the centroids (means). The initial configuration is on the left figure. The algorithm converges after five iterations presented on the figures, from the left to the right. The illustration was prepared with the Mirkes Java applet.<sup id="cite_ref-Mirkes20112_38-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-Mirkes20112-38">[38]</a></sup></div></div></div>
<div class="thumb tright"><div class="thumbinner" style="width:452px;"><a href="https://en.wikipedia.org/wiki/File:Iris_Flowers_Clustering_kMeans.svg" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/450px-Iris_Flowers_Clustering_kMeans.svg.png" decoding="async" width="450" height="211" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/10/Iris_Flowers_Clustering_kMeans.svg/675px-Iris_Flowers_Clustering_kMeans.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/10/Iris_Flowers_Clustering_kMeans.svg/900px-Iris_Flowers_Clustering_kMeans.svg.png 2x" data-file-width="660" data-file-height="309"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:Iris_Flowers_Clustering_kMeans.svg" class="internal" title="Enlarge"></a></div><i>k</i>-means clustering result for the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" title="Iris flower data set">Iris flower data set</a> and actual species visualized using <a href="https://en.wikipedia.org/wiki/Environment_for_DeveLoping_KDD-Applications_Supported_by_Index-Structures" class="mw-redirect" title="Environment for DeveLoping KDD-Applications Supported by Index-Structures">ELKI</a>. Cluster means are marked using larger, semi-transparent symbols.</div></div></div>
<div class="thumb tright"><div class="thumbinner" style="width:452px;"><a href="https://en.wikipedia.org/wiki/File:ClusterAnalysis_Mouse.svg" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/450px-ClusterAnalysis_Mouse.svg.png" decoding="async" width="450" height="182" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/675px-ClusterAnalysis_Mouse.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/900px-ClusterAnalysis_Mouse.svg.png 2x" data-file-width="1355" data-file-height="547"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:ClusterAnalysis_Mouse.svg" class="internal" title="Enlarge"></a></div><i>k</i>-means clustering vs. <a href="https://en.wikipedia.org/wiki/EM_clustering" class="mw-redirect" title="EM clustering">EM clustering</a> on an artificial dataset ("mouse"). The tendency of <i>k</i>-means to produce equal-sized clusters leads to bad results here, while EM benefits from the Gaussian distributions with different radius present in the data set.</div></div></div>
<p>Three key features of <i>k</i>-means that make it efficient are often regarded as its biggest drawbacks:
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> is used as a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)" title="Metric (mathematics)">metric</a> and <a href="https://en.wikipedia.org/wiki/Variance" title="Variance">variance</a> is used as a measure of cluster scatter.</li>
<li>The number of clusters <i>k</i> is an input parameter: an inappropriate choice of <i>k</i> may yield poor results. That is why, when performing <i>k</i>-means, it is important to run diagnostic checks for <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set" title="Determining the number of clusters in a data set">determining the number of clusters in the data set</a>.</li>
<li>Convergence to a local minimum may produce counterintuitive ("wrong") results (see example in Fig.).</li></ul>
<p>A key limitation of <i>k</i>-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying <i>k</i>-means with a value of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k=3}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>=</mo>
        <mn>3</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k=3}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/662e06a2436f8a44fec791f5c794621f10dc8f30" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.472ex; height:2.176ex;" alt="k=3"></span> onto the well-known <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" title="Iris flower data set">Iris flower data set</a>, the result often fails to separate the three <a href="https://en.wikipedia.org/wiki/Iris_(plant)" title="Iris (plant)">Iris</a> species contained in the data set. With <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k=2}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>=</mo>
        <mn>2</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k=2}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/0bd301789e1f25a3da4be297ff637754ebee5f5d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.472ex; height:2.176ex;" alt="k=2"></span>, the two visible clusters (one containing two species) will be discovered, whereas with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k=3}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>=</mo>
        <mn>3</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k=3}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/662e06a2436f8a44fec791f5c794621f10dc8f30" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.472ex; height:2.176ex;" alt="k=3"></span> one of the two clusters will be split into two even parts. In fact, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k=2}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>=</mo>
        <mn>2</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k=2}</annotation>
  </semantics>
</math></span><img src="./k-means clustering - Wikipedia_files/0bd301789e1f25a3da4be297ff637754ebee5f5d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.472ex; height:2.176ex;" alt="k=2"></span> is more appropriate for this data set, despite the data set's containing 3 <i>classes</i>. As with any other clustering algorithm, the <i>k</i>-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.
</p><p>The result of <i>k</i>-means can be seen as the <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" title="Voronoi diagram">Voronoi cells</a> of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the "mouse" example. The Gaussian models used by the <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a> (arguably a generalization of <i>k</i>-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than <i>k</i>-means as well as correlated clusters (not in this example). <i>K</i>-means is closely related to nonparametric <a href="https://en.wikipedia.org/wiki/Bayesian_inference" title="Bayesian inference">Bayesian modeling</a>.<sup id="cite_ref-39" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-39">[39]</a></sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=10" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><i>k</i>-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as <a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm" title="Lloyd&#39;s algorithm">Lloyd's algorithm</a>. It has been successfully used in <a href="https://en.wikipedia.org/wiki/Market_segmentation" title="Market segmentation">market segmentation</a>, <a href="https://en.wikipedia.org/wiki/Computer_vision" title="Computer vision">computer vision</a>, and <a href="https://en.wikipedia.org/wiki/Astronomy" title="Astronomy">astronomy</a> among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.
</p>
<h3><span class="mw-headline" id="Vector_quantization">Vector quantization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=11" title="Edit section: Vector quantization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Vector_quantization" title="Vector quantization">Vector quantization</a></div>
<div class="thumb tright"><div class="thumbinner" style="width:202px;"><a href="https://en.wikipedia.org/wiki/File:Rosa_Gold_Glow_2_small_noblue.png" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/Rosa_Gold_Glow_2_small_noblue.png" decoding="async" width="200" height="196" class="thumbimage" data-file-width="200" data-file-height="196"></a>  <div class="thumbcaption">Two-channel (for illustration purposes -- red and green only) color image.</div></div></div>
<div class="thumb tright"><div class="thumbinner" style="width:252px;"><a href="https://en.wikipedia.org/wiki/File:Rosa_Gold_Glow_2_small_noblue_color_space.png" class="image"><img alt="" src="./k-means clustering - Wikipedia_files/250px-Rosa_Gold_Glow_2_small_noblue_color_space.png" decoding="async" width="250" height="247" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Rosa_Gold_Glow_2_small_noblue_color_space.png/375px-Rosa_Gold_Glow_2_small_noblue_color_space.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Rosa_Gold_Glow_2_small_noblue_color_space.png/500px-Rosa_Gold_Glow_2_small_noblue_color_space.png 2x" data-file-width="803" data-file-height="792"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:Rosa_Gold_Glow_2_small_noblue_color_space.png" class="internal" title="Enlarge"></a></div>Vector quantization of colors present in the image above into Voronoi cells using <i>k</i>-means.</div></div></div>
<p><i>k</i>-means originates from signal processing, and still finds use in this domain. For example, in <a href="https://en.wikipedia.org/wiki/Computer_graphics" title="Computer graphics">computer graphics</a>, <a href="https://en.wikipedia.org/wiki/Color_quantization" title="Color quantization">color quantization</a> is the task of reducing the <a href="https://en.wikipedia.org/wiki/Color_palette" class="mw-redirect" title="Color palette">color palette</a> of an image to a fixed number of colors <i>k</i>. The <i>k</i>-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is <a href="https://en.wikipedia.org/wiki/Image_segmentation" title="Image segmentation">image segmentation</a>. Other uses of vector quantization include <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)" title="Sampling (statistics)">non-random sampling</a>, as <i>k</i>-means can easily be used to choose <i>k</i> different but prototypical objects from a large data set for further analysis.
</p>
<h3><span class="mw-headline" id="Cluster_analysis">Cluster analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=12" title="Edit section: Cluster analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Cluster analysis</a></div><p>In cluster analysis, the <i>k</i>-means algorithm can be used to partition the input data set into <i>k</i> partitions (clusters).
</p><p>However, the pure <i>k</i>-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter <i>k</i> is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.
</p>
<h3><span class="mw-headline" id="Feature_learning">Feature learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=13" title="Edit section: Feature learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><i>k</i>-means clustering has been used as a <a href="https://en.wikipedia.org/wiki/Feature_learning" title="Feature learning">feature learning</a> (or <a href="https://en.wikipedia.org/wiki/Dictionary_learning" class="mw-redirect" title="Dictionary learning">dictionary learning</a>) step, in either (<a href="https://en.wikipedia.org/wiki/Semi-supervised_learning" title="Semi-supervised learning">semi-</a>)<a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> or <a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>.<sup id="cite_ref-Coates20122_40-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-Coates20122-40">[40]</a></sup> The basic approach is first to train a <i>k</i>-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an "encoding" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid,<sup id="cite_ref-Coates20122_40-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-Coates20122-40">[40]</a></sup><sup id="cite_ref-41" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-41">[41]</a></sup> or some smooth transformation of the distance.<sup id="cite_ref-coates20112_42-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-coates20112-42">[42]</a></sup> Alternatively, transforming the sample-cluster distance through a <a href="https://en.wikipedia.org/wiki/Radial_basis_function" title="Radial basis function">Gaussian RBF</a>, obtains the hidden layer of a <a href="https://en.wikipedia.org/wiki/Radial_basis_function_network" title="Radial basis function network">radial basis function network</a>.<sup id="cite_ref-schwenker2_43-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-schwenker2-43">[43]</a></sup>
</p><p>This use of <i>k</i>-means has been successfully combined with simple, <a href="https://en.wikipedia.org/wiki/Linear_classifier" title="Linear classifier">linear classifiers</a> for semi-supervised learning in <a href="https://en.wikipedia.org/wiki/Natural_language_processing" title="Natural language processing">NLP</a> (specifically for <a href="https://en.wikipedia.org/wiki/Named_entity_recognition" class="mw-redirect" title="Named entity recognition">named entity recognition</a>)<sup id="cite_ref-44" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-44">[44]</a></sup> and in <a href="https://en.wikipedia.org/wiki/Computer_vision" title="Computer vision">computer vision</a>. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as <a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">autoencoders</a> and <a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machines</a>.<sup id="cite_ref-coates20112_42-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-coates20112-42">[42]</a></sup> However, it generally requires more data, for equivalent performance, because each data point only contributes to one "feature".<sup id="cite_ref-Coates20122_40-2" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-Coates20122-40">[40]</a></sup>
</p>
<h2><span class="mw-headline" id="Relation_to_other_algorithms">Relation to other algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=14" title="Edit section: Relation to other algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Gaussian_mixture_model">Gaussian mixture model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=15" title="Edit section: Gaussian mixture model">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Gaussian_mixture_model" class="mw-redirect" title="Gaussian mixture model">Gaussian mixture model</a></div>
<p>The slow "standard algorithm" for <i>k</i>-means clustering, and its associated <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectationmaximization algorithm">expectation-maximization algorithm</a>, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance.<sup id="cite_ref-:0_45-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-:0-45">[45]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>850</span></sup> Instead of small variances, a hard cluster assignment can also be used to show another equivalence of <i>k</i>-means clustering to a special case of "hard" Gaussian mixture modelling.<sup id="cite_ref-46" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-46">[46]</a></sup><sup class="reference" style="white-space:nowrap;">(<span>11.4.2.5</span>)</sup> This does not mean that it is efficient to use Gaussian mixture modelling to compute <i>k</i>-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of <i>k</i>-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.<sup id="cite_ref-:0_45-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-:0-45">[45]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>849</span></sup>
</p>
<h3><span class="mw-headline" id="K-SVD">K-SVD</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=16" title="Edit section: K-SVD">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/K-SVD" title="K-SVD">K-SVD</a></div>
<p>Another generalization of the <i>k</i>-means algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of "codebook vectors". <i>k</i>-means corresponds to the special case of using a single codebook vector, with a weight of 1.<sup id="cite_ref-47" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-47">[47]</a></sup>
</p>
<h3><span class="mw-headline" id="Principal_component_analysis">Principal component analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=17" title="Edit section: Principal component analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a></div>
<p>The relaxed solution of <span class="texhtml"><var>k</var></span>-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA).<sup id="cite_ref-48" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-48">[48]</a></sup><sup id="cite_ref-49" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-49">[49]</a></sup> The PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. The intuition is that <i>k</i>-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by <i>k</i>-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. <i>k</i>-means should not be expected to do well on this data.<sup id="cite_ref-50" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-50">[50]</a></sup> It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.<sup id="cite_ref-51" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-51">[51]</a></sup>
</p>
<h3><span class="mw-headline" id="Mean_shift_clustering">Mean shift clustering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=18" title="Edit section: Mean shift clustering">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Mean_shift" title="Mean shift">Mean shift</a></div>
<p>Basic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, <i>k</i>-means restricts this updated set to <i>k</i> points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the <i>input set</i> that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to <i>k</i>-means, called <i>likelihood mean shift</i>, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set.<sup id="cite_ref-Little20112_52-0" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-Little20112-52">[52]</a></sup> One of the advantages of mean shift over <i>k</i>-means is that the number of clusters is not pre-specified, because mean shift is likely to find only a few clusters if only a small number exist. However, mean shift can be much slower than <i>k</i>-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants.
</p>
<h3><span class="mw-headline" id="Independent_component_analysis">Independent component analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=19" title="Edit section: Independent component analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">Independent component analysis</a></div>
<p>Under sparsity assumptions and when input data is pre-processed with the <a href="https://en.wikipedia.org/wiki/Whitening_transformation" title="Whitening transformation">whitening transformation</a>, <i>k</i>-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of <i>k</i>-means to <a href="https://en.wikipedia.org/wiki/K-means_clustering#Feature_learning" title="K-means clustering">feature learning</a>.<sup id="cite_ref-53" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-53">[53]</a></sup>
</p>
<h3><span class="mw-headline" id="Bilateral_filtering">Bilateral filtering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=20" title="Edit section: Bilateral filtering">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Bilateral_filter" title="Bilateral filter">Bilateral filter</a></div>
<p><i>k</i>-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to <i>k</i>-means and <a href="https://en.wikipedia.org/wiki/Mean_shift" title="Mean shift">mean shift</a> in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data.<sup id="cite_ref-Little20112_52-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-Little20112-52">[52]</a></sup> This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.
</p>
<h2><span class="mw-headline" id="Similar_problems">Similar problems</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=21" title="Edit section: Similar problems">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The set of squared error minimizing cluster functions also includes the <a href="https://en.wikipedia.org/wiki/K-medoids" title="K-medoids"><span class="texhtml"><var>k</var></span>-medoids</a> algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e. it uses <a href="https://en.wikipedia.org/wiki/Medoids" class="mw-redirect" title="Medoids">medoids</a> in place of <a href="https://en.wikipedia.org/wiki/Centroids" class="mw-redirect" title="Centroids">centroids</a>.
</p>
<h2><span class="mw-headline" id="Software_implementations">Software implementations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=22" title="Edit section: Software implementations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Different implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours).<sup id="cite_ref-:12_1-1" class="reference"><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_note-:12-1">[1]</a></sup> The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.
</p>
<h3><span id="Free_Software.2FOpen_Source"></span><span class="mw-headline" id="Free_Software/Open_Source">Free Software/Open Source</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=23" title="Edit section: Free Software/Open Source">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The following implementations are available under <a href="https://en.wikipedia.org/wiki/Free_and_open-source_software" title="Free and open-source software">Free/Open Source Software</a> licenses, with publicly available source code.
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/Accord.NET" title="Accord.NET">Accord.NET</a> contains C# implementations for <i>k</i>-means, <i>k</i>-means++ and <i>k</i>-modes.</li>
<li><a href="https://en.wikipedia.org/wiki/ALGLIB" title="ALGLIB">ALGLIB</a> contains parallelized C++ and C# implementations for <i>k</i>-means and <i>k</i>-means++.</li>
<li><a href="https://en.wikipedia.org/wiki/Android_(operating_system)#Open-source_community" title="Android (operating system)">AOSP</a> contains a Java implementation for <i>k</i>-means.</li>
<li><a href="https://en.wikipedia.org/wiki/CrimeStat" title="CrimeStat">CrimeStat</a> implements two spatial <i>k</i>-means algorithms, one of which allows the user to define the starting locations.</li>
<li><a href="https://en.wikipedia.org/wiki/ELKI" title="ELKI">ELKI</a> contains <i>k</i>-means (with Lloyd and MacQueen iteration, along with different initializations such as <i>k</i>-means++ initialization) and various more advanced clustering algorithms.</li>
<li><a href="https://en.wikipedia.org/wiki/Julia_language" class="mw-redirect" title="Julia language">Julia</a> contains a <i>k</i>-means implementation in the JuliaStats Clustering package.</li>
<li><a href="https://en.wikipedia.org/wiki/KNIME" title="KNIME">KNIME</a> contains nodes for <i>k</i>-means and <i>k</i>-medoids.</li>
<li><a href="https://en.wikipedia.org/wiki/Apache_Mahout" title="Apache Mahout">Mahout</a> contains a <a href="https://en.wikipedia.org/wiki/MapReduce" title="MapReduce">MapReduce</a> based <i>k</i>-means.</li>
<li><a href="https://en.wikipedia.org/wiki/Mlpack" title="Mlpack">mlpack</a> contains a C++ implementation of <i>k</i>-means.</li>
<li><a href="https://en.wikipedia.org/wiki/GNU_Octave" title="GNU Octave">Octave</a> contains <i>k</i>-means.</li>
<li><a href="https://en.wikipedia.org/wiki/OpenCV" title="OpenCV">OpenCV</a> contains a <i>k</i>-means implementation.</li>
<li><a href="https://en.wikipedia.org/wiki/Orange_(software)" title="Orange (software)">Orange</a> includes a component for <i>k</i>-means clustering with automatic selection of <i>k</i> and cluster silhouette scoring.</li>
<li><a href="https://en.wikipedia.org/wiki/PSPP" title="PSPP">PSPP</a> contains <i>k</i>-means, The QUICK CLUSTER command performs <i>k</i>-means clustering on the dataset.</li>
<li><a href="https://en.wikipedia.org/wiki/R_(programming_language)" title="R (programming language)">R</a> contains three <i>k</i>-means variations.</li>
<li><a href="https://en.wikipedia.org/wiki/SciPy" title="SciPy">SciPy</a> and <a href="https://en.wikipedia.org/wiki/Scikit-learn" title="Scikit-learn">scikit-learn</a> contain multiple <i>k</i>-means implementations.</li>
<li><a href="https://en.wikipedia.org/wiki/Apache_Spark" title="Apache Spark">Spark</a> MLlib implements a distributed <i>k</i>-means algorithm.</li>
<li><a href="https://en.wikipedia.org/wiki/Torch_(machine_learning)" title="Torch (machine learning)">Torch</a> contains an <i>unsup</i> package that provides <i>k</i>-means clustering.</li>
<li><a href="https://en.wikipedia.org/wiki/Weka_(machine_learning)" title="Weka (machine learning)">Weka</a> contains <i>k</i>-means and <i>x</i>-means.</li></ul>
<h3><span class="mw-headline" id="Proprietary">Proprietary</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=24" title="Edit section: Proprietary">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The following implementations are available under <a href="https://en.wikipedia.org/wiki/Proprietary_software" title="Proprietary software">proprietary</a> license terms, and may not have publicly available source code.
</p>
<div class="div-col columns column-width" style="-moz-column-width: 10em; -webkit-column-width: 10em; column-width: 10em;">
<ul><li><a href="https://en.wikipedia.org/wiki/Ayasdi" title="Ayasdi">Ayasdi</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mathematica" class="mw-redirect" title="Mathematica">Mathematica</a></li>
<li><a href="https://en.wikipedia.org/wiki/MATLAB" title="MATLAB">MATLAB</a></li>
<li><a href="https://en.wikipedia.org/wiki/Origin_(data_analysis_software)" title="Origin (data analysis software)">OriginPro</a></li>
<li><a href="https://en.wikipedia.org/wiki/RapidMiner" title="RapidMiner">RapidMiner</a></li>
<li><a href="https://en.wikipedia.org/wiki/SAP_HANA" title="SAP HANA">SAP HANA</a></li>
<li><a href="https://en.wikipedia.org/wiki/SAS_System" class="mw-redirect" title="SAS System">SAS</a></li>
<li><a href="https://en.wikipedia.org/wiki/SPSS" title="SPSS">SPSS</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stata" title="Stata">Stata</a></li></ul>
</div>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=25" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/BFR_algorithm" title="BFR algorithm">BFR algorithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Centroidal_Voronoi_tessellation" title="Centroidal Voronoi tessellation">Centroidal Voronoi tessellation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Head/tail_Breaks" title="Head/tail Breaks">Head/tail Breaks</a></li>
<li><a href="https://en.wikipedia.org/wiki/K_q-flats" title="K q-flats">k q-flats</a></li>
<li><a href="https://en.wikipedia.org/wiki/K-means%2B%2B" title="K-means++">K-means++</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linde%E2%80%93Buzo%E2%80%93Gray_algorithm" title="LindeBuzoGray algorithm">LindeBuzoGray algorithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">Self-organizing map</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=26" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-:12-1"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-:12_1-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-:12_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal"><a href="https://en.wikipedia.org/wiki/Hans-Peter_Kriegel" title="Hans-Peter Kriegel">Kriegel, Hans-Peter</a>; Schubert, Erich; <a href="https://en.wikipedia.org/wiki/Arthur_Zimek" title="Arthur Zimek">Zimek, Arthur</a> (2016). "The (black) art of runtime evaluation: Are we comparing algorithms or implementations?". <i>Knowledge and Information Systems</i>. <b>52</b> (2): 341378. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10115-016-1004-2">10.1007/s10115-016-1004-2</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0219-1377">0219-1377</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Knowledge+and+Information+Systems&amp;rft.atitle=The+%28black%29+art+of+runtime+evaluation%3A+Are+we+comparing+algorithms+or+implementations%3F&amp;rft.volume=52&amp;rft.issue=2&amp;rft.pages=341-378&amp;rft.date=2016&amp;rft_id=info%3Adoi%2F10.1007%2Fs10115-016-1004-2&amp;rft.issn=0219-1377&amp;rft.aulast=Kriegel&amp;rft.aufirst=Hans-Peter&amp;rft.au=Schubert%2C+Erich&amp;rft.au=Zimek%2C+Arthur&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r886058088">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-macqueen19672-2"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-macqueen19672_2-0" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation conference">MacQueen, J. B. (1967). <a rel="nofollow" class="external text" href="http://projecteuclid.org/euclid.bsmsp/1200512992"><i>Some Methods for classification and Analysis of Multivariate Observations</i></a>. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. <b>1</b>. University of California Press. pp.&nbsp;281297. <a href="https://en.wikipedia.org/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ams.org/mathscinet-getitem?mr=0214227">0214227</a>. <a href="https://en.wikipedia.org/wiki/Zentralblatt_MATH" title="Zentralblatt MATH">Zbl</a>&nbsp;<a rel="nofollow" class="external text" href="https://zbmath.org/?format=complete&amp;q=an:0214.46201">0214.46201</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2009-04-07</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Some+Methods+for+classification+and+Analysis+of+Multivariate+Observations&amp;rft.pages=281-297&amp;rft.pub=University+of+California+Press&amp;rft.date=1967&amp;rft_id=%2F%2Fzbmath.org%2F%3Fformat%3Dcomplete%26q%3Dan%3A0214.46201&amp;rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D0214227&amp;rft.aulast=MacQueen&amp;rft.aufirst=J.+B.&amp;rft_id=http%3A%2F%2Fprojecteuclid.org%2Feuclid.bsmsp%2F1200512992&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-3" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="https://en.wikipedia.org/wiki/Hugo_Steinhaus" title="Hugo Steinhaus">Steinhaus, Hugo</a> (1957). "Sur la division des corps matriels en parties". <i>Bull. Acad. Polon. Sci.</i> (in French). <b>4</b> (12): 801804. <a href="https://en.wikipedia.org/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ams.org/mathscinet-getitem?mr=0090073">0090073</a>. <a href="https://en.wikipedia.org/wiki/Zentralblatt_MATH" title="Zentralblatt MATH">Zbl</a>&nbsp;<a rel="nofollow" class="external text" href="https://zbmath.org/?format=complete&amp;q=an:0079.16403">0079.16403</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bull.+Acad.+Polon.+Sci.&amp;rft.atitle=Sur+la+division+des+corps+mat%C3%A9riels+en+parties&amp;rft.volume=4&amp;rft.issue=12&amp;rft.pages=801-804&amp;rft.date=1957&amp;rft_id=%2F%2Fzbmath.org%2F%3Fformat%3Dcomplete%26q%3Dan%3A0079.16403&amp;rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D0090073&amp;rft.aulast=Steinhaus&amp;rft.aufirst=Hugo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-lloyd19572-4"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-lloyd19572_4-0" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lloyd, Stuart P. (1957). "Least square quantization in PCM". <i>Bell Telephone Laboratories Paper</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bell+Telephone+Laboratories+Paper&amp;rft.atitle=Least+square+quantization+in+PCM&amp;rft.date=1957&amp;rft.aulast=Lloyd&amp;rft.aufirst=Stuart+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"> Published in journal much later: <cite class="citation journal">Lloyd, Stuart P. (1982). <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~roweis/csc2515-2006/readings/lloyd57.pdf">"Least squares quantization in PCM"</a> <span class="cs1-format">(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/IEEE_Transactions_on_Information_Theory" title="IEEE Transactions on Information Theory">IEEE Transactions on Information Theory</a></i>. <b>28</b> (2): 129137. <a href="https://en.wikipedia.org/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&nbsp;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.1338">10.1.1.131.1338</a></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTIT.1982.1056489">10.1109/TIT.1982.1056489</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2009-04-15</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.atitle=Least+squares+quantization+in+PCM&amp;rft.volume=28&amp;rft.issue=2&amp;rft.pages=129-137&amp;rft.date=1982&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.131.1338&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1982.1056489&amp;rft.aulast=Lloyd&amp;rft.aufirst=Stuart+P.&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~roweis%2Fcsc2515-2006%2Freadings%2Flloyd57.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-forgy652-5"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-forgy652_5-0" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Forgy, Edward W. (1965). "Cluster analysis of multivariate data: efficiency versus interpretability of classifications". <i>Biometrics</i>. <b>21</b> (3): 768769. <a href="https://en.wikipedia.org/wiki/JSTOR" title="JSTOR">JSTOR</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.jstor.org/stable/2528559">2528559</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biometrics&amp;rft.atitle=Cluster+analysis+of+multivariate+data%3A+efficiency+versus+interpretability+of+classifications&amp;rft.volume=21&amp;rft.issue=3&amp;rft.pages=768-769&amp;rft.date=1965&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2528559&amp;rft.aulast=Forgy&amp;rft.aufirst=Edward+W.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-6" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Pelleg, Dan; Moore, Andrew (1999). <a rel="nofollow" class="external text" href="http://portal.acm.org/citation.cfm?doid=312129.312248">"Accelerating exact k -means algorithms with geometric reasoning"</a>. <i>Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining  - KDD '99</i>. San Diego, California, United States: ACM Press: 277281. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F312129.312248">10.1145/312129.312248</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781581131437" title="Special:BookSources/9781581131437"><bdi>9781581131437</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+fifth+ACM+SIGKDD+international+conference+on+Knowledge+discovery+and+data+mining++-+KDD+%2799&amp;rft.atitle=Accelerating+exact+k+-means+algorithms+with+geometric+reasoning&amp;rft.pages=277-281&amp;rft.date=1999&amp;rft_id=info%3Adoi%2F10.1145%2F312129.312248&amp;rft.isbn=9781581131437&amp;rft.aulast=Pelleg&amp;rft.aufirst=Dan&amp;rft.au=Moore%2C+Andrew&amp;rft_id=http%3A%2F%2Fportal.acm.org%2Fcitation.cfm%3Fdoid%3D312129.312248&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-7" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite id="mackay2003" class="citation book"><a href="https://en.wikipedia.org/wiki/David_MacKay_(scientist)" class="mw-redirect" title="David MacKay (scientist)">MacKay, David</a> (2003). <a rel="nofollow" class="external text" href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/284.292.pdf">"Chapter 20. An Example Inference Task: Clustering"</a> <span class="cs1-format">(PDF)</span>. <a rel="nofollow" class="external text" href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html"><i>Information Theory, Inference and Learning Algorithms</i></a>. Cambridge University Press. pp.&nbsp;284292. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-64298-9" title="Special:BookSources/978-0-521-64298-9"><bdi>978-0-521-64298-9</bdi></a>. <a href="https://en.wikipedia.org/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ams.org/mathscinet-getitem?mr=2012999">2012999</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+20.+An+Example+Inference+Task%3A+Clustering&amp;rft.btitle=Information+Theory%2C+Inference+and+Learning+Algorithms&amp;rft.pages=284-292&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2003&amp;rft.isbn=978-0-521-64298-9&amp;rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D2012999&amp;rft.aulast=MacKay&amp;rft.aufirst=David&amp;rft_id=http%3A%2F%2Fwww.inference.phy.cam.ac.uk%2Fmackay%2Fitprnn%2Fps%2F284.292.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-8" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text">Since the square root is a monotone function, this also is the minimum Euclidean distance assignment.</span>
</li>
<li id="cite_note-hartigan19792-9"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hartigan19792_9-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hartigan19792_9-1"><sup><i><b>b</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hartigan19792_9-2"><sup><i><b>c</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hartigan19792_9-3"><sup><i><b>d</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hartigan19792_9-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Hartigan, J. A.; Wong, M. A. (1979). "Algorithm AS 136: A <i>k</i>-Means Clustering Algorithm". <i><a href="https://en.wikipedia.org/wiki/Journal_of_the_Royal_Statistical_Society,_Series_C" class="mw-redirect" title="Journal of the Royal Statistical Society, Series C">Journal of the Royal Statistical Society, Series C</a></i>. <b>28</b> (1): 100108. <a href="https://en.wikipedia.org/wiki/JSTOR" title="JSTOR">JSTOR</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.jstor.org/stable/2346830">2346830</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Royal+Statistical+Society%2C+Series+C&amp;rft.atitle=Algorithm+AS+136%3A+A+k-Means+Clustering+Algorithm&amp;rft.volume=28&amp;rft.issue=1&amp;rft.pages=100-108&amp;rft.date=1979&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2346830&amp;rft.aulast=Hartigan&amp;rft.aufirst=J.+A.&amp;rft.au=Wong%2C+M.+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-hamerly4-10"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hamerly4_10-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hamerly4_10-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Hamerly, Greg; Elkan, Charles (2002). <a rel="nofollow" class="external text" href="http://people.csail.mit.edu/tieu/notebook/kmeans/15_p600-hamerly.pdf">"Alternatives to the <i>k</i>-means algorithm that find better clusterings"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of the eleventh international conference on Information and knowledge management (CIKM)</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Alternatives+to+the+k-means+algorithm+that+find+better+clusterings&amp;rft.btitle=Proceedings+of+the+eleventh+international+conference+on+Information+and+knowledge+management+%28CIKM%29&amp;rft.date=2002&amp;rft.aulast=Hamerly&amp;rft.aufirst=Greg&amp;rft.au=Elkan%2C+Charles&amp;rft_id=http%3A%2F%2Fpeople.csail.mit.edu%2Ftieu%2Fnotebook%2Fkmeans%2F15_p600-hamerly.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-11" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Celebi, M. E.; Kingravi, H. A.; Vela, P. A. (2013). "A comparative study of efficient initialization methods for the <i>k</i>-means clustering algorithm". <i><a href="https://en.wikipedia.org/w/index.php?title=Expert_Systems_with_Applications&amp;action=edit&amp;redlink=1" class="new" title="Expert Systems with Applications (page does not exist)">Expert Systems with Applications</a></i>. <b>40</b> (1): 200210. <a href="https://en.wikipedia.org/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1209.1960">1209.1960</a></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.eswa.2012.07.021">10.1016/j.eswa.2012.07.021</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Expert+Systems+with+Applications&amp;rft.atitle=A+comparative+study+of+efficient+initialization+methods+for+the+k-means+clustering+algorithm&amp;rft.volume=40&amp;rft.issue=1&amp;rft.pages=200-210&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1209.1960&amp;rft_id=info%3Adoi%2F10.1016%2Fj.eswa.2012.07.021&amp;rft.aulast=Celebi&amp;rft.aufirst=M.+E.&amp;rft.au=Kingravi%2C+H.+A.&amp;rft.au=Vela%2C+P.+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-12" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation conference">Bradley, Paul S.; <a href="https://en.wikipedia.org/wiki/Usama_Fayyad" title="Usama Fayyad">Fayyad, Usama M.</a> (1998). "Refining Initial Points for <i>k</i>-Means Clustering". <i>Proceedings of the Fifteenth International Conference on Machine Learning</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Refining+Initial+Points+for+k-Means+Clustering&amp;rft.btitle=Proceedings+of+the+Fifteenth+International+Conference+on+Machine+Learning&amp;rft.date=1998&amp;rft.aulast=Bradley&amp;rft.aufirst=Paul+S.&amp;rft.au=Fayyad%2C+Usama+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-13" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Vattani, A. (2011). <a rel="nofollow" class="external text" href="http://cseweb.ucsd.edu/users/avattani/papers/kmeans-journal.pdf">"k-means requires exponentially many iterations even in the plane"</a> <span class="cs1-format">(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Discrete_and_Computational_Geometry" class="mw-redirect" title="Discrete and Computational Geometry">Discrete and Computational Geometry</a></i>. <b>45</b> (4): 596616. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00454-011-9340-1">10.1007/s00454-011-9340-1</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Discrete+and+Computational+Geometry&amp;rft.atitle=k-means+requires+exponentially+many+iterations+even+in+the+plane&amp;rft.volume=45&amp;rft.issue=4&amp;rft.pages=596-616&amp;rft.date=2011&amp;rft_id=info%3Adoi%2F10.1007%2Fs00454-011-9340-1&amp;rft.aulast=Vattani&amp;rft.aufirst=A.&amp;rft_id=http%3A%2F%2Fcseweb.ucsd.edu%2Fusers%2Favattani%2Fpapers%2Fkmeans-journal.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-Arthur,_David;_Manthey,_B.;_Roeglin,_H._20092-14"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-Arthur,_David;_Manthey,_B.;_Roeglin,_H._20092_14-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-Arthur,_David;_Manthey,_B.;_Roeglin,_H._20092_14-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Arthur, David; Manthey, B.; Roeglin, H. (2009). "k-means has polynomial smoothed complexity". <i>Proceedings of the 50th Symposium on Foundations of Computer Science (FOCS)</i>. <a href="https://en.wikipedia.org/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/0904.1113">0904.1113</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=k-means+has+polynomial+smoothed+complexity&amp;rft.btitle=Proceedings+of+the+50th+Symposium+on+Foundations+of+Computer+Science+%28FOCS%29&amp;rft.date=2009&amp;rft_id=info%3Aarxiv%2F0904.1113&amp;rft.aulast=Arthur&amp;rft.aufirst=David&amp;rft.au=Manthey%2C+B.&amp;rft.au=Roeglin%2C+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-15" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Garey, M.; Johnson, D.; Witsenhausen, H. (1982-03-01). <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/document/1056488/">"The complexity of the generalized Lloyd - Max problem (Corresp.)"</a>. <i>IEEE Transactions on Information Theory</i>. <b>28</b> (2): 255256. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTIT.1982.1056488">10.1109/TIT.1982.1056488</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0018-9448">0018-9448</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.atitle=The+complexity+of+the+generalized+Lloyd+-+Max+problem+%28Corresp.%29&amp;rft.volume=28&amp;rft.issue=2&amp;rft.pages=255-256&amp;rft.date=1982-03-01&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1982.1056488&amp;rft.issn=0018-9448&amp;rft.aulast=Garey&amp;rft.aufirst=M.&amp;rft.au=Johnson%2C+D.&amp;rft.au=Witsenhausen%2C+H.&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F1056488%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-16" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Kleinberg, Jon; Papadimitriou, Christos; Raghavan, Prabhakar (1998-12-01). "A Microeconomic View of Data Mining". <i>Data Mining and Knowledge Discovery</i>. <b>2</b> (4): 311324. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1023%2FA%3A1009726428407">10.1023/A:1009726428407</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1384-5810">1384-5810</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Data+Mining+and+Knowledge+Discovery&amp;rft.atitle=A+Microeconomic+View+of+Data+Mining&amp;rft.volume=2&amp;rft.issue=4&amp;rft.pages=311-324&amp;rft.date=1998-12-01&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1009726428407&amp;rft.issn=1384-5810&amp;rft.aulast=Kleinberg&amp;rft.aufirst=Jon&amp;rft.au=Papadimitriou%2C+Christos&amp;rft.au=Raghavan%2C+Prabhakar&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-17" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Aloise, D.; Deshpande, A.; Hansen, P.; Popat, P. (2009). "NP-hardness of Euclidean sum-of-squares clustering". <i><a href="https://en.wikipedia.org/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">Machine Learning</a></i>. <b>75</b> (2): 245249. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10994-009-5103-0">10.1007/s10994-009-5103-0</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=NP-hardness+of+Euclidean+sum-of-squares+clustering&amp;rft.volume=75&amp;rft.issue=2&amp;rft.pages=245-249&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1007%2Fs10994-009-5103-0&amp;rft.aulast=Aloise&amp;rft.aufirst=D.&amp;rft.au=Deshpande%2C+A.&amp;rft.au=Hansen%2C+P.&amp;rft.au=Popat%2C+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-18" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Dasgupta, S.; Freund, Y. (July 2009). "Random Projection Trees for Vector Quantization". <i>IEEE Transactions on Information Theory</i>. <b>55</b> (7): 32293242. <a href="https://en.wikipedia.org/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/0805.1390">0805.1390</a></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTIT.2009.2021326">10.1109/TIT.2009.2021326</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.atitle=Random+Projection+Trees+for+Vector+Quantization&amp;rft.volume=55&amp;rft.issue=7&amp;rft.pages=3229-3242&amp;rft.date=2009-07&amp;rft_id=info%3Aarxiv%2F0805.1390&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.2009.2021326&amp;rft.aulast=Dasgupta&amp;rft.aufirst=S.&amp;rft.au=Freund%2C+Y.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-19" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation book">Mahajan, Meena; Nimbhorkar, Prajakta; Varadarajan, Kasturi (2009). <i>The Planar </i>k<i>-Means Problem is NP-Hard</i>. Lecture Notes in Computer Science. <b>5431</b>. pp.&nbsp;274285. <a href="https://en.wikipedia.org/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&nbsp;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.331.1306">10.1.1.331.1306</a></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-3-642-00202-1_24">10.1007/978-3-642-00202-1_24</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-3-642-00201-4" title="Special:BookSources/978-3-642-00201-4"><bdi>978-3-642-00201-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Planar+k-Means+Problem+is+NP-Hard&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=274-285&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.331.1306&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-00202-1_24&amp;rft.isbn=978-3-642-00201-4&amp;rft.aulast=Mahajan&amp;rft.aufirst=Meena&amp;rft.au=Nimbhorkar%2C+Prajakta&amp;rft.au=Varadarajan%2C+Kasturi&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-20" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation conference">Inaba, M.; Katoh, N.; Imai, H. (1994). <i>Applications of weighted Voronoi diagrams and randomization to variance-based </i>k<i>-clustering</i>. <a href="https://en.wikipedia.org/wiki/Symposium_on_Computational_Geometry" title="Symposium on Computational Geometry">Proceedings of 10th ACM Symposium on Computational Geometry</a>. pp.&nbsp;332339. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F177424.178042">10.1145/177424.178042</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Applications+of+weighted+Voronoi+diagrams+and+randomization+to+variance-based+k-clustering&amp;rft.pages=332-339&amp;rft.date=1994&amp;rft_id=info%3Adoi%2F10.1145%2F177424.178042&amp;rft.aulast=Inaba&amp;rft.aufirst=M.&amp;rft.au=Katoh%2C+N.&amp;rft.au=Imai%2C+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-21" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation book">Manning, Christopher D.; Raghavan, Prabhakar; Schtze, Hinrich (2008). <i>Introduction to information retrieval</i>. New York: Cambridge University Press. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0521865715" title="Special:BookSources/978-0521865715"><bdi>978-0521865715</bdi></a>. <a href="https://en.wikipedia.org/wiki/OCLC" title="OCLC">OCLC</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/190786122">190786122</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+information+retrieval&amp;rft.place=New+York&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2008&amp;rft_id=info%3Aoclcnum%2F190786122&amp;rft.isbn=978-0521865715&amp;rft.aulast=Manning&amp;rft.aufirst=Christopher+D.&amp;rft.au=Raghavan%2C+Prabhakar&amp;rft.au=Sch%C3%BCtze%2C+Hinrich&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-:02-22"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-:02_22-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-:02_22-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Arthur, David; Vassilvitskii, Sergei (2006-01-01). <i>How Slow is the </i>k<i>-means Method?</i>. <i>Proceedings of the Twenty-second Annual Symposium on Computational Geometry</i>. SCG '06. New York, NY, USA: ACM. pp.&nbsp;144153. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F1137856.1137880">10.1145/1137856.1137880</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1595933409" title="Special:BookSources/978-1595933409"><bdi>978-1595933409</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=How+Slow+is+the+k-means+Method%3F&amp;rft.place=New+York%2C+NY%2C+USA&amp;rft.series=SCG+%2706&amp;rft.pages=144-153&amp;rft.pub=ACM&amp;rft.date=2006-01-01&amp;rft_id=info%3Adoi%2F10.1145%2F1137856.1137880&amp;rft.isbn=978-1595933409&amp;rft.aulast=Arthur&amp;rft.aufirst=David&amp;rft.au=Vassilvitskii%2C+Sergei&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-23" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation thesis">Bhowmick, Abhishek (2009). <a rel="nofollow" class="external text" href="https://gautam5.cse.iitk.ac.in/opencs/sites/default/files/final.pdf"><i>A theoretical analysis of Lloyd's algorithm for </i>k<i>-means clustering</i></a> <span class="cs1-format">(PDF)</span> (Thesis).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=A+theoretical+analysis+of+Lloyd%27s+algorithm+for+k-means+clustering&amp;rft.date=2009&amp;rft.aulast=Bhowmick&amp;rft.aufirst=Abhishek&amp;rft_id=https%3A%2F%2Fgautam5.cse.iitk.ac.in%2Fopencs%2Fsites%2Fdefault%2Ffiles%2Ffinal.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"> See also <a rel="nofollow" class="external text" href="https://www.researchgate.net/publication/267854906_A_theoretical_analysis_of_Lloyd&#39;s_algorithm_for_k-means_clustering">here</a>.</span>
</li>
<li id="cite_note-phillips2-24"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-phillips2_24-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-phillips2_24-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Phillips, Steven J. (2002-01-04). "Acceleration of K-Means and Related Clustering Algorithms".  In Mount, David M.; Stein, Clifford (eds.). <i>Acceleration of </i>k<i>-Means and Related Clustering Algorithms</i>. Lecture Notes in Computer Science. <b>2409</b>. Springer Berlin Heidelberg. pp.&nbsp;166177. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F3-540-45643-0_13">10.1007/3-540-45643-0_13</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-3-540-43977-6" title="Special:BookSources/978-3-540-43977-6"><bdi>978-3-540-43977-6</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Acceleration+of+K-Means+and+Related+Clustering+Algorithms&amp;rft.btitle=Acceleration+of+k-Means+and+Related+Clustering+Algorithms&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=166-177&amp;rft.pub=Springer+Berlin+Heidelberg&amp;rft.date=2002-01-04&amp;rft_id=info%3Adoi%2F10.1007%2F3-540-45643-0_13&amp;rft.isbn=978-3-540-43977-6&amp;rft.aulast=Phillips&amp;rft.aufirst=Steven+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-elkan2-25"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-elkan2_25-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-elkan2_25-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Elkan, Charles (2003). <a rel="nofollow" class="external text" href="http://www-cse.ucsd.edu/~elkan/kmeansicml03.pdf">"Using the triangle inequality to accelerate <i>k</i>-means"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of the Twentieth International Conference on Machine Learning (ICML)</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Using+the+triangle+inequality+to+accelerate+k-means&amp;rft.btitle=Proceedings+of+the+Twentieth+International+Conference+on+Machine+Learning+%28ICML%29&amp;rft.date=2003&amp;rft.aulast=Elkan&amp;rft.aufirst=Charles&amp;rft_id=http%3A%2F%2Fwww-cse.ucsd.edu%2F~elkan%2Fkmeansicml03.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-hamerly22-26"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hamerly22_26-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hamerly22_26-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Hamerly, Greg. "Making <i>k</i>-means even faster". <a href="https://en.wikipedia.org/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&nbsp;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.187.3017">10.1.1.187.3017</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Making+k-means+even+faster&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.187.3017&amp;rft.aulast=Hamerly&amp;rft.aufirst=Greg&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span> <span class="cs1-hidden-error error citation-comment">Cite journal requires <code class="cs1-code">|journal=</code> (<a href="https://en.wikipedia.org/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-hamerly32-27"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hamerly32_27-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-hamerly32_27-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Hamerly, Greg; Drake, Jonathan (2015). <i>Accelerating Lloyd's algorithm for </i>k<i>-means clustering</i>. <i>Partitional Clustering Algorithms</i>. pp.&nbsp;4178. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-3-319-09259-1_2">10.1007/978-3-319-09259-1_2</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-3-319-09258-4" title="Special:BookSources/978-3-319-09258-4"><bdi>978-3-319-09258-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Accelerating+Lloyd%27s+algorithm+for+k-means+clustering&amp;rft.pages=41-78&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-09259-1_2&amp;rft.isbn=978-3-319-09258-4&amp;rft.aulast=Hamerly&amp;rft.aufirst=Greg&amp;rft.au=Drake%2C+Jonathan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-28" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Kanungo, Tapas; <a href="https://en.wikipedia.org/wiki/David_Mount" title="David Mount">Mount, David M.</a>; <a href="https://en.wikipedia.org/wiki/Nathan_Netanyahu" title="Nathan Netanyahu">Netanyahu, Nathan S.</a>; <a href="https://en.wikipedia.org/wiki/Christine_Piatko" title="Christine Piatko">Piatko, Christine D.</a>; Silverman, Ruth; Wu, Angela Y. (2002). <a rel="nofollow" class="external text" href="http://www.cs.umd.edu/~mount/Papers/pami02.pdf">"An efficient <i>k</i>-means clustering algorithm: Analysis and implementation"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>24</b> (7): 881892. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTPAMI.2002.1017616">10.1109/TPAMI.2002.1017616</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2009-04-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=An+efficient+k-means+clustering+algorithm%3A+Analysis+and+implementation&amp;rft.volume=24&amp;rft.issue=7&amp;rft.pages=881-892&amp;rft.date=2002&amp;rft_id=info%3Adoi%2F10.1109%2FTPAMI.2002.1017616&amp;rft.aulast=Kanungo&amp;rft.aufirst=Tapas&amp;rft.au=Mount%2C+David+M.&amp;rft.au=Netanyahu%2C+Nathan+S.&amp;rft.au=Piatko%2C+Christine+D.&amp;rft.au=Silverman%2C+Ruth&amp;rft.au=Wu%2C+Angela+Y.&amp;rft_id=http%3A%2F%2Fwww.cs.umd.edu%2F~mount%2FPapers%2Fpami02.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-29" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Drake, Jonathan (2012). <a rel="nofollow" class="external text" href="http://opt.kyb.tuebingen.mpg.de/papers/opt2012_paper_13.pdf">"Accelerated <i>k</i>-means with adaptive distance bounds"</a> <span class="cs1-format">(PDF)</span>. <i>The 5th NIPS Workshop on Optimization for Machine Learning, OPT2012</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+5th+NIPS+Workshop+on+Optimization+for+Machine+Learning%2C+OPT2012&amp;rft.atitle=Accelerated+k-means+with+adaptive+distance+bounds&amp;rft.date=2012&amp;rft.aulast=Drake&amp;rft.aufirst=Jonathan&amp;rft_id=http%3A%2F%2Fopt.kyb.tuebingen.mpg.de%2Fpapers%2Fopt2012_paper_13.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-30" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Dhillon, I. S.; Modha, D. M. (2001). "Concept decompositions for large sparse text data using clustering". <i>Machine Learning</i>. <b>42</b> (1): 143175. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1023%2Fa%3A1007612920971">10.1023/a:1007612920971</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=Concept+decompositions+for+large+sparse+text+data+using+clustering&amp;rft.volume=42&amp;rft.issue=1&amp;rft.pages=143-175&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1023%2Fa%3A1007612920971&amp;rft.aulast=Dhillon&amp;rft.aufirst=I.+S.&amp;rft.au=Modha%2C+D.+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-31" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text">Steinbach, M.; Karypis, G.; &amp; Kumar, V. (2000, August). "A comparison of document clustering techniques". In <i>KDD workshop on text mining</i>, Vol. 400, No. 1, pp. 525-526.</span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-32" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text">Pelleg, D.; &amp; Moore, A. W. (2000, June). "<a rel="nofollow" class="external text" href="http://cs.uef.fi/~zhao/Courses/Clustering2012/Xmeans.pdf">X-means: Extending <i>k</i>-means with Efficient Estimation of the Number of Clusters</a>". In <i>ICML</i>, Vol. 1</span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-33" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text">Hamerly, Greg; &amp; Elkan, Charles (2004). <a rel="nofollow" class="external text" href="http://papers.nips.cc/paper/2526-learning-the-k-in-k-means.pdf">"Learning the k in <i>k</i>-means"</a>. In <i>Advances in neural information processing systems</i>, 16, 281.</span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-34" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Amorim, R. C.; Mirkin, B. (2012). "Minkowski Metric, Feature Weighting and Anomalous Cluster Initialisation in <i>k</i>-Means Clustering". <i>Pattern Recognition</i>. <b>45</b> (3): 10611075. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.patcog.2011.08.012">10.1016/j.patcog.2011.08.012</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition&amp;rft.atitle=Minkowski+Metric%2C+Feature+Weighting+and+Anomalous+Cluster+Initialisation+in+k-Means+Clustering&amp;rft.volume=45&amp;rft.issue=3&amp;rft.pages=1061-1075&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patcog.2011.08.012&amp;rft.aulast=Amorim&amp;rft.aufirst=R.+C.&amp;rft.au=Mirkin%2C+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-35" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Amorim, R. C.; Hennig, C. (2015). "Recovering the number of clusters in data sets with noise features using feature rescaling factors". <i>Information Sciences</i>. <b>324</b>: 126145. <a href="https://en.wikipedia.org/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1602.06989">1602.06989</a></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.ins.2015.06.039">10.1016/j.ins.2015.06.039</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information+Sciences&amp;rft.atitle=Recovering+the+number+of+clusters+in+data+sets+with+noise+features+using+feature+rescaling+factors&amp;rft.volume=324&amp;rft.pages=126-145&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1602.06989&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ins.2015.06.039&amp;rft.aulast=Amorim&amp;rft.aufirst=R.+C.&amp;rft.au=Hennig%2C+C.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-36" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation conference">Sculley, David (2010). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1772862">"Web-scale <i>k</i>-means clustering"</a>. <i>Proceedings of the 19th international conference on World Wide Web</i>. ACM. pp.&nbsp;11771178<span class="reference-accessdate">. Retrieved <span class="nowrap">2016-12-21</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Web-scale+k-means+clustering&amp;rft.btitle=Proceedings+of+the+19th+international+conference+on+World+Wide+Web&amp;rft.pages=1177-1178&amp;rft.pub=ACM&amp;rft.date=2010&amp;rft.aulast=Sculley&amp;rft.aufirst=David&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1772862&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-:22-37"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-:22_37-0" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation web">Telgarsky, Matus. <a rel="nofollow" class="external text" href="http://proceedings.mlr.press/v9/telgarsky10a/telgarsky10a.pdf">"Hartigan's Method: <i>k</i>-means Clustering without Voronoi"</a> <span class="cs1-format">(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Hartigan%27s+Method%3A+k-means+Clustering+without+Voronoi&amp;rft.aulast=Telgarsky&amp;rft.aufirst=Matus&amp;rft_id=http%3A%2F%2Fproceedings.mlr.press%2Fv9%2Ftelgarsky10a%2Ftelgarsky10a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-Mirkes20112-38"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-Mirkes20112_38-0" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation web">Mirkes, E. M. <a rel="nofollow" class="external text" href="http://www.math.le.ac.uk/people/ag153/homepage/KmeansKmedoids/Kmeans_Kmedoids.html">"K-means and <i>k</i>-medoids applet"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=K-means+and+k-medoids+applet&amp;rft.aulast=Mirkes&amp;rft.aufirst=E.+M.&amp;rft_id=http%3A%2F%2Fwww.math.le.ac.uk%2Fpeople%2Fag153%2Fhomepage%2FKmeansKmedoids%2FKmeans_Kmedoids.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-39" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation book">Kulis, Brian; Jordan, Michael I. (2012-06-26). <a rel="nofollow" class="external text" href="https://icml.cc/2012/papers/291.pdf"><i>Revisiting </i>k<i>-means: new algorithms via Bayesian nonparametrics</i></a> <span class="cs1-format">(PDF)</span>. <i>ICML</i>. pp.&nbsp;11311138. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781450312851" title="Special:BookSources/9781450312851"><bdi>9781450312851</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Revisiting+k-means%3A+new+algorithms+via+Bayesian+nonparametrics&amp;rft.pages=1131-1138&amp;rft.date=2012-06-26&amp;rft.isbn=9781450312851&amp;rft.aulast=Kulis&amp;rft.aufirst=Brian&amp;rft.au=Jordan%2C+Michael+I.&amp;rft_id=https%3A%2F%2Ficml.cc%2F2012%2Fpapers%2F291.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-Coates20122-40"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-Coates20122_40-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-Coates20122_40-1"><sup><i><b>b</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-Coates20122_40-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation encyclopaedia">Coates, Adam; Ng, Andrew Y. (2012). <a rel="nofollow" class="external text" href="https://cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf">"Learning feature representations with <i>k</i>-means"</a> <span class="cs1-format">(PDF)</span>.  In Montavon, G.; Orr, G. B.; Mller, K.-R. (eds.). <i>Neural Networks: Tricks of the Trade</i>. Springer.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Learning+feature+representations+with+k-means&amp;rft.btitle=Neural+Networks%3A+Tricks+of+the+Trade&amp;rft.pub=Springer&amp;rft.date=2012&amp;rft.aulast=Coates&amp;rft.aufirst=Adam&amp;rft.au=Ng%2C+Andrew+Y.&amp;rft_id=https%3A%2F%2Fcs.stanford.edu%2F~acoates%2Fpapers%2Fcoatesng_nntot2012.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: uses editors parameter (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_uses_editors_parameter" title="Category:CS1 maint: uses editors parameter">link</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-41" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation conference">Csurka, Gabriella; Dance, Christopher C.; Fan, Lixin; Willamowski, Jutta; Bray, Cdric (2004). <a rel="nofollow" class="external text" href="https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/csurka-eccv-04.pdf"><i>Visual categorization with bags of keypoints</i></a> <span class="cs1-format">(PDF)</span>. ECCV Workshop on Statistical Learning in Computer Vision.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Visual+categorization+with+bags+of+keypoints&amp;rft.date=2004&amp;rft.aulast=Csurka&amp;rft.aufirst=Gabriella&amp;rft.au=Dance%2C+Christopher+C.&amp;rft.au=Fan%2C+Lixin&amp;rft.au=Willamowski%2C+Jutta&amp;rft.au=Bray%2C+C%C3%A9dric&amp;rft_id=https%3A%2F%2Fwww.cs.cmu.edu%2F~efros%2Fcourses%2FLBMV07%2FPapers%2Fcsurka-eccv-04.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-coates20112-42"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-coates20112_42-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-coates20112_42-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Coates, Adam; Lee, Honglak; Ng, Andrew Y. (2011). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20130510120705/http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf"><i>An analysis of single-layer networks in unsupervised feature learning</i></a> <span class="cs1-format">(PDF)</span>. International Conference on Artificial Intelligence and Statistics (AISTATS). Archived from <a rel="nofollow" class="external text" href="http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf">the original</a> <span class="cs1-format">(PDF)</span> on 2013-05-10.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=An+analysis+of+single-layer+networks+in+unsupervised+feature+learning&amp;rft.date=2011&amp;rft.aulast=Coates&amp;rft.aufirst=Adam&amp;rft.au=Lee%2C+Honglak&amp;rft.au=Ng%2C+Andrew+Y.&amp;rft_id=http%3A%2F%2Fwww.stanford.edu%2F~acoates%2Fpapers%2Fcoatesleeng_aistats_2011.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-schwenker2-43"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-schwenker2_43-0" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Schwenker, Friedhelm; Kestler, Hans A.; Palm, Gnther (2001). "Three learning phases for radial-basis-function networks". <i>Neural Networks</i>. <b>14</b> (45): 439458. <a href="https://en.wikipedia.org/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&nbsp;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.312">10.1.1.109.312</a></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fs0893-6080%2801%2900027-2">10.1016/s0893-6080(01)00027-2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=Three+learning+phases+for+radial-basis-function+networks&amp;rft.volume=14&amp;rft.issue=4%E2%80%935&amp;rft.pages=439-458&amp;rft.date=2001&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.109.312&amp;rft_id=info%3Adoi%2F10.1016%2Fs0893-6080%2801%2900027-2&amp;rft.aulast=Schwenker&amp;rft.aufirst=Friedhelm&amp;rft.au=Kestler%2C+Hans+A.&amp;rft.au=Palm%2C+G%C3%BCnther&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-44" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation conference">Lin, Dekang; Wu, Xiaoyun (2009). <a rel="nofollow" class="external text" href="http://www.aclweb.org/anthology/P/P09/P09-1116.pdf"><i>Phrase clustering for discriminative learning</i></a> <span class="cs1-format">(PDF)</span>. Annual Meeting of the <a href="https://en.wikipedia.org/wiki/Association_for_Computational_Linguistics" title="Association for Computational Linguistics">ACL</a> and IJCNLP. pp.&nbsp;10301038.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Phrase+clustering+for+discriminative+learning&amp;rft.pages=1030-1038&amp;rft.date=2009&amp;rft.aulast=Lin&amp;rft.aufirst=Dekang&amp;rft.au=Wu%2C+Xiaoyun&amp;rft_id=http%3A%2F%2Fwww.aclweb.org%2Fanthology%2FP%2FP09%2FP09-1116.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-:0-45"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-:0_45-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-:0_45-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Press, W. H.; Teukolsky, S. A.; Vetterling, W. T.; Flannery, B. P. (2007). <a rel="nofollow" class="external text" href="http://apps.nrbook.com/empanel/index.html#pg=842">"Section 16.1. Gaussian Mixture Models and <i>k</i>-Means Clustering"</a>. <i>Numerical Recipes: The Art of Scientific Computing</i> (3rd ed.). New York (NY): Cambridge University Press. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-88068-8" title="Special:BookSources/978-0-521-88068-8"><bdi>978-0-521-88068-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Section+16.1.+Gaussian+Mixture+Models+and+k-Means+Clustering&amp;rft.btitle=Numerical+Recipes%3A+The+Art+of+Scientific+Computing&amp;rft.place=New+York+%28NY%29&amp;rft.edition=3rd&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2007&amp;rft.isbn=978-0-521-88068-8&amp;rft.aulast=Press&amp;rft.aufirst=W.+H.&amp;rft.au=Teukolsky%2C+S.+A.&amp;rft.au=Vetterling%2C+W.+T.&amp;rft.au=Flannery%2C+B.+P.&amp;rft_id=http%3A%2F%2Fapps.nrbook.com%2Fempanel%2Findex.html%23pg%3D842&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-46" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation book">Kevin P. Murphy (2012). <a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/810414751"><i>Machine learning&nbsp;: a probabilistic perspective</i></a>. Cambridge, Mass.: MIT Press. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-262-30524-2" title="Special:BookSources/978-0-262-30524-2"><bdi>978-0-262-30524-2</bdi></a>. <a href="https://en.wikipedia.org/wiki/OCLC" title="OCLC">OCLC</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/810414751">810414751</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Machine+learning+%3A+a+probabilistic+perspective&amp;rft.place=Cambridge%2C+Mass.&amp;rft.pub=MIT+Press&amp;rft.date=2012&amp;rft_id=info%3Aoclcnum%2F810414751&amp;rft.isbn=978-0-262-30524-2&amp;rft.au=Kevin+P.+Murphy&amp;rft_id=https%3A%2F%2Fwww.worldcat.org%2Foclc%2F810414751&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-47" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Aharon, Michal; Elad, Michael; Bruckstein, Alfred (2006). <a rel="nofollow" class="external text" href="http://www.cs.technion.ac.il/FREDDY/papers/120.pdf">"K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Signal Processing</i>. <b>54</b> (11): 4311. <a href="https://en.wikipedia.org/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2006ITSP...54.4311A">2006ITSP...54.4311A</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTSP.2006.881199">10.1109/TSP.2006.881199</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Signal+Processing&amp;rft.atitle=K-SVD%3A+An+Algorithm+for+Designing+Overcomplete+Dictionaries+for+Sparse+Representation&amp;rft.volume=54&amp;rft.issue=11&amp;rft.pages=4311&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1109%2FTSP.2006.881199&amp;rft_id=info%3Abibcode%2F2006ITSP...54.4311A&amp;rft.aulast=Aharon&amp;rft.aufirst=Michal&amp;rft.au=Elad%2C+Michael&amp;rft.au=Bruckstein%2C+Alfred&amp;rft_id=http%3A%2F%2Fwww.cs.technion.ac.il%2FFREDDY%2Fpapers%2F120.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-48" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Zha, Hongyuan; Ding, Chris; Gu, Ming; He, Xiaofeng; Simon, Horst D. (December 2001). <a rel="nofollow" class="external text" href="http://ranger.uta.edu/~chqding/papers/Zha-Kmeans.pdf">"Spectral Relaxation for <i>k</i>-means Clustering"</a> <span class="cs1-format">(PDF)</span>. <i>Neural Information Processing Systems Vol.14 (NIPS 2001)</i>: 10571064.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Information+Processing+Systems+Vol.14+%28NIPS+2001%29&amp;rft.atitle=Spectral+Relaxation+for+k-means+Clustering&amp;rft.pages=1057-1064&amp;rft.date=2001-12&amp;rft.aulast=Zha&amp;rft.aufirst=Hongyuan&amp;rft.au=Ding%2C+Chris&amp;rft.au=Gu%2C+Ming&amp;rft.au=He%2C+Xiaofeng&amp;rft.au=Simon%2C+Horst+D.&amp;rft_id=http%3A%2F%2Franger.uta.edu%2F~chqding%2Fpapers%2FZha-Kmeans.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-49"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-49" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Ding, Chris; He, Xiaofeng (July 2004). <a rel="nofollow" class="external text" href="http://ranger.uta.edu/~chqding/papers/KmeansPCA1.pdf">"K-means Clustering via Principal Component Analysis"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of International Conference on Machine Learning (ICML 2004)</i>: 225232.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+International+Conference+on+Machine+Learning+%28ICML+2004%29&amp;rft.atitle=K-means+Clustering+via+Principal+Component+Analysis&amp;rft.pages=225-232&amp;rft.date=2004-07&amp;rft.aulast=Ding&amp;rft.aufirst=Chris&amp;rft.au=He%2C+Xiaofeng&amp;rft_id=http%3A%2F%2Franger.uta.edu%2F~chqding%2Fpapers%2FKmeansPCA1.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-50"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-50" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Drineas, Petros; Frieze, Alan M.; Kannan, Ravi; Vempala, Santosh; Vinay, Vishwanathan (2004). <a rel="nofollow" class="external text" href="http://www.cc.gatech.edu/~vempala/papers/dfkvv.pdf">"Clustering large graphs via the singular value decomposition"</a> <span class="cs1-format">(PDF)</span>. <i>Machine Learning</i>. <b>56</b> (13): 933. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1023%2Fb%3Amach.0000033113.59016.96">10.1023/b:mach.0000033113.59016.96</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2012-08-02</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=Clustering+large+graphs+via+the+singular+value+decomposition&amp;rft.volume=56&amp;rft.issue=1%E2%80%933&amp;rft.pages=9-33&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1023%2Fb%3Amach.0000033113.59016.96&amp;rft.aulast=Drineas&amp;rft.aufirst=Petros&amp;rft.au=Frieze%2C+Alan+M.&amp;rft.au=Kannan%2C+Ravi&amp;rft.au=Vempala%2C+Santosh&amp;rft.au=Vinay%2C+Vishwanathan&amp;rft_id=http%3A%2F%2Fwww.cc.gatech.edu%2F~vempala%2Fpapers%2Fdfkvv.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-51"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-51" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Cohen, Michael B.; Elder, Sam; Musco, Cameron; Musco, Christopher; Persu, Madalina (2014). "Dimensionality reduction for <i>k</i>-means clustering and low rank approximation (Appendix B)". <a href="https://en.wikipedia.org/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1410.6801">1410.6801</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.DS">cs.DS</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Dimensionality+reduction+for+k-means+clustering+and+low+rank+approximation+%28Appendix+B%29&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1410.6801&amp;rft.aulast=Cohen&amp;rft.aufirst=Michael+B.&amp;rft.au=Elder%2C+Sam&amp;rft.au=Musco%2C+Cameron&amp;rft.au=Musco%2C+Christopher&amp;rft.au=Persu%2C+Madalina&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-Little20112-52"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-Little20112_52-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-Little20112_52-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Little, Max A.; Jones, Nick S. (2011). <a rel="nofollow" class="external text" href="http://www.maxlittle.net/publications/pwc_filtering_arxiv.pdf">"Generalized Methods and Solvers for Piecewise Constant Signals: Part I"</a> <span class="cs1-format">(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Proceedings_of_the_Royal_Society_A" class="mw-redirect" title="Proceedings of the Royal Society A">Proceedings of the Royal Society A</a></i>. <b>467</b> (2135): 30883114. <a href="https://en.wikipedia.org/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2011RSPSA.467.3088L">2011RSPSA.467.3088L</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1098%2Frspa.2010.0671">10.1098/rspa.2010.0671</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Central" title="PubMed Central">PMC</a>&nbsp;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3191861">3191861</a></span>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pubmed/22003312">22003312</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Royal+Society+A&amp;rft.atitle=Generalized+Methods+and+Solvers+for+Piecewise+Constant+Signals%3A+Part+I&amp;rft.volume=467&amp;rft.issue=2135&amp;rft.pages=3088-3114&amp;rft.date=2011&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3191861&amp;rft_id=info%3Apmid%2F22003312&amp;rft_id=info%3Adoi%2F10.1098%2Frspa.2010.0671&amp;rft_id=info%3Abibcode%2F2011RSPSA.467.3088L&amp;rft.aulast=Little&amp;rft.aufirst=Max+A.&amp;rft.au=Jones%2C+Nick+S.&amp;rft_id=http%3A%2F%2Fwww.maxlittle.net%2Fpublications%2Fpwc_filtering_arxiv.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
<li id="cite_note-53"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/K-means_clustering#cite_ref-53" aria-label="Jump up" title="Jump up">^</a></b></span> <span class="reference-text"><cite class="citation journal">Vinnikov, Alon; Shalev-Shwartz, Shai (2014). <a rel="nofollow" class="external text" href="http://www.cs.huji.ac.il/~shais/papers/KmeansICA_ICML2014.pdf">"K-means Recovers ICA Filters when Independent Components are Sparse"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of the International Conference on Machine Learning (ICML 2014)</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+International+Conference+on+Machine+Learning+%28ICML+2014%29&amp;rft.atitle=K-means+Recovers+ICA+Filters+when+Independent+Components+are+Sparse&amp;rft.date=2014&amp;rft.aulast=Vinnikov&amp;rft.aufirst=Alon&amp;rft.au=Shalev-Shwartz%2C+Shai&amp;rft_id=http%3A%2F%2Fwww.cs.huji.ac.il%2F~shais%2Fpapers%2FKmeansICA_ICML2014.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"></span>
</li>
</ol></div>
<!-- 
NewPP limit report
Parsed by mw1345
Cached time: 20191203123041
Cache expiry: 2592000
Dynamic content: false
Complications: [varyrevisionsha1]
CPU time usage: 1.456 seconds
Real time usage: 2.614 seconds
Preprocessor visited node count: 3367/1000000
Preprocessor generated node count: 0/1500000
Postexpand include size: 127038/2097152 bytes
Template argument size: 1194/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 10/500
Unstrip recursion depth: 1/20
Unstrip postexpand size: 169494/5000000 bytes
Number of Wikibase entities loaded: 10/400
Lua time usage: 0.892/10.000 seconds
Lua memory usage: 6.42 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 2237.755      1 -total
 81.99% 1834.822      1 Template:Reflist
 50.57% 1131.718     26 Template:Cite_journal
 13.20%  295.455      9 Template:Cite_book
  6.17%  138.038     10 Template:Cite_conference
  6.00%  134.358      1 Template:Short_description
  5.52%  123.612      1 Template:Pagetype
  4.56%  102.079      1 Template:Machine_learning_bar
  4.33%   96.823      1 Template:Sidebar_with_collapsible_lists
  4.00%   89.485      1 Template:Cite_encyclopedia
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1860407-0!canonical!math=5 and timestamp 20191203123039 and revision id 929064204
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;oldid=929064204">https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;oldid=929064204</a>"</div>
		
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="https://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Cluster_analysis_algorithms" title="Category:Cluster analysis algorithms">Cluster analysis algorithms</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="https://en.wikipedia.org/wiki/Category:CS1_French-language_sources_(fr)" title="Category:CS1 French-language sources (fr)">CS1 French-language sources (fr)</a></li><li><a href="https://en.wikipedia.org/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_uses_editors_parameter" title="Category:CS1 maint: uses editors parameter">CS1 maint: uses editors parameter</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id="mw-data-after-content">
	<div class="read-more-container"></div>
</div>


		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="https://en.wikipedia.org/wiki/Special:MyTalk" title="Discussion about edits from this IP address [alt-shift-n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="https://en.wikipedia.org/wiki/Special:MyContributions" title="A list of edits made from this IP address [alt-shift-y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=K-means+clustering" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=K-means+clustering" title="You&#39;re encouraged to log in; however, it&#39;s not mandatory. [alt-shift-o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><a href="https://en.wikipedia.org/wiki/K-means_clustering" title="View the content page [alt-shift-c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="https://en.wikipedia.org/wiki/Talk:K-means_clustering" rel="discussion" title="Discussion about the content page [alt-shift-t]" accesskey="t">Talk</a></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label">
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<ul class="menu">
													</ul>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><a href="https://en.wikipedia.org/wiki/K-means_clustering">Read</a></li><li id="ca-edit" class="collapsible"><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit" title="Edit this page [alt-shift-e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=history" title="Past revisions of this page [alt-shift-h]" accesskey="h">View history</a></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label"><span>More</span></h3>
						<ul class="menu">
													</ul>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="https://en.wikipedia.org/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [alt-shift-f]" accesskey="f" id="searchInput" autocomplete="off"><input type="hidden" value="Special:Search" name="title"><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton">							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page [alt-shift-z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="https://en.wikipedia.org/wiki/Portal:Featured_content" title="Featured content  the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="https://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="https://en.wikipedia.org/wiki/Special:Random" title="Load a random article [alt-shift-x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="https://shop.wikimedia.org/" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="https://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="https://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [alt-shift-r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="https://en.wikipedia.org/wiki/Special:WhatLinksHere/K-means_clustering" title="List of all English Wikipedia pages containing links to this page [alt-shift-j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="https://en.wikipedia.org/wiki/Special:RecentChangesLinked/K-means_clustering" rel="nofollow" title="Recent changes in pages linked from this page [alt-shift-k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [alt-shift-u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="https://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [alt-shift-q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;oldid=929064204" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q310401" title="Link to connected data repository item [alt-shift-g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=K-means_clustering&amp;id=929064204" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=K-means+clustering">Create a book</a></li><li id="coll-download-as-rl"><a href="https://en.wikipedia.org/w/index.php?title=Special:ElectronPdf&amp;page=K-means+clustering&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;printable=yes" title="Printable version of this page [alt-shift-p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label"><button class="uls-settings-trigger" title="Language settings"></button>
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-ar" style=""><a href="https://ar.wikipedia.org/wiki/%D8%AE%D9%88%D8%A7%D8%B1%D8%B2%D9%85%D9%8A%D8%A9_%D8%AA%D8%B5%D9%86%D9%8A%D9%81%D9%8A%D8%A9" title="   Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-ca" style="display: none;"><a href="https://ca.wikipedia.org/wiki/Algorisme_k-means" title="Algorisme k-means  Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target">Catal</a></li><li class="interlanguage-link interwiki-cs" style="display: none;"><a href="https://cs.wikipedia.org/wiki/K-means" title="K-means  Czech" lang="cs" hreflang="cs" class="interlanguage-link-target">etina</a></li><li class="interlanguage-link interwiki-de" style=""><a href="https://de.wikipedia.org/wiki/K-Means-Algorithmus" title="K-Means-Algorithmus  German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-et" style="display: none;"><a href="https://et.wikipedia.org/wiki/K-keskmiste_klasterdamine" title="K-keskmiste klasterdamine  Estonian" lang="et" hreflang="et" class="interlanguage-link-target">Eesti</a></li><li class="interlanguage-link interwiki-el" style="display: none;"><a href="https://el.wikipedia.org/wiki/%CE%9F%CE%BC%CE%B1%CE%B4%CE%BF%CF%80%CE%BF%CE%AF%CE%B7%CF%83%CE%B7_%CE%9A-%CE%BC%CE%AD%CF%83%CF%89%CE%BD" title=" -  Greek" lang="el" hreflang="el" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-es" style=""><a href="https://es.wikipedia.org/wiki/K-medias" title="K-medias  Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Espaol</a></li><li class="interlanguage-link interwiki-fa" style="display: none;"><a href="https://fa.wikipedia.org/wiki/%D8%AE%D9%88%D8%B4%D9%87%E2%80%8C%D8%A8%D9%86%D8%AF%DB%8C_%DA%A9%DB%8C-%D9%85%DB%8C%D8%A7%D9%86%DA%AF%DB%8C%D9%86" title=" -  Persian" lang="fa" hreflang="fa" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-fr" style=""><a href="https://fr.wikipedia.org/wiki/K-moyennes" title="K-moyennes  French" lang="fr" hreflang="fr" class="interlanguage-link-target">Franais</a></li><li class="interlanguage-link interwiki-ko" style="display: none;"><a href="https://ko.wikipedia.org/wiki/K-%ED%8F%89%EA%B7%A0_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98" title="K-   Korean" lang="ko" hreflang="ko" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-id" style=""><a href="https://id.wikipedia.org/wiki/K-means" title="K-means  Indonesian" lang="id" hreflang="id" class="interlanguage-link-target">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-it" style="display: none;"><a href="https://it.wikipedia.org/wiki/K-means" title="K-means  Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li><li class="interlanguage-link interwiki-he" style="display: none;"><a href="https://he.wikipedia.org/wiki/%D7%90%D7%9C%D7%92%D7%95%D7%A8%D7%99%D7%AA%D7%9D_k-%D7%9E%D7%A8%D7%9B%D7%96%D7%99%D7%9D" title=" k-  Hebrew" lang="he" hreflang="he" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-ja" style="display: none;"><a href="https://ja.wikipedia.org/wiki/K%E5%B9%B3%E5%9D%87%E6%B3%95" title="K  Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-no" style="display: none;"><a href="https://no.wikipedia.org/wiki/K-means" title="K-means  Norwegian" lang="no" hreflang="no" class="interlanguage-link-target">Norsk</a></li><li class="interlanguage-link interwiki-pl" style="display: none;"><a href="https://pl.wikipedia.org/wiki/Algorytm_centroid%C3%B3w" title="Algorytm centroidw  Polish" lang="pl" hreflang="pl" class="interlanguage-link-target">Polski</a></li><li class="interlanguage-link interwiki-pt" style=""><a href="https://pt.wikipedia.org/wiki/K-means" title="K-means  Portuguese" lang="pt" hreflang="pt" class="interlanguage-link-target">Portugus</a></li><li class="interlanguage-link interwiki-ru" style=""><a href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D1%81%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%85" title=" k-  Russian" lang="ru" hreflang="ru" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-sr" style="display: none;"><a href="https://sr.wikipedia.org/wiki/Klasterizacija_metodom_K-srednjih_vrednosti" title="Klasterizacija metodom K-srednjih vrednosti  Serbian" lang="sr" hreflang="sr" class="interlanguage-link-target"> / srpski</a></li><li class="interlanguage-link interwiki-th" style="display: none;"><a href="https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%81%E0%B8%9A%E0%B9%88%E0%B8%87%E0%B8%81%E0%B8%A5%E0%B8%B8%E0%B9%88%E0%B8%A1%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B9%80%E0%B8%84%E0%B8%A1%E0%B8%B5%E0%B8%99" title="  Thai" lang="th" hreflang="th" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-tr" style="display: none;"><a href="https://tr.wikipedia.org/wiki/K-means_k%C3%BCmeleme" title="K-means kmeleme  Turkish" lang="tr" hreflang="tr" class="interlanguage-link-target">Trke</a></li><li class="interlanguage-link interwiki-uk" style="display: none;"><a href="https://uk.wikipedia.org/wiki/%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D1%96%D1%8F_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%BE%D0%BC_%D0%BA%E2%80%93%D1%81%D0%B5%D1%80%D0%B5%D0%B4%D0%BD%D1%96%D1%85" title="    Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-ur" style=""><a href="https://ur.wikipedia.org/wiki/%DA%A9-%D8%A7%D9%88%D8%B3%D8%B7_%D8%AE%D9%88%D8%B4%DB%81_%DA%86%DB%8C%D9%86%DB%8C" title="-    Urdu" lang="ur" hreflang="ur" class="interlanguage-link-target"></a></li><li class="interlanguage-link interwiki-zh" style=""><a href="https://zh.wikipedia.org/wiki/K-%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95" title="K-  Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target"></a></li>				<button class="mw-interlanguage-selector mw-ui-button" title="All languages (initial selection from common choices by you and others)">15 more</button></ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q310401#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 3 December 2019, at 12:30<span class="anonymous-show">&nbsp;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="https://foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="https://foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia is a registered trademark of the <a href="https://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/v2/#/en.wikipedia.org">Statistics</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="https://en.m.wikipedia.org/w/index.php?title=K-means_clustering&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							<li style="display: none;"><a href="https://en.wikipedia.org/wiki/K-means_clustering#">Enable previews</a></li></ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="./k-means clustering - Wikipedia_files/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"></a>					</li>
										<li id="footer-poweredbyico">
						<a href="https://www.mediawiki.org/"><img src="./k-means clustering - Wikipedia_files/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"1.456","walltime":"2.614","ppvisitednodes":{"value":3367,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":127038,"limit":2097152},"templateargumentsize":{"value":1194,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":10,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":169494,"limit":5000000},"entityaccesscount":{"value":10,"limit":400},"timingprofile":["100.00% 2237.755      1 -total"," 81.99% 1834.822      1 Template:Reflist"," 50.57% 1131.718     26 Template:Cite_journal"," 13.20%  295.455      9 Template:Cite_book","  6.17%  138.038     10 Template:Cite_conference","  6.00%  134.358      1 Template:Short_description","  5.52%  123.612      1 Template:Pagetype","  4.56%  102.079      1 Template:Machine_learning_bar","  4.33%   96.823      1 Template:Sidebar_with_collapsible_lists","  4.00%   89.485      1 Template:Cite_encyclopedia"]},"scribunto":{"limitreport-timeusage":{"value":"0.892","limit":"10.000"},"limitreport-memusage":{"value":6733571,"limit":52428800}},"cachereport":{"origin":"mw1345","timestamp":"20191203123041","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"K-means clustering","url":"https:\/\/en.wikipedia.org\/wiki\/K-means_clustering","sameAs":"http:\/\/www.wikidata.org\/entity\/Q310401","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q310401","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2005-05-09T00:06:48Z","dateModified":"2019-12-03T12:30:32Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Kernel_Machine.svg","headline":"algorithm for cluster analysis in data mining"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":111,"wgHostname":"mw1331"});});</script>


<div class="suggestions" style="display: none; font-size: 13px;"><div class="suggestions-results"></div><div class="suggestions-special"></div></div><a accesskey="v" href="https://en.wikipedia.org/wiki/K-means_clustering?action=edit" class="oo-ui-element-hidden"></a><div id="mwe-popups-svg"><svg xmlns="http://www.w3.org/2000/svg" width="0" height="0"><defs><clippath id="mwe-popups-mask"><path d="M0 8h10l8-8 8 8h974v992H0z"></path></clippath><clippath id="mwe-popups-mask-flip"><path d="M0 8h294l8-8 8 8h690v992H0z"></path></clippath><clippath id="mwe-popups-landscape-mask"><path d="M0 8h174l8-8 8 8h810v992H0z"></path></clippath><clippath id="mwe-popups-landscape-mask-flip"><path d="M0 0h1000v242H190l-8 8-8-8H0z"></path></clippath></defs></svg></div></body></html>